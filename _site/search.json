[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Overview of the step"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Home",
    "section": "",
    "text": "Overview of the step"
  },
  {
    "objectID": "index.html#steps",
    "href": "index.html#steps",
    "title": "Home",
    "section": "2 Steps",
    "text": "2 Steps\nHere are the steps."
  },
  {
    "objectID": "pipelines/api-developer-portal-service.html",
    "href": "pipelines/api-developer-portal-service.html",
    "title": "üë®üèæ‚Äçüíª API Developer Portal Service Setup",
    "section": "",
    "text": "Content goes here‚Ä¶",
    "crumbs": [
      "Home",
      "7. API Developer Portal Service Setup"
    ]
  },
  {
    "objectID": "pipelines/api-developer-portal-service.html#overview",
    "href": "pipelines/api-developer-portal-service.html#overview",
    "title": "üë®üèæ‚Äçüíª API Developer Portal Service Setup",
    "section": "",
    "text": "Content goes here‚Ä¶",
    "crumbs": [
      "Home",
      "7. API Developer Portal Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html",
    "href": "pipelines/weather-ingestion-service.html",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Weather ingestion service, which fetches live weather data for the Portland area from the Open-Meteo API and stores it in your Postgres database every five minutes.\nThis service collects unstructured weather readings‚Äîlike temperature and wind speed‚Äîand writes them to the weather_json_data table as raw JSON for later transformation and analysis. The data will later be joined with flight data to investigate correlations such as weather-driven no-fly windows.\n\n\n\n\nFor this demo, we‚Äôre using the following Open-Meteo endpoint:\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\n\nlatitude=45.52 and longitude=-122.68 pinpoint downtown Portland.\ncurrent=... specifies the current weather variables to return:\n\ntemperature_2m: air temperature 2 meters above the ground.\nwind_speed_10m: wind speed 10 meters above the ground.\nweathercode: numeric weather condition code (e.g., fog, rain, snow).\n\n\nüìö Learn more at:\nüîó Open-Meteo API Docs\n\n\n\n\n\n\n\n\nIn your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce the image is deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nweather_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\n‚úÖ Tip: Use Railway‚Äôs variable picker to link to your Postgres service when setting DATABASE_URL.\n\n\n\n\n\n\n\n\n\nGo to the Settings tab of the Web2DB Weather service.\nScroll to Triggers and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the trigger type.\nUse this schedule: */5 * * * *\n\nThis means the service will run every 5 minutes.\nüìö Reference: Crontab Guru ‚Äî an easy tool to preview cron expressions.\n\n\n\n\n\nOnce your cron trigger and environment variables are saved, the service will begin pulling weather data.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if you haven‚Äôt already).\nRun:\n\nSELECT * FROM weather_json_data ORDER BY timestamptz DESC LIMIT 10;\nYou should see recent entries showing full JSON responses from the Open-Meteo API.\n\n\n\n\n- Check the Deployments tab in Railway to confirm successful runs.\n- Use the Logs tab to inspect real-time debug output.\n- Ensure the SITE_URL is formatted correctly (with no line breaks or spaces).\n- If the table is still empty after 5‚Äì10 minutes, double-check the DATABASE_URL, TABLE_NAME, and cron settings.\n\n\n\nWith both Web2DB Flights and Web2DB Weather streaming live data into your Postgres instance, you‚Äôre ready to build the DataTransform service that will process this raw data into structured, relational models‚Äîmaking it easier to perform analytics and build dashboards.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#overview",
    "href": "pipelines/weather-ingestion-service.html#overview",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Weather ingestion service, which fetches live weather data for the Portland area from the Open-Meteo API and stores it in your Postgres database every five minutes.\nThis service collects unstructured weather readings‚Äîlike temperature and wind speed‚Äîand writes them to the weather_json_data table as raw JSON for later transformation and analysis. The data will later be joined with flight data to investigate correlations such as weather-driven no-fly windows.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#about-the-open-meteo-api-url",
    "href": "pipelines/weather-ingestion-service.html#about-the-open-meteo-api-url",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "For this demo, we‚Äôre using the following Open-Meteo endpoint:\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\n\nlatitude=45.52 and longitude=-122.68 pinpoint downtown Portland.\ncurrent=... specifies the current weather variables to return:\n\ntemperature_2m: air temperature 2 meters above the ground.\nwind_speed_10m: wind speed 10 meters above the ground.\nweathercode: numeric weather condition code (e.g., fog, rain, snow).\n\n\nüìö Learn more at:\nüîó Open-Meteo API Docs",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#deploying-the-web2db-weather-service",
    "href": "pipelines/weather-ingestion-service.html#deploying-the-web2db-weather-service",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "In your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce the image is deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nweather_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\n‚úÖ Tip: Use Railway‚Äôs variable picker to link to your Postgres service when setting DATABASE_URL.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#scheduling-the-ingestion",
    "href": "pipelines/weather-ingestion-service.html#scheduling-the-ingestion",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "Go to the Settings tab of the Web2DB Weather service.\nScroll to Triggers and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the trigger type.\nUse this schedule: */5 * * * *\n\nThis means the service will run every 5 minutes.\nüìö Reference: Crontab Guru ‚Äî an easy tool to preview cron expressions.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#testing-the-ingestion",
    "href": "pipelines/weather-ingestion-service.html#testing-the-ingestion",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "Once your cron trigger and environment variables are saved, the service will begin pulling weather data.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if you haven‚Äôt already).\nRun:\n\nSELECT * FROM weather_json_data ORDER BY timestamptz DESC LIMIT 10;\nYou should see recent entries showing full JSON responses from the Open-Meteo API.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#troubleshooting-tips",
    "href": "pipelines/weather-ingestion-service.html#troubleshooting-tips",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "- Check the Deployments tab in Railway to confirm successful runs.\n- Use the Logs tab to inspect real-time debug output.\n- Ensure the SITE_URL is formatted correctly (with no line breaks or spaces).\n- If the table is still empty after 5‚Äì10 minutes, double-check the DATABASE_URL, TABLE_NAME, and cron settings.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#whats-next",
    "href": "pipelines/weather-ingestion-service.html#whats-next",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "With both Web2DB Flights and Web2DB Weather streaming live data into your Postgres instance, you‚Äôre ready to build the DataTransform service that will process this raw data into structured, relational models‚Äîmaking it easier to perform analytics and build dashboards.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html",
    "href": "pipelines/db-api-bridge-service.html",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "Once your structured data is ready for exploration and integration, the next step is to expose it via an API. This allows your data to be accessed securely by dashboards, external applications, or developer tools. Instead of building a custom REST API, we can use PostgREST, which automates this process by generating RESTful endpoints directly from your PostgreSQL schema.\n\n\n\n\nPostgREST is a lightweight, open-source tool that turns your PostgreSQL database into a secure RESTful API. With PostgREST, your views and tables become accessible via HTTP without writing backend code.\n\n\n\nüöÄ Generates REST endpoints for tables, views, and stored procedures.\nüîê Uses PostgreSQL‚Äôs native roles and permissions for access control.\nüìä Perfect for exposing analytic views to tools like Swagger, frontend apps, and more.\n\n\n\n\n\n\n\n\n\nClick Create in your Railway project.\nChoose Docker Image.\nEnter the Docker image name:\npostgrest/postgrest\nClick Deploy.\n\n\n\n\n\nAfter deployment:\n\nClick on the Deployments tab, then select Settings.\nUnder Public Networking, click Generate Domain.\n\nYou can customize the domain to a meaningful name if desired.\nCopy the public URL shown.\n\nScroll to the Serverless section.\nToggle Enable Serverless, then click Deploy again.\n\n\nEnabling Serverless mode helps reduce costs ‚Äî the service sleeps when not in use and wakes up on demand.\n\n\n\n\n\nNavigate to the Variables tab and add the following:\n\n\n\nKey\nValue\n\n\n\n\nPGRST_DB_ANON_ROLE\nweb_anon\n\n\nPGRST_DB_SCHEMA\napi\n\n\nPGRST_DB_URI\n${{DATABASE_URL}}\n\n\nPGRST_OPENAPI_SERVER_PROXY_URI\n&lt;your public hostname&gt;\n\n\n\n\nReplace &lt;your public hostname&gt; with the public domain you generated in the previous step.\n\n\n\n\n\n\nOpen Beekeeper Studio and execute the following SQL:\n-- Create a non-login role to define anonymous API access\nCREATE ROLE web_anon NOLOGIN;\n\n-- Allow that role to access the \"api\" schema\nGRANT USAGE ON SCHEMA api TO web_anon;\n\n-- Limit access only to this view\nGRANT SELECT ON api.general_aviation_weather_view TO web_anon;\nThis ensures PostgREST can only read from a specific schema (api) and specific view (general_aviation_weather_view). It does not grant access to other tables or schemas.\n\n\n\n\nRather than using the powerful postgres role to connect from PostgREST, we create a separate authenticator user that can assume the web_anon role. This is more secure and scalable.\nCREATE ROLE authenticator NOINHERIT LOGIN PASSWORD '&lt;your password&gt;';\nGRANT web_anon TO authenticator;\n\n\n\nNOINHERIT: the authenticator role cannot directly access anything.\nLOGIN: it can be used to log in from PostgREST.\nGRANT web_anon: allows this user to act as web_anon.\n\nüìö See PostgREST Tutorial, Step 3\n\n\n\n\n\nTo test your PostgREST API:\n\nOpen your browser or use a tool like curl.\nVisit your public API URL, appending the view name:\n\nhttps://&lt;your-domain&gt;.up.railway.app/general_aviation_weather_view\nIf everything is configured correctly, you should see a JSON response:\n[\n  {\n    \"timestamp\": \"2025-04-08T16:00:00Z\",\n    \"callsign\": \"N12345\",\n    \"altitude_meters\": 1524,\n    \"precipitation_mm\": 0.8,\n    \"weathercode\": 61\n  },\n  ...\n]\n‚úÖ If you see data, your DB API bridge is fully operational!\n\n\n\n\n\n‚úÖ PostgREST converts your database views into REST endpoints.\n‚úÖ You isolated access using a dedicated schema (api) and view.\n‚úÖ You secured the system using PostgreSQL roles (web_anon and authenticator).\n‚úÖ You can now integrate this endpoint with Swagger, frontend tools, or other systems.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#why-set-up-a-db-api-bridge",
    "href": "pipelines/db-api-bridge-service.html#why-set-up-a-db-api-bridge",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "Once your structured data is ready for exploration and integration, the next step is to expose it via an API. This allows your data to be accessed securely by dashboards, external applications, or developer tools. Instead of building a custom REST API, we can use PostgREST, which automates this process by generating RESTful endpoints directly from your PostgreSQL schema.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#what-is-postgrest",
    "href": "pipelines/db-api-bridge-service.html#what-is-postgrest",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "PostgREST is a lightweight, open-source tool that turns your PostgreSQL database into a secure RESTful API. With PostgREST, your views and tables become accessible via HTTP without writing backend code.\n\n\n\nüöÄ Generates REST endpoints for tables, views, and stored procedures.\nüîê Uses PostgreSQL‚Äôs native roles and permissions for access control.\nüìä Perfect for exposing analytic views to tools like Swagger, frontend apps, and more.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#step-by-step-deploy-postgrest-in-railway",
    "href": "pipelines/db-api-bridge-service.html#step-by-step-deploy-postgrest-in-railway",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "Click Create in your Railway project.\nChoose Docker Image.\nEnter the Docker image name:\npostgrest/postgrest\nClick Deploy.\n\n\n\n\n\nAfter deployment:\n\nClick on the Deployments tab, then select Settings.\nUnder Public Networking, click Generate Domain.\n\nYou can customize the domain to a meaningful name if desired.\nCopy the public URL shown.\n\nScroll to the Serverless section.\nToggle Enable Serverless, then click Deploy again.\n\n\nEnabling Serverless mode helps reduce costs ‚Äî the service sleeps when not in use and wakes up on demand.\n\n\n\n\n\nNavigate to the Variables tab and add the following:\n\n\n\nKey\nValue\n\n\n\n\nPGRST_DB_ANON_ROLE\nweb_anon\n\n\nPGRST_DB_SCHEMA\napi\n\n\nPGRST_DB_URI\n${{DATABASE_URL}}\n\n\nPGRST_OPENAPI_SERVER_PROXY_URI\n&lt;your public hostname&gt;\n\n\n\n\nReplace &lt;your public hostname&gt; with the public domain you generated in the previous step.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#set-up-database-permissions",
    "href": "pipelines/db-api-bridge-service.html#set-up-database-permissions",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "Open Beekeeper Studio and execute the following SQL:\n-- Create a non-login role to define anonymous API access\nCREATE ROLE web_anon NOLOGIN;\n\n-- Allow that role to access the \"api\" schema\nGRANT USAGE ON SCHEMA api TO web_anon;\n\n-- Limit access only to this view\nGRANT SELECT ON api.general_aviation_weather_view TO web_anon;\nThis ensures PostgREST can only read from a specific schema (api) and specific view (general_aviation_weather_view). It does not grant access to other tables or schemas.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#create-a-dedicated-authenticator-role",
    "href": "pipelines/db-api-bridge-service.html#create-a-dedicated-authenticator-role",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "Rather than using the powerful postgres role to connect from PostgREST, we create a separate authenticator user that can assume the web_anon role. This is more secure and scalable.\nCREATE ROLE authenticator NOINHERIT LOGIN PASSWORD '&lt;your password&gt;';\nGRANT web_anon TO authenticator;\n\n\n\nNOINHERIT: the authenticator role cannot directly access anything.\nLOGIN: it can be used to log in from PostgREST.\nGRANT web_anon: allows this user to act as web_anon.\n\nüìö See PostgREST Tutorial, Step 3",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#test-the-api-endpoint",
    "href": "pipelines/db-api-bridge-service.html#test-the-api-endpoint",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "To test your PostgREST API:\n\nOpen your browser or use a tool like curl.\nVisit your public API URL, appending the view name:\n\nhttps://&lt;your-domain&gt;.up.railway.app/general_aviation_weather_view\nIf everything is configured correctly, you should see a JSON response:\n[\n  {\n    \"timestamp\": \"2025-04-08T16:00:00Z\",\n    \"callsign\": \"N12345\",\n    \"altitude_meters\": 1524,\n    \"precipitation_mm\": 0.8,\n    \"weathercode\": 61\n  },\n  ...\n]\n‚úÖ If you see data, your DB API bridge is fully operational!",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#summary",
    "href": "pipelines/db-api-bridge-service.html#summary",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "‚úÖ PostgREST converts your database views into REST endpoints.\n‚úÖ You isolated access using a dedicated schema (api) and view.\n‚úÖ You secured the system using PostgreSQL roles (web_anon and authenticator).\n‚úÖ You can now integrate this endpoint with Swagger, frontend tools, or other systems.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html",
    "href": "pipelines/dashboard-service.html",
    "title": "Dashboard Service Setup",
    "section": "",
    "text": "A View in Postgres is a virtual table based on a SQL SELECT query. It behaves just like a table for reading, but doesn‚Äôt store any data itself‚Äîit simply runs the query behind it whenever it‚Äôs accessed.\nViews are excellent for dashboards because:\n\n‚úÖ They abstract complex queries behind a simple name.\n‚úÖ They can be reused by multiple clients (like Grafana).\n‚úÖ You can restrict access to views rather than raw tables using Postgres roles.\n‚úÖ They allow you to pre-join and pre-aggregate data in a way that‚Äôs efficient and secure for visualization.",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#but-first-views",
    "href": "pipelines/dashboard-service.html#but-first-views",
    "title": "Dashboard Service Setup",
    "section": "",
    "text": "A View in Postgres is a virtual table based on a SQL SELECT query. It behaves just like a table for reading, but doesn‚Äôt store any data itself‚Äîit simply runs the query behind it whenever it‚Äôs accessed.\nViews are excellent for dashboards because:\n\n‚úÖ They abstract complex queries behind a simple name.\n‚úÖ They can be reused by multiple clients (like Grafana).\n‚úÖ You can restrict access to views rather than raw tables using Postgres roles.\n‚úÖ They allow you to pre-join and pre-aggregate data in a way that‚Äôs efficient and secure for visualization.",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#create-a-view-for-the-dashboard",
    "href": "pipelines/dashboard-service.html#create-a-view-for-the-dashboard",
    "title": "Dashboard Service Setup",
    "section": "2 Create a View for the Dashboard",
    "text": "2 Create a View for the Dashboard\nLet‚Äôs create a view that answers our research question: &gt; Are there ‚Äúno-fly windows‚Äù correlated with weather thresholds?\nThis view joins structured flight and weather data to support correlation queries:\nCREATE OR REPLACE VIEW flight_weather AS\nSELECT\n    f.timestamp,\n    f.callsign,\n    f.altitude_meters,\n    f.velocity_knots,\n    f.latitude AS flight_latitude,\n    f.longitude AS flight_longitude,\n    w.precipitation_mm,\n    w.weathercode,\n    w.latitude AS weather_latitude,\n    w.longitude AS weather_longitude\nFROM\n    flights f\nJOIN\n    weather_observations w\nON\n    date_trunc('minute', f.timestamp) = date_trunc('minute', w.timestamp)\nORDER BY\n    f.timestamp DESC;",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#deploying-grafana-in-railway",
    "href": "pipelines/dashboard-service.html#deploying-grafana-in-railway",
    "title": "Dashboard Service Setup",
    "section": "3 Deploying Grafana in Railway",
    "text": "3 Deploying Grafana in Railway\n\n3.1 Add the Grafana Template\n\nIn your Railway project, click ‚ÄúCreate‚Äù ‚Üí ‚ÄúTemplate‚Äù.\nType Grafana in the search bar.\nSelect the version provided by Andre Lademann‚Äôs Projects.\nAdd the following environment variables:\n\n\n\nKey\nValue\n\n\n\n\nGF_SECURITY_ADMIN_USER\ngrafanareader\n\n\nGF_DEFAULT_INSTANCE_NAME\ngrafanapg\n\n\nGF_SECURITY_ADMIN_PASSWORD\nyour-password\n\n\n\nLeave the 4 pre-configured environment variables as they are.\nOnce you‚Äôve entered the variables, the service will check connectivity and display a message saying ‚ÄúReady to be deployed‚Äù.\nClick [Deploy]\nThis will start the deployment process. It may take a few minutes.\n\n\n\n\n3.2 Enable Serverless Mode on Grafana\nAfter deployment completes:\n\nReturn to the Grafana service in Railway.\nClick on Settings.\nScroll down to ‚ÄúServerless‚Äù.\nToggle Enable Serverless and click Deploy again.\n\n\n‚úÖ Enabling serverless mode helps reduce cost because the service will go to sleep when not in use and wake up on demand. This is ideal for tools like Grafana that are used periodically.",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#add-a-grafana-specific-db-user",
    "href": "pipelines/dashboard-service.html#add-a-grafana-specific-db-user",
    "title": "Dashboard Service Setup",
    "section": "4 Add a Grafana-Specific DB User",
    "text": "4 Add a Grafana-Specific DB User\nTo avoid giving Grafana full access to your database, we‚Äôll create a read-only user that can only access the view.\n\n4.1 Steps:\n\nOpen your Postgres database in Beekeeper Studio.\nRun the following SQL (replace 'password' with one you‚Äôll remember):\n\nCREATE USER grafanareader WITH PASSWORD 'your_password';\nGRANT USAGE ON SCHEMA \"public\" TO grafanareader;\nGRANT SELECT ON \"public\".flight_weather TO grafanareader;",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#connect-grafana-to-postgres",
    "href": "pipelines/dashboard-service.html#connect-grafana-to-postgres",
    "title": "Dashboard Service Setup",
    "section": "5 Connect Grafana to Postgres",
    "text": "5 Connect Grafana to Postgres\n\n5.1 Copy PGHOST from Railway\n\nGo to your Postgres service in Railway.\nUnder Environment Variables, copy the value for PGHOST.\nIt will look like: postgres.railway.internal.\n\n\n\n\n5.2 Open Grafana and Log In\n\nGo to your Grafana URL shown in the Deployments tab.\nExample:\nhttps://grafana-rbi9-production.up.railway.app\nLog in with:\n\nUsername: grafanareader\nPassword: the password you assigned earlier\n\n\n\n\n\n5.3 Add a Data Source\n\nIn Grafana, click Connections on the left panel.\nClick Data Sources.\nSearch for PostgreSQL and select it.\n\n\n5.3.1 Fill in the following:\n\n\n\nField\nValue\n\n\n\n\nHost\npostgres.railway.internal:5432\n\n\nDatabase\nrailway\n\n\nUser\ngrafanareader\n\n\nPassword\nyour_password\n\n\n\nClick Save & Test to confirm connectivity.",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#create-your-first-dashboard-in-grafana",
    "href": "pipelines/dashboard-service.html#create-your-first-dashboard-in-grafana",
    "title": "Dashboard Service Setup",
    "section": "6 Create Your First Dashboard in Grafana",
    "text": "6 Create Your First Dashboard in Grafana\n\nGo back to the Home page in Grafana.\nClick ‚ÄúCreate your first dashboard‚Äù.\nClick ‚ÄúAdd Visualization‚Äù.\n\n\n6.1 Configure the Panel\n\nChoose Data Source: Select the PostgreSQL source you just set up.\nFrom Table: Select the flight_weather view.\nAdd Columns: Choose at least one time-based column (timestamp) and one data column (e.g., altitude_meters or precipitation_mm).\nClick Run Query.\nUse Table Mode or switch to a chart via Suggested Visualizations.\n\nYou now have a live dashboard pulling from your structured data pipeline!",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#whats-next",
    "href": "pipelines/dashboard-service.html#whats-next",
    "title": "Dashboard Service Setup",
    "section": "7 What‚Äôs Next?",
    "text": "7 What‚Äôs Next?\nFrom here, you can:\n\nAdd filters to segment by weathercode.\nAggregate values like average flight altitude during rain conditions.\nBuild a full visualization of ‚Äúno-fly‚Äù windows over time.\n\nIn the next step we‚Äôll expose your view as a REST API using PostgREST and document it using Swagger.",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html",
    "href": "pipelines/transformation-service.html",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "In this section, you‚Äôll design and deploy a transformation service called DataTransform. This service will extract meaningful fields from your raw JSON flight and weather data and insert them into clean, structured tables in your Postgres database.\nThis transformation process involves: - Understanding the structure of raw JSON. - Designing normalized physical tables for analysis. - Writing SQL to extract values using Postgres JSON functions. - Automating the transformation with a scheduled job in Railway.\n\n\n\n\nBefore transforming the data, you must understand what the raw JSON looks like. You can do this by running:\nSELECT raw_json FROM flight_json_data ORDER BY timestamptz DESC LIMIT 1;\nSELECT raw_json FROM weather_json_data ORDER BY timestamptz DESC LIMIT 1;\nExplore the shape and nesting of each payload. For example: - OpenSky states is an array of arrays. OpenSky JSON Structure - Open-Meteo JSON is a nested object. Open-Meteo Example\nüìö Reference: PostgreSQL JSON Functions and Operators\n\n\n\nBased on the JSON contents, you‚Äôll need normalized tables to store flight and weather data. Here‚Äôs the schema you‚Äôll use:\nCREATE TABLE flight_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE weather_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Structured output tables\nCREATE TABLE flights (\n    id SERIAL PRIMARY KEY,\n    icao24 VARCHAR(10) NOT NULL,\n    callsign VARCHAR(10),\n    country VARCHAR(64),\n    latitude DOUBLE PRECISION,\n    longitude DOUBLE PRECISION,\n    altitude_meters DOUBLE PRECISION,\n    velocity_knots DOUBLE PRECISION,\n    heading_degrees DOUBLE PRECISION,\n    vertical_rate DOUBLE PRECISION,\n    timestamp TIMESTAMPTZ NOT NULL\n);\n\nCREATE TABLE weather_observations (\n    id SERIAL PRIMARY KEY,\n    latitude DOUBLE PRECISION NOT NULL,\n    longitude DOUBLE PRECISION NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL,\n    precipitation_mm DOUBLE PRECISION,\n    weathercode SMALLINT\n);\n\nCREATE TABLE weather_condition (\n    code SMALLINT PRIMARY KEY,\n    description TEXT\n);\nThese tables will be the targets of your transformation logic.\n\n\n\nWhat‚Äôs a Common Table Expression?\nA Common Table Expression (CTE) is a temporary result set defined at the beginning of a SQL statement. It helps modularize complex queries and allows you to: - Break down a multi-step transformation into readable pieces. - Reference intermediate results multiple times.\nüìö Reference: CTEs in PostgreSQL\nExample Transformation for Flights\nBEGIN;\n\nWITH raw AS (\n    SELECT id, timestamptz, jsonb_array_elements(raw_json-&gt;'states') AS state\n    FROM flight_json_data\n),\nparsed AS (\n    SELECT\n        id,\n        state-&gt;&gt;0 AS icao24,\n        state-&gt;&gt;1 AS callsign,\n        state-&gt;&gt;2 AS country,\n        (state-&gt;&gt;6)::DOUBLE PRECISION AS longitude,\n        (state-&gt;&gt;5)::DOUBLE PRECISION AS latitude,\n        (state-&gt;&gt;7)::DOUBLE PRECISION AS altitude_meters,\n        (state-&gt;&gt;9)::DOUBLE PRECISION AS velocity_knots,\n        (state-&gt;&gt;10)::DOUBLE PRECISION AS heading_degrees,\n        (state-&gt;&gt;11)::DOUBLE PRECISION AS vertical_rate,\n        timestamptz\n    FROM raw\n)\nINSERT INTO flights (\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamp\n)\nSELECT\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamptz\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM flight_json_data;\n\nCOMMIT;\nExample Transformation for Weather\nBEGIN;\n\nWITH raw AS (\n    SELECT id, raw_json, timestamptz\n    FROM weather_json_data\n),\nparsed AS (\n    SELECT\n        (raw_json-&gt;'latitude')::DOUBLE PRECISION AS latitude,\n        (raw_json-&gt;'longitude')::DOUBLE PRECISION AS longitude,\n        (raw_json-&gt;'current'-&gt;'precipitation')::DOUBLE PRECISION AS precipitation_mm,\n        (raw_json-&gt;'current'-&gt;'weathercode')::SMALLINT AS weathercode,\n        timestamptz AS timestamp\n    FROM raw\n)\nINSERT INTO weather_observations (\n    latitude, longitude, precipitation_mm, weathercode, timestamp\n)\nSELECT\n    latitude, longitude, precipitation_mm, weathercode, timestamp\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM weather_json_data;\n\nCOMMIT;\n\n\nA TRANSACTION ensures that either all your steps complete successfully, or none of them are applied. This prevents partial writes and data corruption. ‚Ä¢ BEGIN; starts the transaction. ‚Ä¢ COMMIT; applies all changes if no error occurred. ‚Ä¢ If an error occurs, the entire transaction can be rolled back.\n\n\n\n\n1.  [Use the db_transform GitHub template](https://github.com/LucasCordova/db_transform).\n2.  Click the ‚ÄúUse this template‚Äù button.\n3.  Name your repository: `weather_flight_db_transform`\n4.  Choose your GitHub account and click Create Repository.\n\n\n\n1.  Open the new repository on GitHub (recommended) or clone it locally.\n2.  Edit the clean.sql file. If you are editing in the browser, choose the `pencil` icon. Paste your flight and weather transformation SQL inside it.\n3.  Save the file and commit. In the browser editor, click the Commit changes... button, add a message, and click the final Commit changes button.\n\n\n\n1.  Go to your Railway project.\n2.  Click New ‚Üí Deploy from GitHub Repo.\n3.  Click ‚ÄúConfigure GitHub App‚Äù to give Railway access to your GitHub account.\n4.  Choose the repository weather_flight_db_transform.\n5.  Add the following environment variables in the Variables tab. \n\n\n\nKey\nValue\n\n\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\n\n6.  Click Deploy.\n\n\n\n1.  Go to the Settings tab of the DataTransform service.\n2.  Scroll to Triggers ‚Üí New Trigger.\n3.  Choose Cron and enter: `0 * * * *`.\nThis means the service runs once every hour, at the top of the hour.\n\n\n\n‚úÖ Check the Tables in Beekeeper\n\n    After the first cron run, inspect the structured tables:\n\n    ```sql\n    SELECT * FROM flights ORDER BY timestamp DESC LIMIT 10;\n    SELECT * FROM weather_observations ORDER BY timestamp DESC LIMIT 10;\n    ```\n\n‚úÖ View Logs in Railway\n    Use the Logs tab to inspect any errors or debug output.\n\n\n\nNow that your structured data is flowing, you can visualize and analyze it in the next step using Grafana Dashboards. You‚Äôll use SQL queries to explore patterns in flight activity and weather data‚Äîlaying the foundation for answering your core research question.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#overview",
    "href": "pipelines/transformation-service.html#overview",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "In this section, you‚Äôll design and deploy a transformation service called DataTransform. This service will extract meaningful fields from your raw JSON flight and weather data and insert them into clean, structured tables in your Postgres database.\nThis transformation process involves: - Understanding the structure of raw JSON. - Designing normalized physical tables for analysis. - Writing SQL to extract values using Postgres JSON functions. - Automating the transformation with a scheduled job in Railway.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#understand-the-json-structure",
    "href": "pipelines/transformation-service.html#understand-the-json-structure",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "Before transforming the data, you must understand what the raw JSON looks like. You can do this by running:\nSELECT raw_json FROM flight_json_data ORDER BY timestamptz DESC LIMIT 1;\nSELECT raw_json FROM weather_json_data ORDER BY timestamptz DESC LIMIT 1;\nExplore the shape and nesting of each payload. For example: - OpenSky states is an array of arrays. OpenSky JSON Structure - Open-Meteo JSON is a nested object. Open-Meteo Example\nüìö Reference: PostgreSQL JSON Functions and Operators",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#design-your-physical-schema",
    "href": "pipelines/transformation-service.html#design-your-physical-schema",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "Based on the JSON contents, you‚Äôll need normalized tables to store flight and weather data. Here‚Äôs the schema you‚Äôll use:\nCREATE TABLE flight_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE weather_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Structured output tables\nCREATE TABLE flights (\n    id SERIAL PRIMARY KEY,\n    icao24 VARCHAR(10) NOT NULL,\n    callsign VARCHAR(10),\n    country VARCHAR(64),\n    latitude DOUBLE PRECISION,\n    longitude DOUBLE PRECISION,\n    altitude_meters DOUBLE PRECISION,\n    velocity_knots DOUBLE PRECISION,\n    heading_degrees DOUBLE PRECISION,\n    vertical_rate DOUBLE PRECISION,\n    timestamp TIMESTAMPTZ NOT NULL\n);\n\nCREATE TABLE weather_observations (\n    id SERIAL PRIMARY KEY,\n    latitude DOUBLE PRECISION NOT NULL,\n    longitude DOUBLE PRECISION NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL,\n    precipitation_mm DOUBLE PRECISION,\n    weathercode SMALLINT\n);\n\nCREATE TABLE weather_condition (\n    code SMALLINT PRIMARY KEY,\n    description TEXT\n);\nThese tables will be the targets of your transformation logic.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#write-sql-to-transform-the-data",
    "href": "pipelines/transformation-service.html#write-sql-to-transform-the-data",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "What‚Äôs a Common Table Expression?\nA Common Table Expression (CTE) is a temporary result set defined at the beginning of a SQL statement. It helps modularize complex queries and allows you to: - Break down a multi-step transformation into readable pieces. - Reference intermediate results multiple times.\nüìö Reference: CTEs in PostgreSQL\nExample Transformation for Flights\nBEGIN;\n\nWITH raw AS (\n    SELECT id, timestamptz, jsonb_array_elements(raw_json-&gt;'states') AS state\n    FROM flight_json_data\n),\nparsed AS (\n    SELECT\n        id,\n        state-&gt;&gt;0 AS icao24,\n        state-&gt;&gt;1 AS callsign,\n        state-&gt;&gt;2 AS country,\n        (state-&gt;&gt;6)::DOUBLE PRECISION AS longitude,\n        (state-&gt;&gt;5)::DOUBLE PRECISION AS latitude,\n        (state-&gt;&gt;7)::DOUBLE PRECISION AS altitude_meters,\n        (state-&gt;&gt;9)::DOUBLE PRECISION AS velocity_knots,\n        (state-&gt;&gt;10)::DOUBLE PRECISION AS heading_degrees,\n        (state-&gt;&gt;11)::DOUBLE PRECISION AS vertical_rate,\n        timestamptz\n    FROM raw\n)\nINSERT INTO flights (\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamp\n)\nSELECT\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamptz\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM flight_json_data;\n\nCOMMIT;\nExample Transformation for Weather\nBEGIN;\n\nWITH raw AS (\n    SELECT id, raw_json, timestamptz\n    FROM weather_json_data\n),\nparsed AS (\n    SELECT\n        (raw_json-&gt;'latitude')::DOUBLE PRECISION AS latitude,\n        (raw_json-&gt;'longitude')::DOUBLE PRECISION AS longitude,\n        (raw_json-&gt;'current'-&gt;'precipitation')::DOUBLE PRECISION AS precipitation_mm,\n        (raw_json-&gt;'current'-&gt;'weathercode')::SMALLINT AS weathercode,\n        timestamptz AS timestamp\n    FROM raw\n)\nINSERT INTO weather_observations (\n    latitude, longitude, precipitation_mm, weathercode, timestamp\n)\nSELECT\n    latitude, longitude, precipitation_mm, weathercode, timestamp\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM weather_json_data;\n\nCOMMIT;\n\n\nA TRANSACTION ensures that either all your steps complete successfully, or none of them are applied. This prevents partial writes and data corruption. ‚Ä¢ BEGIN; starts the transaction. ‚Ä¢ COMMIT; applies all changes if no error occurred. ‚Ä¢ If an error occurs, the entire transaction can be rolled back.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#use-the-github-template",
    "href": "pipelines/transformation-service.html#use-the-github-template",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "1.  [Use the db_transform GitHub template](https://github.com/LucasCordova/db_transform).\n2.  Click the ‚ÄúUse this template‚Äù button.\n3.  Name your repository: `weather_flight_db_transform`\n4.  Choose your GitHub account and click Create Repository.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#add-your-sql-to-clean.sql",
    "href": "pipelines/transformation-service.html#add-your-sql-to-clean.sql",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "1.  Open the new repository on GitHub (recommended) or clone it locally.\n2.  Edit the clean.sql file. If you are editing in the browser, choose the `pencil` icon. Paste your flight and weather transformation SQL inside it.\n3.  Save the file and commit. In the browser editor, click the Commit changes... button, add a message, and click the final Commit changes button.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#deploy-the-service-in-railway",
    "href": "pipelines/transformation-service.html#deploy-the-service-in-railway",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "1.  Go to your Railway project.\n2.  Click New ‚Üí Deploy from GitHub Repo.\n3.  Click ‚ÄúConfigure GitHub App‚Äù to give Railway access to your GitHub account.\n4.  Choose the repository weather_flight_db_transform.\n5.  Add the following environment variables in the Variables tab. \n\n\n\nKey\nValue\n\n\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\n\n6.  Click Deploy.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#set-cron-schedule",
    "href": "pipelines/transformation-service.html#set-cron-schedule",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "1.  Go to the Settings tab of the DataTransform service.\n2.  Scroll to Triggers ‚Üí New Trigger.\n3.  Choose Cron and enter: `0 * * * *`.\nThis means the service runs once every hour, at the top of the hour.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#testing-and-validation",
    "href": "pipelines/transformation-service.html#testing-and-validation",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "‚úÖ Check the Tables in Beekeeper\n\n    After the first cron run, inspect the structured tables:\n\n    ```sql\n    SELECT * FROM flights ORDER BY timestamp DESC LIMIT 10;\n    SELECT * FROM weather_observations ORDER BY timestamp DESC LIMIT 10;\n    ```\n\n‚úÖ View Logs in Railway\n    Use the Logs tab to inspect any errors or debug output.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#whats-next",
    "href": "pipelines/transformation-service.html#whats-next",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "Now that your structured data is flowing, you can visualize and analyze it in the next step using Grafana Dashboards. You‚Äôll use SQL queries to explore patterns in flight activity and weather data‚Äîlaying the foundation for answering your core research question.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/index.html",
    "href": "pipelines/index.html",
    "title": "Pipelines",
    "section": "",
    "text": "The ability to synthesize new knowledge by integrating disparate data sources is a powerful asset. This guide will walk you through the design and deployment of a data pipeline architecture on Railway.app, a cloud-native development platform that allows you to quickly spin up backend infrastructure with minimal configuration overhead.\nThe goal of this pipeline is to collect, structure, and expose data in a way that supports meaningful analysis and knowledge generation. By combining unstructured and structured data from multiple services into a single relational data model, this system enables real-time insights via a customizable dashboard and a developer-friendly REST API.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/index.html#overview",
    "href": "pipelines/index.html#overview",
    "title": "Pipelines",
    "section": "",
    "text": "The ability to synthesize new knowledge by integrating disparate data sources is a powerful asset. This guide will walk you through the design and deployment of a data pipeline architecture on Railway.app, a cloud-native development platform that allows you to quickly spin up backend infrastructure with minimal configuration overhead.\nThe goal of this pipeline is to collect, structure, and expose data in a way that supports meaningful analysis and knowledge generation. By combining unstructured and structured data from multiple services into a single relational data model, this system enables real-time insights via a customizable dashboard and a developer-friendly REST API.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/index.html#scenario-detecting-no-fly-windows-correlated-with-weather-thresholds",
    "href": "pipelines/index.html#scenario-detecting-no-fly-windows-correlated-with-weather-thresholds",
    "title": "Pipelines",
    "section": "2 Scenario Detecting ‚ÄúNo-Fly Windows‚Äù Correlated with Weather Thresholds",
    "text": "2 Scenario Detecting ‚ÄúNo-Fly Windows‚Äù Correlated with Weather Thresholds\n\n2.1 Research Question\nCan we identify specific weather conditions that correlate with significantly reduced flight activity‚Äîwhat we‚Äôll call ‚Äúno-fly windows‚Äù‚Äîover the Portland metro region? These windows may align with thresholds like heavy precipitation, low visibility, or high wind speeds. To answer this question, we‚Äôll construct a data pipeline that combines live flight and weather data, transforms it into a relational model, and exposes it through dashboards and APIs for analysis and discovery.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/index.html#architecture",
    "href": "pipelines/index.html#architecture",
    "title": "Pipelines",
    "section": "3 Architecture",
    "text": "3 Architecture\nYou will build an architecture composed of six distinct services as illustrated below.\n\n\n\n\n\n   graph TD\n    subgraph Railway.app\n        Postgres[(Postgres Database)]\n        Web2DB1[Web2DB - Scraper/API Service #1 - Flights]\n        Web2DB2[Web2DB - Scraper/API Service #2 - Weather]\n        DataTransform[DataTransform Service]\n        Grafana[Grafana Dashboard]\n        DB2API[PostgREST - DB2API Bridge]\n        Swagger[Swagger UI - API Dev Tool]\n    end\n\n    Web2DB1 --&gt; Postgres\n    Web2DB2 --&gt; Postgres\n    Postgres --&gt; DataTransform\n    DataTransform --&gt; Postgres\n    Postgres --&gt; Grafana\n    Postgres --&gt; DB2API\n    DB2API --&gt; Swagger\n\n\n\n\n\n\n\n3.1 How Each Service Contributes\n\n3.1.1 üíΩ Postgres Database\nThe Postgres database is the central data repository. It stores: - Unstructured data: Raw JSON or payloads directly from the APIs during collection. - Structured data: Normalized tables and time-series records created after transformation. This separation allows for historical data to be archived while maintaining clean, queryable models for analysis.\n\n\n3.1.2 ‚úàÔ∏è Web2DB Flights\nThe Web2DB Flights ingestion service calls the OpenSky Network API every 5 minutes to gather live air traffic data over the Portland metro area. It logs: - Aircraft positions - Altitudes and speeds - Callsigns and ICAO codes - Timestamps and bounding box info This unstructured data is stored directly into Postgres for later processing.\n\n\n3.1.3 üåßÔ∏è Web2DB Weather\nThe Web2DB Weather ingestion service calls the Open-Meteo API every 5 minutes, retrieving weather information for the Portland region. It collects: - Temperature, wind speed and direction - Visibility, cloud cover, and precipitation type - Timestamps and geo-coordinates This raw weather data is logged to the database alongside flight data, aligned by time and location.\n\n\n3.1.4 üîÑ Data Transformation Service (DataTransform)\nOnce per hour, the DataTransform service processes the accumulated unstructured data into a structured relational schema. Key transformations include: - Joining flight and weather records by timestamp - Extracting metrics such as flight count per interval, average altitude, or visibility index - Normalizing date/time formats and location metadata The result is a set of structured tables optimized for query performance and analytical depth.\n\n\n3.1.5 üìä Dashboard Service (Grafana)\nGrafana connects to the structured Postgres schema and generates interactive dashboards to visualize: - Flight activity (e.g., counts, density maps) over time - Weather variable trends - Correlation graphs between weather metrics (e.g., wind speed) and flight drop-offs These dashboards help identify possible ‚Äúno-fly windows‚Äù by aligning dips in flight activity with adverse weather thresholds.\n\n\n3.1.6 üåê DB/API Bridge Service (DB2API)\nUsing PostgREST, this service turns the structured database into a RESTful API. It allows: - Querying historical flight and weather data - Filtering based on date/time, weather conditions, or flight metrics - Serving clean, JSON-based endpoints for developers, analysts, or downstream systems\n\n\n3.1.7 üë®üèæ‚Äçüíª API Developer Interface (Swagger)\nThe Swagger service documents the exposed API endpoints and provides an interactive UI for developers to: - Explore and test API requests in real-time - Understand available data models and query parameters - Integrate this data into custom applications, dashboards, or research tools\n\nTogether, this architecture enables rich, real-time and historical analysis to determine when and why flight activity slows or stops due to environmental conditions. The result is a powerful system for answering not only our current research question, but also a broader range of aviation and climate analytics challenges.\nHead to the next section, where we‚Äôll begin by setting up the Postgres database service and preparing the schema for incoming unstructured data.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html",
    "href": "pipelines/flight-ingestion-service.html",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Flights ingestion service, which fetches live flight data from the OpenSky Network API and stores it in your Postgres database every five minutes.\nThis service will collect unstructured flight telemetry data and append it to the flight_json_data table in raw JSON format. You‚Äôll deploy it as a container on Railway using the lucascordova/web2db image and configure it to run on a schedule.\n\n\n\n\nIf you‚Äôre building your own ingestion service using a different API, here‚Äôs the general process:\n\nCheck if the API requires authentication (e.g., an API key or access token).\nRead the API documentation to understand query parameters, rate limits, and response structure.\nBuild your request URL including any necessary geographic filters, authentication tokens, or query strings.\nTest the request manually (e.g., in your browser or Postman) before deploying it inside a service like Railway.\n\nFor APIs that require keys or tokens, you should always inject these as environment variables, never hardcoded in your code or Docker image.\n\n\n\n\nFor this demo, we‚Äôre using the following public OpenSky endpoint: `https://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00‚Äô\n\n\n\nlamin and lamax: latitude min/max (45.08 to 45.88)\nlomin and lomax: longitude min/max (‚àí123.50 to ‚àí122.00)\n\nThis bounding box covers the Portland metropolitan area and surrounding airspace.\nüìö Learn more at:\nüîó OpenSky Network REST API Documentation\n\n\n\n\n\n\n\n\nIn your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nflight_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\nüí° Use the variable picker in Railway to reference your Postgres service directly for DATABASE_URL.\n\n\n\n\n\n\n\n\n\nGo to the Settings tab of the Web2DB Flights service.\nScroll to the Triggers section and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the type.\nEnter the schedule: */5 * * * *\n\nThis means: ‚ÄúRun every 5 minutes.‚Äù\nüß† Cron Format Reference: - */5 = every 5 minutes\n- Full format is minute hour day month weekday\nüìö Crontab Guru is a great tool to preview and test your expressions.\n\n\n\n\n\nAfter saving the environment variables and trigger, your ingestion service should begin collecting data every 5 minutes.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if not already connected).\nRun the following SQL query:\n\nSELECT * FROM flight_json_data ORDER BY timestamptz DESC LIMIT 10;\nIf data is being ingested correctly, you should see rows with timestamps and raw JSON.\nüõ†Ô∏è Troubleshooting Tips ‚Ä¢ Go to the Deployments tab to check if the latest deployment Completed successfully. ‚Ä¢ Check the Logs tab to view debug output (especially helpful if DEBUG=TRUE is set). ‚Ä¢ Review the Cron Triggers to make sure they‚Äôre running on schedule. ‚Ä¢ Double-check the SITE_URL and DATABASE_URL for typos or incorrect formatting.\n\n\n\n\n\nNow that your Web2DB Flights service is live and filling your database with raw flight data, we‚Äôll set up the companion Web2DB Weather service to do the same with meteorological data. Together, these two data streams will power our future correlation and transformation services.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#overview",
    "href": "pipelines/flight-ingestion-service.html#overview",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Flights ingestion service, which fetches live flight data from the OpenSky Network API and stores it in your Postgres database every five minutes.\nThis service will collect unstructured flight telemetry data and append it to the flight_json_data table in raw JSON format. You‚Äôll deploy it as a container on Railway using the lucascordova/web2db image and configure it to run on a schedule.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#using-public-apis-in-your-own-projects",
    "href": "pipelines/flight-ingestion-service.html#using-public-apis-in-your-own-projects",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "If you‚Äôre building your own ingestion service using a different API, here‚Äôs the general process:\n\nCheck if the API requires authentication (e.g., an API key or access token).\nRead the API documentation to understand query parameters, rate limits, and response structure.\nBuild your request URL including any necessary geographic filters, authentication tokens, or query strings.\nTest the request manually (e.g., in your browser or Postman) before deploying it inside a service like Railway.\n\nFor APIs that require keys or tokens, you should always inject these as environment variables, never hardcoded in your code or Docker image.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#about-the-opensky-api-url",
    "href": "pipelines/flight-ingestion-service.html#about-the-opensky-api-url",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "For this demo, we‚Äôre using the following public OpenSky endpoint: `https://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00‚Äô\n\n\n\nlamin and lamax: latitude min/max (45.08 to 45.88)\nlomin and lomax: longitude min/max (‚àí123.50 to ‚àí122.00)\n\nThis bounding box covers the Portland metropolitan area and surrounding airspace.\nüìö Learn more at:\nüîó OpenSky Network REST API Documentation",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#deploying-the-web2db-flights-service",
    "href": "pipelines/flight-ingestion-service.html#deploying-the-web2db-flights-service",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "In your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nflight_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\nüí° Use the variable picker in Railway to reference your Postgres service directly for DATABASE_URL.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#scheduling-the-ingestion",
    "href": "pipelines/flight-ingestion-service.html#scheduling-the-ingestion",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "Go to the Settings tab of the Web2DB Flights service.\nScroll to the Triggers section and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the type.\nEnter the schedule: */5 * * * *\n\nThis means: ‚ÄúRun every 5 minutes.‚Äù\nüß† Cron Format Reference: - */5 = every 5 minutes\n- Full format is minute hour day month weekday\nüìö Crontab Guru is a great tool to preview and test your expressions.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#testing-the-ingestion",
    "href": "pipelines/flight-ingestion-service.html#testing-the-ingestion",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "After saving the environment variables and trigger, your ingestion service should begin collecting data every 5 minutes.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if not already connected).\nRun the following SQL query:\n\nSELECT * FROM flight_json_data ORDER BY timestamptz DESC LIMIT 10;\nIf data is being ingested correctly, you should see rows with timestamps and raw JSON.\nüõ†Ô∏è Troubleshooting Tips ‚Ä¢ Go to the Deployments tab to check if the latest deployment Completed successfully. ‚Ä¢ Check the Logs tab to view debug output (especially helpful if DEBUG=TRUE is set). ‚Ä¢ Review the Cron Triggers to make sure they‚Äôre running on schedule. ‚Ä¢ Double-check the SITE_URL and DATABASE_URL for typos or incorrect formatting.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#whats-next",
    "href": "pipelines/flight-ingestion-service.html#whats-next",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "Now that your Web2DB Flights service is live and filling your database with raw flight data, we‚Äôll set up the companion Web2DB Weather service to do the same with meteorological data. Together, these two data streams will power our future correlation and transformation services.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html",
    "href": "pipelines/db-service.html",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "In this pipeline, the Postgres service functions as the central knowledge repository for all collected and transformed data. It plays a dual role:\n\nRaw Data Storage ‚Äì Capturing unstructured API responses from external ingestion services (e.g., OpenSky Network for flights, Open-Meteo for weather).\nStructured Relational Modeling ‚Äì Housing clean, normalized data models post-transformation‚Äîready for querying, visualization, and API exposure.\n\nAs we build a system to detect ‚Äúno-fly windows‚Äù based on weather thresholds over Portland, Postgres serves as the long-term memory of the pipeline. In this step, we‚Äôll deploy it using Railway and connect via Beekeeper Studio to validate connectivity and inspect future data.\n\n\n\n\nBefore proceeding, make sure you have:\n\nA GitHub account.\nA Railway account with at least Hobby tier access.\nThis requires a $5 credit deposit using a credit card.\nBeekeeper Studio installed on your computer.\nBeekeeper is an open-source desktop SQL client for inspecting and querying PostgreSQL databases.\n\n\n\n\n\n\n\n\nLog in at railway.app.\nClick ‚ÄúNew Project‚Äù.\nSelect ‚ÄúBlank Project‚Äù.\nName your project (e.g., no-fly-pipeline) and click ‚ÄúCreate Project‚Äù.\n\n\n\n\n\n\nIn your project dashboard, click ‚ÄúNew‚Äù to add a service.\nChoose ‚ÄúDatabase‚Äù, then click ‚ÄúPostgreSQL‚Äù.\nRailway will now deploy your Postgres service.\n\nOnce provisioned, this database becomes the shared backend for all services in your pipeline.\n\n\n\n\n\nClick into the Postgres service from the project dashboard.\nOpen the Variables tab.\nCopy the DATABASE_PUBLIC_URL ‚Äî it will look something like: postgresql://postgres:&lt;some-passsword&gt;@&lt;some-server&gt;.proxy.rlwy.net:20848/railway.\n\nThis URL is your access point for Beekeeper and other services that will connect to the database.\n\n\n\n\nYou‚Äôll use Beekeeper Studio to explore and verify your connection to the database.\n\n\nOpen the app and click ‚ÄúNew Connection‚Äù.\n\n\n\n\nChoose ‚ÄúPostgreSQL‚Äù from the connection type options.\nClick ‚ÄúImport from URL‚Äù (top-right or near the bottom of the connection screen).\nPaste the DATABASE_PUBLIC_URL from Railway.\nClick ‚ÄúConnect‚Äù.\n\n\n\n\nOnce connected, you‚Äôll see an empty database. That‚Äôs expected‚Äîyour ingestion and transformation services will populate it in later steps. For now, you can:\n\nTest SQL queries.\nMonitor schema evolution.\nVerify your database is reachable from your local environment.\n\n\n\n\n\n\nBefore data ingestion can begin, you need to create two tables in Postgres to hold the raw, unstructured API responses from the flight and weather ingestion services. These tables will serve as append-only logs that store the full JSON payloads alongside timestamps for later transformation.\nWe‚Äôll create:\n\nflight_json_data: stores data retrieved from the OpenSky Network.\nweather_json_data: stores data retrieved from the Open-Meteo API.\n\nEach table includes:\n\nid: an auto-incrementing primary key.\nraw_json: the unmodified API response stored as a jsonb object.\ntimestamptz: a timestamp indicating when the data was ingested.\n\n\n\nPaste the following SQL into Beekeeper Studio and execute it to create both tables:\nCREATE TABLE flight_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\nCREATE TABLE weather_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\n\n\n\n\n\nNow that your Postgres service is up and running, it‚Äôs ready to accept incoming unstructured data from your ingestion services: - ‚úàÔ∏è Web2DB Flights: gathers flight telemetry from the OpenSky Network. - üåßÔ∏è Web2DB Weather: gathers Portland weather conditions from Open-Meteo.\nThese services will write raw JSON payloads to the database every 5 minutes. Later, the DataTransform service will structure that data hourly into relational tables optimized for analysis.\nIn the next step, we‚Äôll configure the Web2DB Flights service to begin live ingestion.\nüìö References - https://docs.railway.app/guides/postgresql - https://www.beekeeperstudio.io/ - https://docs.railway.app/databases/external-access",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#overview",
    "href": "pipelines/db-service.html#overview",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "In this pipeline, the Postgres service functions as the central knowledge repository for all collected and transformed data. It plays a dual role:\n\nRaw Data Storage ‚Äì Capturing unstructured API responses from external ingestion services (e.g., OpenSky Network for flights, Open-Meteo for weather).\nStructured Relational Modeling ‚Äì Housing clean, normalized data models post-transformation‚Äîready for querying, visualization, and API exposure.\n\nAs we build a system to detect ‚Äúno-fly windows‚Äù based on weather thresholds over Portland, Postgres serves as the long-term memory of the pipeline. In this step, we‚Äôll deploy it using Railway and connect via Beekeeper Studio to validate connectivity and inspect future data.",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#prerequisites",
    "href": "pipelines/db-service.html#prerequisites",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "Before proceeding, make sure you have:\n\nA GitHub account.\nA Railway account with at least Hobby tier access.\nThis requires a $5 credit deposit using a credit card.\nBeekeeper Studio installed on your computer.\nBeekeeper is an open-source desktop SQL client for inspecting and querying PostgreSQL databases.",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#steps-to-set-up-the-postgres-service",
    "href": "pipelines/db-service.html#steps-to-set-up-the-postgres-service",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "Log in at railway.app.\nClick ‚ÄúNew Project‚Äù.\nSelect ‚ÄúBlank Project‚Äù.\nName your project (e.g., no-fly-pipeline) and click ‚ÄúCreate Project‚Äù.\n\n\n\n\n\n\nIn your project dashboard, click ‚ÄúNew‚Äù to add a service.\nChoose ‚ÄúDatabase‚Äù, then click ‚ÄúPostgreSQL‚Äù.\nRailway will now deploy your Postgres service.\n\nOnce provisioned, this database becomes the shared backend for all services in your pipeline.\n\n\n\n\n\nClick into the Postgres service from the project dashboard.\nOpen the Variables tab.\nCopy the DATABASE_PUBLIC_URL ‚Äî it will look something like: postgresql://postgres:&lt;some-passsword&gt;@&lt;some-server&gt;.proxy.rlwy.net:20848/railway.\n\nThis URL is your access point for Beekeeper and other services that will connect to the database.\n\n\n\n\nYou‚Äôll use Beekeeper Studio to explore and verify your connection to the database.\n\n\nOpen the app and click ‚ÄúNew Connection‚Äù.\n\n\n\n\nChoose ‚ÄúPostgreSQL‚Äù from the connection type options.\nClick ‚ÄúImport from URL‚Äù (top-right or near the bottom of the connection screen).\nPaste the DATABASE_PUBLIC_URL from Railway.\nClick ‚ÄúConnect‚Äù.\n\n\n\n\nOnce connected, you‚Äôll see an empty database. That‚Äôs expected‚Äîyour ingestion and transformation services will populate it in later steps. For now, you can:\n\nTest SQL queries.\nMonitor schema evolution.\nVerify your database is reachable from your local environment.\n\n\n\n\n\n\nBefore data ingestion can begin, you need to create two tables in Postgres to hold the raw, unstructured API responses from the flight and weather ingestion services. These tables will serve as append-only logs that store the full JSON payloads alongside timestamps for later transformation.\nWe‚Äôll create:\n\nflight_json_data: stores data retrieved from the OpenSky Network.\nweather_json_data: stores data retrieved from the Open-Meteo API.\n\nEach table includes:\n\nid: an auto-incrementing primary key.\nraw_json: the unmodified API response stored as a jsonb object.\ntimestamptz: a timestamp indicating when the data was ingested.\n\n\n\nPaste the following SQL into Beekeeper Studio and execute it to create both tables:\nCREATE TABLE flight_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\nCREATE TABLE weather_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#whats-next",
    "href": "pipelines/db-service.html#whats-next",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "Now that your Postgres service is up and running, it‚Äôs ready to accept incoming unstructured data from your ingestion services: - ‚úàÔ∏è Web2DB Flights: gathers flight telemetry from the OpenSky Network. - üåßÔ∏è Web2DB Weather: gathers Portland weather conditions from Open-Meteo.\nThese services will write raw JSON payloads to the database every 5 minutes. Later, the DataTransform service will structure that data hourly into relational tables optimized for analysis.\nIn the next step, we‚Äôll configure the Web2DB Flights service to begin live ingestion.\nüìö References - https://docs.railway.app/guides/postgresql - https://www.beekeeperstudio.io/ - https://docs.railway.app/databases/external-access",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  }
]