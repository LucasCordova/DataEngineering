[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Engineering Guides",
    "section": "",
    "text": "Welcome to the guide site for the Data Engineering course at Willamette University.\nThis site is used to share setup guides, deployment instructions, and project documentation to help you build and manage your data pipelines.\nStay tuned for:\n- üìò Step-by-step tutorials.\n- ‚öôÔ∏è Infrastructure walkthroughs.\n- üöÄ Deployment recipes for Railway, PostgreSQL, Grafana, PostgREST, and more.\nLet‚Äôs build pipelines that generate insight and impact. Happy hacking!"
  },
  {
    "objectID": "psql/performance.html#method-1-explain-analyze",
    "href": "psql/performance.html#method-1-explain-analyze",
    "title": "‚è± PostgreSQL Query Performance & Benchmarking",
    "section": "Method 1: EXPLAIN ANALYZE",
    "text": "Method 1: EXPLAIN ANALYZE\nEXPLAIN ANALYZE SELECT * FROM film WHERE title = 'ACADEMY DINOSAUR';\n\nShows the query plan and actual execution time\nHelps diagnose performance bottlenecks",
    "crumbs": [
      "Performance Tuning"
    ]
  },
  {
    "objectID": "psql/performance.html#method-2-iming-in-psql",
    "href": "psql/performance.html#method-2-iming-in-psql",
    "title": "‚è± PostgreSQL Query Performance & Benchmarking",
    "section": "Method 2: iming in psql",
    "text": "Method 2: iming in psql\nEnable timing:\n\\timing\nSELECT * FROM film WHERE title = 'ACADEMY DINOSAUR';",
    "crumbs": [
      "Performance Tuning"
    ]
  },
  {
    "objectID": "psql/performance.html#sample-query-to-benchmark",
    "href": "psql/performance.html#sample-query-to-benchmark",
    "title": "‚è± PostgreSQL Query Performance & Benchmarking",
    "section": "Sample Query to Benchmark",
    "text": "Sample Query to Benchmark\nSELECT * FROM customer WHERE last_name = 'SMITH';\nLet‚Äôs try with and without indexing to observe time difference.",
    "crumbs": [
      "Performance Tuning"
    ]
  },
  {
    "objectID": "psql/performance.html#create-an-index",
    "href": "psql/performance.html#create-an-index",
    "title": "‚è± PostgreSQL Query Performance & Benchmarking",
    "section": "Create an Index",
    "text": "Create an Index\nCREATE INDEX idx_customer_last_name ON customer(last_name);\nThen re-run:\nSELECT * FROM customer WHERE last_name = 'SMITH';",
    "crumbs": [
      "Performance Tuning"
    ]
  },
  {
    "objectID": "psql/performance.html#remove-the-index",
    "href": "psql/performance.html#remove-the-index",
    "title": "‚è± PostgreSQL Query Performance & Benchmarking",
    "section": "Remove the Index",
    "text": "Remove the Index\nDROP INDEX idx_customer_last_name;\nCompare the performance with and without the index.",
    "crumbs": [
      "Performance Tuning"
    ]
  },
  {
    "objectID": "psql/csv-copy-local-remote.html#windows-verifying-you-can-run-the-psql-command",
    "href": "psql/csv-copy-local-remote.html#windows-verifying-you-can-run-the-psql-command",
    "title": "üõü Copying Local CSV Files from Local to Remote System",
    "section": "Windows: Verifying You Can Run the psql Command",
    "text": "Windows: Verifying You Can Run the psql Command\n\nGo to a command prompt (cmd) or PowerShell and try to run:\npsql --version\nIf it returns a version, you‚Äôre good to go!\nIf the command fails, try these steps:\n\n\nOpen Windows Explorer and navigate to your PostgreSQL installation directory. This is usually in C:\\Program Files\\PostgreSQL\\&lt;version&gt;\\bin.\n\nLook for psql.exe in the bin folder.",
    "crumbs": [
      "Copying CSV Files Local to Remote"
    ]
  },
  {
    "objectID": "psql/csv-copy-local-remote.html#windows-adding-postgresql-to-path",
    "href": "psql/csv-copy-local-remote.html#windows-adding-postgresql-to-path",
    "title": "üõü Copying Local CSV Files from Local to Remote System",
    "section": "Windows: Adding PostgreSQL to PATH",
    "text": "Windows: Adding PostgreSQL to PATH\n\nIf you find it, add the path to your system‚Äôs PATH variable:\n - In the Start menu, type in **Environment Variables** and click it.\n - Click on **Environment Variables**.\n - Under **System Variables**, find `Path`, click **Edit**.\n - Click **New** and add the path to the `bin` folder.\n - Click **OK** to close all dialog boxes.\n - Restart your command prompt or PowerShell to apply changes.\nTry running psql --version again and ensure it returns a version number.",
    "crumbs": [
      "Copying CSV Files Local to Remote"
    ]
  },
  {
    "objectID": "psql/csv-copy-local-remote.html#mac-verifying-you-can-run-the-psql-command",
    "href": "psql/csv-copy-local-remote.html#mac-verifying-you-can-run-the-psql-command",
    "title": "üõü Copying Local CSV Files from Local to Remote System",
    "section": "Mac: Verifying You Can Run the psql Command",
    "text": "Mac: Verifying You Can Run the psql Command\n\nOpen a terminal and run:\npsql --version\nIf it returns a version, you‚Äôre good to go!\nIf the command fails, try these steps:\n\nAlternative 1 (using brew): 1 Open a terminal and run: bash      brew install postgresql 2 After installation, run: bash      brew services start postgresql 3 Try running psql --version again and ensure it returns a version number.",
    "crumbs": [
      "Copying CSV Files Local to Remote"
    ]
  },
  {
    "objectID": "psql/csv-copy-local-remote.html#mac-verifying-you-can-run-the-psql-command-alt",
    "href": "psql/csv-copy-local-remote.html#mac-verifying-you-can-run-the-psql-command-alt",
    "title": "üõü Copying Local CSV Files from Local to Remote System",
    "section": "Mac: Verifying You Can Run the psql Command Alt",
    "text": "Mac: Verifying You Can Run the psql Command Alt\n\nLocate your PostgreSQL installation directory. This is usually in /Library/PostgreSQL/&lt;version&gt;/bin.\nOpen your terminal and run:\n\nexport PATH=\"/Library/PostgreSQL/&lt;version&gt;/bin:$PATH\"\n\nTo make this change permanent:\n\nOpen your terminal and run (be sure to replace &lt;version&gt; with your installed version):\n\n\necho 'export PATH=\"/Library/PostgreSQL/&lt;version&gt;/bin:$PATH\"' &gt;&gt; ~/.bash_profile\nThen run:\nsource ~/.bash_profile\n\nTry running psql --version again and ensure it returns a version number.",
    "crumbs": [
      "Copying CSV Files Local to Remote"
    ]
  },
  {
    "objectID": "psql/csv-copy-local-remote.html#copying-csv-file-using-psql-on-windows",
    "href": "psql/csv-copy-local-remote.html#copying-csv-file-using-psql-on-windows",
    "title": "üõü Copying Local CSV Files from Local to Remote System",
    "section": "Copying CSV File Using psql on Windows",
    "text": "Copying CSV File Using psql on Windows\n\nIn your command prompt (cmd) or PowerShell, navigate to the directory where your .csv file is located (i.e.¬†cd ~\\\\Documents\\\\.).\nRun the following command:\n\npsql postgresql://postgres:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;dbname&gt; -c \"\\copy tablename FROM 'data.csv' WITH CSV HEADER\"\n\nIf successful, the command prints a success message indicating the number of rows imported.",
    "crumbs": [
      "Copying CSV Files Local to Remote"
    ]
  },
  {
    "objectID": "psql/csv-copy-local-remote.html#copying-csv-file-using-psql-on-mac",
    "href": "psql/csv-copy-local-remote.html#copying-csv-file-using-psql-on-mac",
    "title": "üõü Copying Local CSV Files from Local to Remote System",
    "section": "Copying CSV File Using psql on Mac",
    "text": "Copying CSV File Using psql on Mac\n\nIn your terminal, navigate to the directory where your .csv file is located (i.e.¬†cd ~/Documents/).\nRun the following command:\n\npsql postgresql://postgres:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;dbname&gt; -c \"\\copy books FROM 'data.csv' WITH CSV HEADER\"\n\nIf successful, the command prints a success message indicating the number of rows imported.",
    "crumbs": [
      "Copying CSV Files Local to Remote"
    ]
  },
  {
    "objectID": "psql/psql.html#dataset-used",
    "href": "psql/psql.html#dataset-used",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Dataset Used",
    "text": "Dataset Used\nFor this lecture, we use the Pagila database ‚Äî a PostgreSQL-compatible version of the classic Sakila DVD rental database.\nDownload the SQL file from Canvas to follow along.\nTo load it into Postgres: 1. Create a new database called pagila_dvd. 2. Execute the contents of the SQL file you downloaded."
  },
  {
    "objectID": "psql/psql.html#learning-objectives",
    "href": "psql/psql.html#learning-objectives",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand and create Views in PostgreSQL\nUnderstand and create User-Defined Functions\nUnderstand and create Stored Procedures\nUnderstand and create Triggers\nSee practical examples using the Pagila dataset"
  },
  {
    "objectID": "psql/psql.html#views-in-postgresql",
    "href": "psql/psql.html#views-in-postgresql",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Views in PostgreSQL",
    "text": "Views in PostgreSQL\nA View is a stored query that you can treat like a table.\nWhy Use Views?\n\nSimplify complex queries\nReuse SQL logic\nAdd a security layer\nEncapsulate business logic"
  },
  {
    "objectID": "psql/psql.html#creating-a-simple-view",
    "href": "psql/psql.html#creating-a-simple-view",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Creating a Simple View",
    "text": "Creating a Simple View\nExample: Top 5 Most Rented Films\nCREATE VIEW top_5_rented_films AS\nSELECT f.title, COUNT(r.rental_id) AS rental_count\nFROM film f\nJOIN inventory i ON f.film_id = i.film_id\nJOIN rental r ON i.inventory_id = r.inventory_id\nGROUP BY f.title\nORDER BY rental_count DESC\nLIMIT 5;\nYou can now query it like a table:\nSELECT * FROM top_5_rented_films;"
  },
  {
    "objectID": "psql/psql.html#updatable-views",
    "href": "psql/psql.html#updatable-views",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Updatable Views",
    "text": "Updatable Views\nViews are read-only unless:\n\nBased on a single table\nNo aggregates or GROUP BY\nNo DISTINCT, LIMIT, OFFSET, JOIN\n\nYou can make them updatable with INSTEAD OF triggers."
  },
  {
    "objectID": "psql/psql.html#functions-in-postgresql",
    "href": "psql/psql.html#functions-in-postgresql",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Functions in PostgreSQL",
    "text": "Functions in PostgreSQL\nUser-defined functions (UDFs) allow you to write reusable logic. They can return:\n\nScalar values\nTable results (set-returning functions)\n\nWritten in SQL or PL/pgSQL (PostgreSQL‚Äôs procedural language).\nExample: Scalar Function\nReturn number of films an actor has appeared in:\nCREATE FUNCTION film_count(actor_id INT) RETURNS INT AS $$\n  SELECT COUNT(*)\n  FROM film_actor\n  WHERE film_actor.actor_id = $1;\n$$ LANGUAGE SQL;\nUsage:\nSELECT film_count(1);\nExample: Table Function\nList all films rented by a customer:\nCREATE OR REPLACE FUNCTION customer_rentals(cust_id INT)\nRETURNS TABLE(title TEXT, rental_date TIMESTAMP) AS $$\nBEGIN\n  RETURN QUERY\n  SELECT f.title, r.rental_date::TIMESTAMP\n  FROM rental r\n  JOIN inventory i ON r.inventory_id = i.inventory_id\n  JOIN film f ON i.film_id = f.film_id\n  WHERE r.customer_id = cust_id;\nEND;\n$$ LANGUAGE plpgsql\nUsage:\nSELECT * FROM customer_rentals(1);"
  },
  {
    "objectID": "psql/psql.html#stored-procedures",
    "href": "psql/psql.html#stored-procedures",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Stored Procedures",
    "text": "Stored Procedures\nProcedures are like functions but:\n\nDo not return values\nSupport transactions (e.g., COMMIT/ROLLBACK)\n\nUseful for batch operations, data migrations, etc.\nExample: Stored Procedure\nLet‚Äôs use a Stored Procedure to store deleted customers.\nFirst, let‚Äôs create a customer_deletion_log table.\nCREATE TABLE customer_deletion_log (\n    customer_id SERIAL PRIMARY KEY,\n    deleted_at TIMESTAMP default NOW()\n);\nExample: Stored Procedure\nNow, let‚Äôs create a Stored Procedure to delete a customer and log the deletion:\nCREATE PROCEDURE delete_customer(cust_id INT)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n  DELETE FROM payment WHERE customer_id = cust_id;\n  DELETE FROM rental WHERE customer_id = cust_id;\n  DELETE FROM customer WHERE customer_id = cust_id;\n  INSERT INTO customer_deletion_log(customer_id, deleted_at)\n  VALUES (cust_id, NOW());\nEND;\n$$;\nCall it with:\nCALL delete_customer(1);"
  },
  {
    "objectID": "psql/psql.html#functions-vs-stored-procedures",
    "href": "psql/psql.html#functions-vs-stored-procedures",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Functions vs Stored Procedures",
    "text": "Functions vs Stored Procedures\nFeature | Function | Stored Procedure |\n||-|‚Äì| | Returns | Scalar or table data | No return (output via OUT params) | | Used in SQL | Yes (can be called in queries) | No (must use CALL) | | Transaction Control | No | Yes (can use COMMIT/ROLLBACK) | | Ideal Use Case | Computation, data transformation | Batch jobs, transactional workflows |\nWhen to Use Which?\n\nUse functions when you need return values or reusable logic inside queries.\nUse procedures for complex tasks that modify many rows, interact with multiple tables, or require transaction control."
  },
  {
    "objectID": "psql/psql.html#triggers-in-postgresql",
    "href": "psql/psql.html#triggers-in-postgresql",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Triggers in PostgreSQL",
    "text": "Triggers in PostgreSQL\nA trigger runs a function automatically when a specified event occurs on a table.\nEvents:\n\nBEFORE or AFTER\nINSERT, UPDATE, DELETE, or TRUNCATE"
  },
  {
    "objectID": "psql/psql.html#trigger-example-log-customer-deletion",
    "href": "psql/psql.html#trigger-example-log-customer-deletion",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Trigger Example: Log Customer Deletion",
    "text": "Trigger Example: Log Customer Deletion\nStep 1: Create Log Table (if you haven‚Äôt already)\nCREATE TABLE customer_deletion_log (\n  customer_id INT,\n  deleted_at TIMESTAMP DEFAULT NOW()\n);"
  },
  {
    "objectID": "psql/psql.html#create-trigger",
    "href": "psql/psql.html#create-trigger",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Create Trigger",
    "text": "Create Trigger\nStep 2: Create Trigger Function\nCREATE OR REPLACE FUNCTION log_customer_deletion()\nRETURNS TRIGGER AS $$\nBEGIN\n  INSERT INTO customer_deletion_log(customer_id, deleted_at)\n  VALUES (OLD.customer_id, NOW());\n  RETURN OLD;\nEND;\n$$ LANGUAGE plpgsql;"
  },
  {
    "objectID": "psql/psql.html#create-trigger-1",
    "href": "psql/psql.html#create-trigger-1",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Create Trigger",
    "text": "Create Trigger\nStep 3: Attach Trigger to customer Table\nCREATE TRIGGER trg_log_customer_delete\nAFTER DELETE ON customer\nFOR EACH ROW\nEXECUTE FUNCTION log_customer_deletion();\nTest It!\nDELETE FROM customer WHERE customer_id = 1;\nSELECT * FROM customer_deletion_log;"
  },
  {
    "objectID": "psql/psql.html#summary",
    "href": "psql/psql.html#summary",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Summary",
    "text": "Summary\n\nViews: Encapsulate reusable queries\nFunctions: Reusable logic that can return scalar/table data\nProcedures: Logic with transaction control, no return value\nTriggers: Automatically invoke logic on data events (e.g., inserts/deletes)"
  },
  {
    "objectID": "psql/psql.html#windows-verifying-you-can-run-the-psql-command",
    "href": "psql/psql.html#windows-verifying-you-can-run-the-psql-command",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Windows: Verifying You Can Run the psql Command",
    "text": "Windows: Verifying You Can Run the psql Command\n\nGo to a command prompt (cmd) or PowerShell and try to run:\npsql --version\nIf it returns a version, you‚Äôre good to go!\nIf the command fails, try these steps:\n\n\nOpen Windows Explorer and navigate to your PostgreSQL installation directory. This is usually in C:\\Program Files\\PostgreSQL\\&lt;version&gt;\\bin.\n\nLook for psql.exe in the bin folder."
  },
  {
    "objectID": "psql/psql.html#windows-adding-postgresql-to-path",
    "href": "psql/psql.html#windows-adding-postgresql-to-path",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Windows: Adding PostgreSQL to PATH",
    "text": "Windows: Adding PostgreSQL to PATH\n\nIf you find it, add the path to your system‚Äôs PATH variable:\n - In the Start menu, type in **Environment Variables** and click it.\n - Click on **Environment Variables**.\n - Under **System Variables**, find `Path`, click **Edit**.\n - Click **New** and add the path to the `bin` folder.\n - Click **OK** to close all dialog boxes.\n - Restart your command prompt or PowerShell to apply changes.\nTry running psql --version again and ensure it returns a version number."
  },
  {
    "objectID": "psql/psql.html#mac-verifying-you-can-run-the-psql-command",
    "href": "psql/psql.html#mac-verifying-you-can-run-the-psql-command",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Mac: Verifying You Can Run the psql Command",
    "text": "Mac: Verifying You Can Run the psql Command\n\nOpen a terminal and run:\npsql --version\nIf it returns a version, you‚Äôre good to go!\nIf the command fails, try these steps:\n\nAlternative 1 (using brew): 1 Open a terminal and run: bash      brew install postgresql 2 After installation, run: bash      brew services start postgresql 3 Try running psql --version again and ensure it returns a version number."
  },
  {
    "objectID": "psql/psql.html#mac-verifying-you-can-run-the-psql-command-alt",
    "href": "psql/psql.html#mac-verifying-you-can-run-the-psql-command-alt",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Mac: Verifying You Can Run the psql Command Alt",
    "text": "Mac: Verifying You Can Run the psql Command Alt\n\nLocate your PostgreSQL installation directory. This is usually in /Library/PostgreSQL/&lt;version&gt;/bin.\nOpen your terminal and run:\n\nexport PATH=\"/Library/PostgreSQL/&lt;version&gt;/bin:$PATH\"\n\nTo make this change permanent:\n\nOpen your terminal and run (be sure to replace &lt;version&gt; with your installed version):\n\n\necho 'export PATH=\"/Library/PostgreSQL/&lt;version&gt;/bin:$PATH\"' &gt;&gt; ~/.bash_profile\nThen run:\nsource ~/.bash_profile\n\nTry running psql --version again and ensure it returns a version number."
  },
  {
    "objectID": "psql/psql.html#copying-csv-file-using-psql-on-windows",
    "href": "psql/psql.html#copying-csv-file-using-psql-on-windows",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Copying CSV File Using psql on Windows",
    "text": "Copying CSV File Using psql on Windows\n\nIn your command prompt (cmd) or PowerShell, navigate to the directory where your .csv file is located (i.e.¬†cd ~\\\\Documents\\\\.).\nRun the following command:\n\npsql postgresql://postgres:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;dbname&gt; -c \"\\copy tablename FROM 'data.csv' WITH CSV HEADER\"\n\nIf successful, the command prints a success message indicating the number of rows imported."
  },
  {
    "objectID": "psql/psql.html#copying-csv-file-using-psql-on-mac",
    "href": "psql/psql.html#copying-csv-file-using-psql-on-mac",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Copying CSV File Using psql on Mac",
    "text": "Copying CSV File Using psql on Mac\n\nIn your terminal, navigate to the directory where your .csv file is located (i.e.¬†cd ~/Documents/).\nRun the following command:\n\npsql postgresql://postgres:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;dbname&gt; -c \"\\copy books FROM 'data.csv' WITH CSV HEADER\"\n\nIf successful, the command prints a success message indicating the number of rows imported."
  },
  {
    "objectID": "psql/psql.html#method-1-explain-analyze",
    "href": "psql/psql.html#method-1-explain-analyze",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Method 1: EXPLAIN ANALYZE",
    "text": "Method 1: EXPLAIN ANALYZE\nEXPLAIN ANALYZE SELECT * FROM film WHERE title = 'ACADEMY DINOSAUR';\n\nShows the query plan and actual execution time\nHelps diagnose performance bottlenecks"
  },
  {
    "objectID": "psql/psql.html#method-2-iming-in-psql",
    "href": "psql/psql.html#method-2-iming-in-psql",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Method 2: iming in psql",
    "text": "Method 2: iming in psql\nEnable timing:\n\\timing\nSELECT * FROM film WHERE title = 'ACADEMY DINOSAUR';"
  },
  {
    "objectID": "psql/psql.html#sample-query-to-benchmark",
    "href": "psql/psql.html#sample-query-to-benchmark",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Sample Query to Benchmark",
    "text": "Sample Query to Benchmark\nSELECT * FROM customer WHERE last_name = 'SMITH';\nLet‚Äôs try with and without indexing to observe time difference."
  },
  {
    "objectID": "psql/psql.html#create-an-index",
    "href": "psql/psql.html#create-an-index",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Create an Index",
    "text": "Create an Index\nCREATE INDEX idx_customer_last_name ON customer(last_name);\nThen re-run:\nSELECT * FROM customer WHERE last_name = 'SMITH';"
  },
  {
    "objectID": "psql/psql.html#remove-the-index",
    "href": "psql/psql.html#remove-the-index",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Remove the Index",
    "text": "Remove the Index\nDROP INDEX idx_customer_last_name;\nCompare the performance with and without the index."
  },
  {
    "objectID": "psql/psql.html#what-is-data-mining",
    "href": "psql/psql.html#what-is-data-mining",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "What is Data Mining?",
    "text": "What is Data Mining?\n\nThe process of discovering patterns in large datasets\nInvolves techniques from:\n\nStatistics\nMachine Learning\nPattern Recognition\n\nPostgreSQL provides extensible SQL querying capabilities perfect for early-stage data mining"
  },
  {
    "objectID": "psql/psql.html#the-pagila-database",
    "href": "psql/psql.html#the-pagila-database",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "The Pagila Database",
    "text": "The Pagila Database\n\nInspired by the Sakila database from MySQL\nContains:\n\nFilms, actors, and categories\nRentals and payments\nCustomers and their demographics"
  },
  {
    "objectID": "psql/psql.html#regular-expressions-in-postgresql",
    "href": "psql/psql.html#regular-expressions-in-postgresql",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Regular Expressions in PostgreSQL",
    "text": "Regular Expressions in PostgreSQL\n\n~ : match regex\n\n~* : match case-insensitive\n\n!~ : does not match regex\n\n!~* : does not match case-insensitive"
  },
  {
    "objectID": "psql/psql.html#examples",
    "href": "psql/psql.html#examples",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "psql/psql.html#case-insensitive-matching",
    "href": "psql/psql.html#case-insensitive-matching",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Case-Insensitive Matching",
    "text": "Case-Insensitive Matching"
  },
  {
    "objectID": "psql/psql.html#group-films-by-those-containing-love-vs-not",
    "href": "psql/psql.html#group-films-by-those-containing-love-vs-not",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Group films by those containing ‚Äòlove‚Äô vs not",
    "text": "Group films by those containing ‚Äòlove‚Äô vs not\nSELECT\n  CASE WHEN title ~* 'love' THEN 'Contains Love'\n       ELSE 'Does Not Contain Love' END AS love_category,\n  COUNT(*) AS film_count\nFROM film\nGROUP BY 1;"
  },
  {
    "objectID": "pipelines/api-developer-portal-service.html",
    "href": "pipelines/api-developer-portal-service.html",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "",
    "text": "Once your data is available through a RESTful interface (via PostgREST), the next step is to document it and make it easy for developers to discover and test. This is where Swagger UI comes in.\n\n\nSwagger UI is an open-source interface that visualizes APIs using the OpenAPI specification. It allows developers to: - üìñ Read documentation for your API endpoints. - üîé Explore input and output parameters. - üß™ Interactively test endpoints with live data. - ü§ù Share API functionality with frontend developers and integrators.\nSwagger connects seamlessly with PostgREST, which automatically serves an OpenAPI schema that Swagger can consume.\nüìö References: - Swagger UI Docs - PostgREST OpenAPI Support\n\n\n\n\n\n\n\n\nIn Railway, click Create ‚Üí New Service.\nSelect Docker Image.\nType in the image name:\nswaggerapi/swagger-ui\nPress Enter, then click Deploy.\n\n\n\n\n\nOnce the service is deployed:\n\nGo to the Environment Variables tab.\nClick New Variable and add:\n\n\n\n\nKey\nValue\n\n\n\n\nAPI_URL\n${{postgrest.PGRST_OPENAPI_SERVER_PROXY_URI}}\n\n\n\n\n‚úÖ This tells Swagger where to fetch the OpenAPI specification generated by PostgREST. It allows Swagger to automatically read and render your live API documentation.\n\n\n\n\n\nAfter setting the API_URL environment variable:\n\nClick Deploy again to apply the change.\n\n\n\n\n\n\nNavigate to the Settings tab of the Swagger service.\nUnder Networking, click Generate Domain.\nChoose the default port: 8080.\nClick Deploy.\n\nOnce deployment is complete, a public URL will appear.\n\n\n\n\n\nOpen the public URL you generated. If everything was configured correctly, you should see the Swagger UI load your API documentation automatically. You‚Äôll be able to:\n\nView your general_aviation_weather_view endpoint.\nClick to expand and see parameters, response schema, and sample data.\nRun test requests directly from the browser.\n\n\n\n\n\n\n‚úÖ Swagger UI provides a developer-friendly interface to your API.\n‚úÖ It connects automatically to PostgREST using the OpenAPI spec.\n‚úÖ Your live API is now self-documenting, testable, and ready to share.\n\nYou now have a complete pipeline: data ingestion ‚Üí transformation ‚Üí structured storage ‚Üí API ‚Üí developer docs.",
    "crumbs": [
      "Home",
      "7. API Developer Portal Service Setup"
    ]
  },
  {
    "objectID": "pipelines/api-developer-portal-service.html#why-create-an-api-developer-portal",
    "href": "pipelines/api-developer-portal-service.html#why-create-an-api-developer-portal",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "",
    "text": "Once your data is available through a RESTful interface (via PostgREST), the next step is to document it and make it easy for developers to discover and test. This is where Swagger UI comes in.\n\n\nSwagger UI is an open-source interface that visualizes APIs using the OpenAPI specification. It allows developers to: - üìñ Read documentation for your API endpoints. - üîé Explore input and output parameters. - üß™ Interactively test endpoints with live data. - ü§ù Share API functionality with frontend developers and integrators.\nSwagger connects seamlessly with PostgREST, which automatically serves an OpenAPI schema that Swagger can consume.\nüìö References: - Swagger UI Docs - PostgREST OpenAPI Support",
    "crumbs": [
      "Home",
      "7. API Developer Portal Service Setup"
    ]
  },
  {
    "objectID": "pipelines/api-developer-portal-service.html#step-by-step-deploy-swagger-in-railway",
    "href": "pipelines/api-developer-portal-service.html#step-by-step-deploy-swagger-in-railway",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "",
    "text": "In Railway, click Create ‚Üí New Service.\nSelect Docker Image.\nType in the image name:\nswaggerapi/swagger-ui\nPress Enter, then click Deploy.\n\n\n\n\n\nOnce the service is deployed:\n\nGo to the Environment Variables tab.\nClick New Variable and add:\n\n\n\n\nKey\nValue\n\n\n\n\nAPI_URL\n${{postgrest.PGRST_OPENAPI_SERVER_PROXY_URI}}\n\n\n\n\n‚úÖ This tells Swagger where to fetch the OpenAPI specification generated by PostgREST. It allows Swagger to automatically read and render your live API documentation.\n\n\n\n\n\nAfter setting the API_URL environment variable:\n\nClick Deploy again to apply the change.\n\n\n\n\n\n\nNavigate to the Settings tab of the Swagger service.\nUnder Networking, click Generate Domain.\nChoose the default port: 8080.\nClick Deploy.\n\nOnce deployment is complete, a public URL will appear.",
    "crumbs": [
      "Home",
      "7. API Developer Portal Service Setup"
    ]
  },
  {
    "objectID": "pipelines/api-developer-portal-service.html#test-the-swagger-ui",
    "href": "pipelines/api-developer-portal-service.html#test-the-swagger-ui",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "",
    "text": "Open the public URL you generated. If everything was configured correctly, you should see the Swagger UI load your API documentation automatically. You‚Äôll be able to:\n\nView your general_aviation_weather_view endpoint.\nClick to expand and see parameters, response schema, and sample data.\nRun test requests directly from the browser.",
    "crumbs": [
      "Home",
      "7. API Developer Portal Service Setup"
    ]
  },
  {
    "objectID": "pipelines/api-developer-portal-service.html#summary",
    "href": "pipelines/api-developer-portal-service.html#summary",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "",
    "text": "‚úÖ Swagger UI provides a developer-friendly interface to your API.\n‚úÖ It connects automatically to PostgREST using the OpenAPI spec.\n‚úÖ Your live API is now self-documenting, testable, and ready to share.\n\nYou now have a complete pipeline: data ingestion ‚Üí transformation ‚Üí structured storage ‚Üí API ‚Üí developer docs.",
    "crumbs": [
      "Home",
      "7. API Developer Portal Service Setup"
    ]
  },
  {
    "objectID": "pipelines/pipelines.html#overview-of-pipeline-stages",
    "href": "pipelines/pipelines.html#overview-of-pipeline-stages",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Overview of Pipeline Stages",
    "text": "Overview of Pipeline Stages\n\nDatabase Service Setup\nFlight Data Ingestion\nWeather Data Ingestion\nData Transformation\nDashboard Service\nDB/API Bridge\nAPI Developer Portal"
  },
  {
    "objectID": "pipelines/pipelines.html#overview",
    "href": "pipelines/pipelines.html#overview",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Overview",
    "text": "Overview\nIn this pipeline, the Postgres service functions as the central knowledge repository for all collected and transformed data. It plays a dual role:\n\nRaw Data Storage ‚Äì Capturing unstructured API responses from external ingestion services (e.g., OpenSky Network for flights, Open-Meteo for weather).\nStructured Relational Modeling ‚Äì Housing clean, normalized data models post-transformation‚Äîready for querying, visualization, and API exposure."
  },
  {
    "objectID": "pipelines/pipelines.html#overview-continued",
    "href": "pipelines/pipelines.html#overview-continued",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Overview Continued",
    "text": "Overview Continued\nAs we build a system to detect ‚Äúno-fly windows‚Äù based on weather thresholds over Portland, Postgres serves as the long-term memory of the pipeline. In this step, we‚Äôll deploy it using Railway and connect via Beekeeper Studio to validate connectivity and inspect future data."
  },
  {
    "objectID": "pipelines/pipelines.html#prerequisites",
    "href": "pipelines/pipelines.html#prerequisites",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore proceeding, make sure you have:\n\nA GitHub account.\nA Railway account with at least Hobby tier access.\nThis requires a $5 credit deposit using a credit card.\nBeekeeper Studio installed on your computer.\nBeekeeper is an open-source desktop SQL client for inspecting and querying PostgreSQL databases."
  },
  {
    "objectID": "pipelines/pipelines.html#steps-to-set-up-the-postgres-service",
    "href": "pipelines/pipelines.html#steps-to-set-up-the-postgres-service",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Steps to Set Up the Postgres Service",
    "text": "Steps to Set Up the Postgres Service\nCreate a New Project in Railway\n\nLog in at railway.app.\nClick ‚ÄúNew Project‚Äù.\nSelect ‚ÄúBlank Project‚Äù.\nName your project (e.g., no-fly-pipeline) and click ‚ÄúCreate Project‚Äù."
  },
  {
    "objectID": "pipelines/pipelines.html#raw-json-storage-continued",
    "href": "pipelines/pipelines.html#raw-json-storage-continued",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "RAW JSON Storage Continued",
    "text": "RAW JSON Storage Continued\nEach table includes:\n\nid: an auto-incrementing primary key.\nraw_json: the unmodified API response stored as a jsonb object.\ntimestamptz: a timestamp indicating when the data was ingested.\n\nCreate Tables Using SQL\nPaste the following SQL into Beekeeper Studio and execute it to create both tables:\nCREATE TABLE flight_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\nCREATE TABLE weather_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);"
  },
  {
    "objectID": "pipelines/pipelines.html#whats-next",
    "href": "pipelines/pipelines.html#whats-next",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nNow that your Postgres service is up and running, it‚Äôs ready to accept incoming unstructured data from your ingestion services: - ‚úàÔ∏è Web2DB Flights: gathers flight telemetry from the OpenSky Network. - üåßÔ∏è Web2DB Weather: gathers Portland weather conditions from Open-Meteo.\nThese services will write raw JSON payloads to the database every 5 minutes. Later, the DataTransform service will structure that data hourly into relational tables optimized for analysis.\nIn the next step, we‚Äôll configure the Web2DB Flights service to begin live ingestion.\nüìö References - https://docs.railway.app/guides/postgresql - https://www.beekeeperstudio.io/ - https://docs.railway.app/databases/external-access"
  },
  {
    "objectID": "pipelines/pipelines.html#overview-1",
    "href": "pipelines/pipelines.html#overview-1",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Overview",
    "text": "Overview\nIn this step, you‚Äôll set up the Web2DB Flights ingestion service, which fetches live flight data from the OpenSky Network API and stores it in your Postgres database every five minutes.\nThis service will collect unstructured flight telemetry data and append it to the flight_json_data table in raw JSON format. You‚Äôll deploy it as a container on Railway using the lucascordova/web2db image and configure it to run on a schedule."
  },
  {
    "objectID": "pipelines/pipelines.html#using-public-apis-in-your-own-projects",
    "href": "pipelines/pipelines.html#using-public-apis-in-your-own-projects",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Using Public APIs in Your Own Projects",
    "text": "Using Public APIs in Your Own Projects\nIf you‚Äôre building your own ingestion service using a different API, here‚Äôs the general process:\n\nCheck if the API requires authentication (e.g., an API key or access token).\nRead the API documentation to understand query parameters, rate limits, and response structure.\nBuild your request URL including any necessary geographic filters, authentication tokens, or query strings.\nTest the request manually (e.g., in your browser or Postman) before deploying it inside a service like Railway."
  },
  {
    "objectID": "pipelines/pipelines.html#using-public-apis-continued",
    "href": "pipelines/pipelines.html#using-public-apis-continued",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Using Public APIs Continued",
    "text": "Using Public APIs Continued\nFor APIs that require keys or tokens, you should always inject these as environment variables, never hardcoded in your code or Docker image."
  },
  {
    "objectID": "pipelines/pipelines.html#about-the-opensky-api-url",
    "href": "pipelines/pipelines.html#about-the-opensky-api-url",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "About the OpenSky API URL",
    "text": "About the OpenSky API URL\nFor this demo, we‚Äôre using the following public OpenSky endpoint: `https://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00‚Äô\nüó∫Ô∏è URL Breakdown:\n\nlamin and lamax: latitude min/max (45.08 to 45.88)\nlomin and lomax: longitude min/max (‚àí123.50 to ‚àí122.00)\n\nThis bounding box covers the Portland metropolitan area and surrounding airspace.\nüìö Learn more at:\nüîó OpenSky Network REST API Documentation"
  },
  {
    "objectID": "pipelines/pipelines.html#deploying-the-web2db-flights-service",
    "href": "pipelines/pipelines.html#deploying-the-web2db-flights-service",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Deploying the Web2DB Flights Service",
    "text": "Deploying the Web2DB Flights Service\nAdd the Docker Service in Railway\n\nIn your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy."
  },
  {
    "objectID": "pipelines/pipelines.html#scheduling-the-ingestion",
    "href": "pipelines/pipelines.html#scheduling-the-ingestion",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Scheduling the Ingestion",
    "text": "Scheduling the Ingestion\nSet Up Cron Trigger\n\nGo to the Settings tab of the Web2DB Flights service.\nScroll to the Triggers section and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the type.\nEnter the schedule: */5 * * * *\n\nThis means: ‚ÄúRun every 5 minutes.‚Äù\nüß† Cron Format Reference: - */5 = every 5 minutes\n- Full format is minute hour day month weekday\nüìö Crontab Guru is a great tool to preview and test your expressions."
  },
  {
    "objectID": "pipelines/pipelines.html#testing-the-ingestion",
    "href": "pipelines/pipelines.html#testing-the-ingestion",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Testing the Ingestion",
    "text": "Testing the Ingestion\nAfter saving the environment variables and trigger, your ingestion service should begin collecting data every 5 minutes.\n‚úÖ Verify the Data is Flowing\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if not already connected).\nRun the following SQL query:\n\nSELECT * FROM flight_json_data ORDER BY timestamptz DESC LIMIT 10;\nIf data is being ingested correctly, you should see rows with timestamps and raw JSON.\nüõ†Ô∏è Troubleshooting Tips ‚Ä¢ Go to the Deployments tab to check if the latest deployment Completed successfully. ‚Ä¢ Check the Logs tab to view debug output (especially helpful if DEBUG=TRUE is set). ‚Ä¢ Review the Cron Triggers to make sure they‚Äôre running on schedule. ‚Ä¢ Double-check the SITE_URL and DATABASE_URL for typos or incorrect formatting."
  },
  {
    "objectID": "pipelines/pipelines.html#whats-next-1",
    "href": "pipelines/pipelines.html#whats-next-1",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nNow that your Web2DB Flights service is live and filling your database with raw flight data, we‚Äôll set up the companion Web2DB Weather service to do the same with meteorological data. Together, these two data streams will power our future correlation and transformation services."
  },
  {
    "objectID": "pipelines/pipelines.html#overview-2",
    "href": "pipelines/pipelines.html#overview-2",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Overview",
    "text": "Overview\nIn this step, you‚Äôll set up the Web2DB Weather ingestion service, which fetches live weather data for the Portland area from the Open-Meteo API and stores it in your Postgres database every five minutes.\nThis service collects unstructured weather readings‚Äîlike temperature and wind speed‚Äîand writes them to the weather_json_data table as raw JSON for later transformation and analysis. The data will later be joined with flight data to investigate correlations such as weather-driven no-fly windows."
  },
  {
    "objectID": "pipelines/pipelines.html#about-the-open-meteo-api-url",
    "href": "pipelines/pipelines.html#about-the-open-meteo-api-url",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "About the Open-Meteo API URL",
    "text": "About the Open-Meteo API URL\nFor this demo, we‚Äôre using the following Open-Meteo endpoint:\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\nüåê URL Breakdown:\n\nlatitude=45.52 and longitude=-122.68 pinpoint downtown Portland.\ncurrent=... specifies the current weather variables to return:\n\ntemperature_2m: air temperature 2 meters above the ground.\nwind_speed_10m: wind speed 10 meters above the ground.\nweathercode: numeric weather condition code (e.g., fog, rain, snow).\n\n\n\n\n\n### üìö Learn more at:\n\n\nüîó Open-Meteo API Docs"
  },
  {
    "objectID": "pipelines/pipelines.html#deploying-the-web2db-weather-service",
    "href": "pipelines/pipelines.html#deploying-the-web2db-weather-service",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Deploying the Web2DB Weather Service",
    "text": "Deploying the Web2DB Weather Service\nAdd the Docker Service in Railway\n\nIn your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy."
  },
  {
    "objectID": "pipelines/pipelines.html#scheduling-the-ingestion-1",
    "href": "pipelines/pipelines.html#scheduling-the-ingestion-1",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Scheduling the Ingestion",
    "text": "Scheduling the Ingestion\nSet Up Cron Trigger\n\nGo to the Settings tab of the Web2DB Weather service.\nScroll to Triggers and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the trigger type.\nUse this schedule: */5 * * * *\n\nThis means the service will run every 5 minutes.\nüìö Reference: Crontab Guru ‚Äî an easy tool to preview cron expressions."
  },
  {
    "objectID": "pipelines/pipelines.html#testing-the-ingestion-1",
    "href": "pipelines/pipelines.html#testing-the-ingestion-1",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Testing the Ingestion",
    "text": "Testing the Ingestion\nOnce your cron trigger and environment variables are saved, the service will begin pulling weather data.\n‚úÖ Verify the Data\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if you haven‚Äôt already).\nRun:\n\nSELECT * FROM weather_json_data ORDER BY timestamptz DESC LIMIT 10;\nYou should see recent entries showing full JSON responses from the Open-Meteo API."
  },
  {
    "objectID": "pipelines/pipelines.html#troubleshooting-tips",
    "href": "pipelines/pipelines.html#troubleshooting-tips",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "üõ†Ô∏è Troubleshooting Tips",
    "text": "üõ†Ô∏è Troubleshooting Tips\n- Check the Deployments tab in Railway to confirm successful runs.\n- Use the Logs tab to inspect real-time debug output.\n- Ensure the SITE_URL is formatted correctly (with no line breaks or spaces).\n- If the table is still empty after 5‚Äì10 minutes, double-check the DATABASE_URL, TABLE_NAME, and cron settings."
  },
  {
    "objectID": "pipelines/pipelines.html#whats-next-2",
    "href": "pipelines/pipelines.html#whats-next-2",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nWith both Web2DB Flights and Web2DB Weather streaming live data into your Postgres instance, you‚Äôre ready to build the DataTransform service that will process this raw data into structured, relational models‚Äîmaking it easier to perform analytics and build dashboards."
  },
  {
    "objectID": "pipelines/pipelines.html#overview-3",
    "href": "pipelines/pipelines.html#overview-3",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Overview",
    "text": "Overview\nIn this section, you‚Äôll design and deploy a transformation service called DataTransform. This service will extract meaningful fields from your raw JSON flight and weather data and insert them into clean, structured tables in your Postgres database.\nThis transformation process involves: - Understanding the structure of raw JSON. - Designing normalized physical tables for analysis. - Writing SQL to extract values using Postgres JSON functions. - Automating the transformation with a scheduled job in Railway."
  },
  {
    "objectID": "pipelines/pipelines.html#understand-the-json-structure",
    "href": "pipelines/pipelines.html#understand-the-json-structure",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Understand the JSON Structure",
    "text": "Understand the JSON Structure\nBefore transforming the data, you must understand what the raw JSON looks like. You can do this by running:\nSELECT raw_json FROM flight_json_data ORDER BY timestamptz DESC LIMIT 1;\nSELECT raw_json FROM weather_json_data ORDER BY timestamptz DESC LIMIT 1;\nExplore the shape and nesting of each payload. For example: - OpenSky states is an array of arrays. OpenSky JSON Structure - Open-Meteo JSON is a nested object. Open-Meteo Example\nüìö Reference: PostgreSQL JSON Functions and Operators"
  },
  {
    "objectID": "pipelines/pipelines.html#design-your-physical-schema",
    "href": "pipelines/pipelines.html#design-your-physical-schema",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Design Your Physical Schema",
    "text": "Design Your Physical Schema\nBased on the JSON contents, you‚Äôll need normalized tables to store flight and weather data. Here‚Äôs the schema you‚Äôll use:\nCREATE TABLE flight_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE weather_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Structured output tables\nCREATE TABLE flights (\n    id SERIAL PRIMARY KEY,\n    icao24 VARCHAR(10) NOT NULL,\n    callsign VARCHAR(10),\n    country VARCHAR(64),\n    latitude DOUBLE PRECISION,\n    longitude DOUBLE PRECISION,\n    altitude_meters DOUBLE PRECISION,\n    velocity_knots DOUBLE PRECISION,\n    heading_degrees DOUBLE PRECISION,\n    vertical_rate DOUBLE PRECISION,\n    timestamp TIMESTAMPTZ NOT NULL\n);\n\nCREATE TABLE weather_observations (\n    id SERIAL PRIMARY KEY,\n    latitude DOUBLE PRECISION NOT NULL,\n    longitude DOUBLE PRECISION NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL,\n    precipitation_mm DOUBLE PRECISION,\n    weathercode SMALLINT\n);\n\nCREATE TABLE weather_condition (\n    code SMALLINT PRIMARY KEY,\n    description TEXT\n);\nThese tables will be the targets of your transformation logic."
  },
  {
    "objectID": "pipelines/pipelines.html#write-sql-to-transform-the-data",
    "href": "pipelines/pipelines.html#write-sql-to-transform-the-data",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Write SQL to Transform the Data",
    "text": "Write SQL to Transform the Data\nWhat‚Äôs a Common Table Expression?\nA Common Table Expression (CTE) is a temporary result set defined at the beginning of a SQL statement. It helps modularize complex queries and allows you to: - Break down a multi-step transformation into readable pieces. - Reference intermediate results multiple times.\nüìö Reference: CTEs in PostgreSQL"
  },
  {
    "objectID": "pipelines/pipelines.html#example-transformation-for-flights",
    "href": "pipelines/pipelines.html#example-transformation-for-flights",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Example Transformation for Flights",
    "text": "Example Transformation for Flights\nBEGIN;\n\nWITH raw AS (\n    SELECT id, timestamptz, jsonb_array_elements(raw_json-&gt;'states') AS state\n    FROM flight_json_data\n),\nparsed AS (\n    SELECT\n        id,\n        state-&gt;&gt;0 AS icao24,\n        state-&gt;&gt;1 AS callsign,\n        state-&gt;&gt;2 AS country,\n        (state-&gt;&gt;6)::DOUBLE PRECISION AS longitude,\n        (state-&gt;&gt;5)::DOUBLE PRECISION AS latitude,\n        (state-&gt;&gt;7)::DOUBLE PRECISION AS altitude_meters,\n        (state-&gt;&gt;9)::DOUBLE PRECISION AS velocity_knots,\n        (state-&gt;&gt;10)::DOUBLE PRECISION AS heading_degrees,\n        (state-&gt;&gt;11)::DOUBLE PRECISION AS vertical_rate,\n        timestamptz\n    FROM raw\n)\nINSERT INTO flights (\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamp\n)\nSELECT\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamptz\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM flight_json_data;\n\nCOMMIT;"
  },
  {
    "objectID": "pipelines/pipelines.html#example-transformation-for-weather",
    "href": "pipelines/pipelines.html#example-transformation-for-weather",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Example Transformation for Weather",
    "text": "Example Transformation for Weather\nBEGIN;\n\nWITH raw AS (\n    SELECT id, raw_json, timestamptz\n    FROM weather_json_data\n),\nparsed AS (\n    SELECT\n        (raw_json-&gt;'latitude')::DOUBLE PRECISION AS latitude,\n        (raw_json-&gt;'longitude')::DOUBLE PRECISION AS longitude,\n        (raw_json-&gt;'current'-&gt;'precipitation')::DOUBLE PRECISION AS precipitation_mm,\n        (raw_json-&gt;'current'-&gt;'weathercode')::SMALLINT AS weathercode,\n        timestamptz AS timestamp\n    FROM raw\n)\nINSERT INTO weather_observations (\n    latitude, longitude, precipitation_mm, weathercode, timestamp\n)\nSELECT\n    latitude, longitude, precipitation_mm, weathercode, timestamp\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM weather_json_data;\n\nCOMMIT;"
  },
  {
    "objectID": "pipelines/pipelines.html#use-the-github-template",
    "href": "pipelines/pipelines.html#use-the-github-template",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Use the GitHub Template",
    "text": "Use the GitHub Template\n1.  [Use the db_transform GitHub template](https://github.com/LucasCordova/db_transform).\n2.  Click the ‚ÄúUse this template‚Äù button.\n3.  Name your repository: `weather_flight_db_transform`\n4.  Choose your GitHub account and click Create Repository."
  },
  {
    "objectID": "pipelines/pipelines.html#add-your-sql-to-clean.sql",
    "href": "pipelines/pipelines.html#add-your-sql-to-clean.sql",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Add Your SQL to clean.sql",
    "text": "Add Your SQL to clean.sql\n1.  Open the new repository on GitHub (recommended) or clone it locally.\n2.  Edit the clean.sql file. If you are editing in the browser, choose the `pencil` icon. Paste your flight and weather transformation SQL inside it.\n3.  Save the file and commit. In the browser editor, click the Commit changes... button, add a message, and click the final Commit changes button."
  },
  {
    "objectID": "pipelines/pipelines.html#deploy-the-service-in-railway",
    "href": "pipelines/pipelines.html#deploy-the-service-in-railway",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Deploy the Service in Railway",
    "text": "Deploy the Service in Railway\n1.  Go to your Railway project.\n2.  Click New ‚Üí Deploy from GitHub Repo.\n3.  Click ‚ÄúConfigure GitHub App‚Äù to give Railway access to your GitHub account.\n4.  Choose the repository weather_flight_db_transform.\n5.  Add the following environment variables in the Variables tab. \n\n\n\nKey\nValue\n\n\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\n\n6.  Click Deploy."
  },
  {
    "objectID": "pipelines/pipelines.html#set-cron-schedule",
    "href": "pipelines/pipelines.html#set-cron-schedule",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Set Cron Schedule",
    "text": "Set Cron Schedule\n1.  Go to the Settings tab of the DataTransform service.\n2.  Scroll to Triggers ‚Üí New Trigger.\n3.  Choose Cron and enter: `0 * * * *`.\nThis means the service runs once every hour, at the top of the hour."
  },
  {
    "objectID": "pipelines/pipelines.html#testing-and-validation",
    "href": "pipelines/pipelines.html#testing-and-validation",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Testing and Validation",
    "text": "Testing and Validation\n‚úÖ Check the Tables in Beekeeper\n\n    After the first cron run, inspect the structured tables:\nSELECT * FROM flights ORDER BY timestamp DESC LIMIT 10;\nSELECT * FROM weather_observations ORDER BY timestamp DESC LIMIT 10;\n‚úÖ View Logs in Railway\n    Use the Logs tab to inspect any errors or debug output."
  },
  {
    "objectID": "pipelines/pipelines.html#whats-next-3",
    "href": "pipelines/pipelines.html#whats-next-3",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nNow that your structured data is flowing, you can visualize and analyze it in the next step using Grafana Dashboards. You‚Äôll use SQL queries to explore patterns in flight activity and weather data‚Äîlaying the foundation for answering your core research question."
  },
  {
    "objectID": "pipelines/pipelines.html#but-first-views",
    "href": "pipelines/pipelines.html#but-first-views",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "üåÅ But first, Views",
    "text": "üåÅ But first, Views\nA View in Postgres is a virtual table based on a SQL SELECT query. It behaves just like a table for reading, but doesn‚Äôt store any data itself‚Äîit simply runs the query behind it whenever it‚Äôs accessed.\nViews are excellent for dashboards because:\n\n‚úÖ They abstract complex queries behind a simple name.\n‚úÖ They can be reused by multiple clients (like Grafana).\n‚úÖ You can restrict access to views rather than raw tables using Postgres roles.\n‚úÖ They allow you to pre-join and pre-aggregate data in a way that‚Äôs efficient and secure for visualization."
  },
  {
    "objectID": "pipelines/pipelines.html#create-a-view-for-the-dashboard",
    "href": "pipelines/pipelines.html#create-a-view-for-the-dashboard",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Create a View for the Dashboard",
    "text": "Create a View for the Dashboard\nLet‚Äôs create a view that answers our research question: &gt; Are there ‚Äúno-fly windows‚Äù correlated with weather thresholds?\nThis view joins structured flight and weather data to support correlation queries:\nCREATE OR REPLACE VIEW flight_weather AS\nSELECT\n    f.timestamp,\n    f.callsign,\n    f.altitude_meters,\n    f.velocity_knots,\n    f.latitude AS flight_latitude,\n    f.longitude AS flight_longitude,\n    w.precipitation_mm,\n    w.weathercode,\n    w.latitude AS weather_latitude,\n    w.longitude AS weather_longitude\nFROM\n    flights f\nJOIN\n    weather_observations w\nON\n    date_trunc('minute', f.timestamp) = date_trunc('minute', w.timestamp)\nORDER BY\n    f.timestamp DESC;"
  },
  {
    "objectID": "pipelines/pipelines.html#deploying-grafana-in-railway",
    "href": "pipelines/pipelines.html#deploying-grafana-in-railway",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Deploying Grafana in Railway",
    "text": "Deploying Grafana in Railway\nAdd the Grafana Template\n\nIn your Railway project, click ‚ÄúCreate‚Äù ‚Üí ‚ÄúTemplate‚Äù.\nType Grafana in the search bar.\nSelect the version provided by Andre Lademann‚Äôs Projects.\nAdd the following environment variables:\n\n\n\nKey\nValue\n\n\n\n\nGF_SECURITY_ADMIN_USER\ngrafanareader\n\n\nGF_DEFAULT_INSTANCE_NAME\ngrafanapg\n\n\nGF_SECURITY_ADMIN_PASSWORD\nyour-password\n\n\n\nLeave the 4 pre-configured environment variables as they are.\nOnce you‚Äôve entered the variables, the service will check connectivity and display a message saying ‚ÄúReady to be deployed‚Äù.\nClick [Deploy]\nThis will start the deployment process. It may take a few minutes."
  },
  {
    "objectID": "pipelines/pipelines.html#add-a-grafana-specific-db-user",
    "href": "pipelines/pipelines.html#add-a-grafana-specific-db-user",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Add a Grafana-Specific DB User",
    "text": "Add a Grafana-Specific DB User\nTo avoid giving Grafana full access to your database, we‚Äôll create a read-only user that can only access the view.\nSteps:\n\nOpen your Postgres database in Beekeeper Studio.\nRun the following SQL (replace 'password' with one you‚Äôll remember):\n\nCREATE USER grafanareader WITH PASSWORD 'your_password';\nGRANT USAGE ON SCHEMA \"public\" TO grafanareader;\nGRANT SELECT ON \"public\".flight_weather TO grafanareader;"
  },
  {
    "objectID": "pipelines/pipelines.html#connect-grafana-to-postgres",
    "href": "pipelines/pipelines.html#connect-grafana-to-postgres",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Connect Grafana to Postgres",
    "text": "Connect Grafana to Postgres\nCopy PGHOST from Railway\n\nGo to your Postgres service in Railway.\nUnder Environment Variables, copy the value for PGHOST.\nIt will look like: postgres.railway.internal."
  },
  {
    "objectID": "pipelines/pipelines.html#create-your-first-dashboard-in-grafana",
    "href": "pipelines/pipelines.html#create-your-first-dashboard-in-grafana",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Create Your First Dashboard in Grafana",
    "text": "Create Your First Dashboard in Grafana\n\nGo back to the Home page in Grafana.\nClick ‚ÄúCreate your first dashboard‚Äù.\nClick ‚ÄúAdd Visualization‚Äù.\n\nConfigure the Panel\n\nChoose Data Source: Select the PostgreSQL source you just set up.\nFrom Table: Select the flight_weather view.\nAdd Columns: Choose at least one time-based column (timestamp) and one data column (e.g., altitude_meters or precipitation_mm).\nClick Run Query.\nUse Table Mode or switch to a chart via Suggested Visualizations.\n\nYou now have a live dashboard pulling from your structured data pipeline!"
  },
  {
    "objectID": "pipelines/pipelines.html#whats-next-4",
    "href": "pipelines/pipelines.html#whats-next-4",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nFrom here, you can:\n\nAdd filters to segment by weathercode.\nAggregate values like average flight altitude during rain conditions.\nBuild a full visualization of ‚Äúno-fly‚Äù windows over time.\n\nIn the next step we‚Äôll expose your view as a REST API using PostgREST and document it using Swagger."
  },
  {
    "objectID": "pipelines/pipelines.html#why-set-up-a-db-api-bridge",
    "href": "pipelines/pipelines.html#why-set-up-a-db-api-bridge",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Why Set Up a DB API Bridge?",
    "text": "Why Set Up a DB API Bridge?\nOnce your structured data is ready for exploration and integration, the next step is to expose it via an API. This allows your data to be accessed securely by dashboards, external applications, or developer tools. Instead of building a custom REST API, we can use PostgREST, which automates this process by generating RESTful endpoints directly from your PostgreSQL schema."
  },
  {
    "objectID": "pipelines/pipelines.html#what-is-postgrest",
    "href": "pipelines/pipelines.html#what-is-postgrest",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "What is PostgREST?",
    "text": "What is PostgREST?\nPostgREST is a lightweight, open-source tool that turns your PostgreSQL database into a secure RESTful API. With PostgREST, your views and tables become accessible via HTTP without writing backend code.\nKey Features:\n\nüöÄ Generates REST endpoints for tables, views, and stored procedures.\nüîê Uses PostgreSQL‚Äôs native roles and permissions for access control.\nüìä Perfect for exposing analytic views to tools like Swagger, frontend apps, and more."
  },
  {
    "objectID": "pipelines/pipelines.html#step-by-step-deploy-postgrest-in-railway",
    "href": "pipelines/pipelines.html#step-by-step-deploy-postgrest-in-railway",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Step-by-Step: Deploy PostgREST in Railway",
    "text": "Step-by-Step: Deploy PostgREST in Railway\nCreate the Service in Railway\n\nClick Create in your Railway project.\nChoose Docker Image.\nEnter the Docker image name:\npostgrest/postgrest\nClick Deploy."
  },
  {
    "objectID": "pipelines/pipelines.html#set-up-database-permissions",
    "href": "pipelines/pipelines.html#set-up-database-permissions",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Set Up Database Permissions",
    "text": "Set Up Database Permissions\nOpen Beekeeper Studio and execute the following SQL:\n-- Create a non-login role to define anonymous API access\nCREATE ROLE web_anon NOLOGIN;\n\n-- Allow that role to access the \"api\" schema\nGRANT USAGE ON SCHEMA api TO web_anon;\n\n-- Limit access only to this view\nGRANT SELECT ON api.general_aviation_weather_view TO web_anon;\nThis ensures PostgREST can only read from a specific schema (api) and specific view (general_aviation_weather_view). It does not grant access to other tables or schemas."
  },
  {
    "objectID": "pipelines/pipelines.html#create-a-dedicated-authenticator-role",
    "href": "pipelines/pipelines.html#create-a-dedicated-authenticator-role",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Create a Dedicated Authenticator Role",
    "text": "Create a Dedicated Authenticator Role\nRather than using the powerful postgres role to connect from PostgREST, we create a separate authenticator user that can assume the web_anon role. This is more secure and scalable.\nCREATE ROLE authenticator NOINHERIT LOGIN PASSWORD '&lt;your password&gt;';\nGRANT web_anon TO authenticator;\nWhy This Matters:\n\nNOINHERIT: the authenticator role cannot directly access anything.\nLOGIN: it can be used to log in from PostgREST.\nGRANT web_anon: allows this user to act as web_anon.\n\nüìö See PostgREST Tutorial, Step 3"
  },
  {
    "objectID": "pipelines/pipelines.html#test-the-api-endpoint",
    "href": "pipelines/pipelines.html#test-the-api-endpoint",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Test the API Endpoint",
    "text": "Test the API Endpoint\nTo test your PostgREST API:\n\nOpen your browser or use a tool like curl.\nVisit your public API URL, appending the view name:\n\nhttps://&lt;your-domain&gt;.up.railway.app/general_aviation_weather_view\nIf everything is configured correctly, you should see a JSON response:\n[\n  {\n    \"timestamp\": \"2025-04-08T16:00:00Z\",\n    \"callsign\": \"N12345\",\n    \"altitude_meters\": 1524,\n    \"precipitation_mm\": 0.8,\n    \"weathercode\": 61\n  },\n  ...\n]\n‚úÖ If you see data, your DB API bridge is fully operational!"
  },
  {
    "objectID": "pipelines/pipelines.html#summary",
    "href": "pipelines/pipelines.html#summary",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "üéâ Summary",
    "text": "üéâ Summary\n\n‚úÖ PostgREST converts your database views into REST endpoints.\n‚úÖ You isolated access using a dedicated schema (api) and view.\n‚úÖ You secured the system using PostgreSQL roles (web_anon and authenticator).\n‚úÖ You can now integrate this endpoint with Swagger, frontend tools, or other systems."
  },
  {
    "objectID": "pipelines/pipelines.html#why-create-an-api-developer-portal",
    "href": "pipelines/pipelines.html#why-create-an-api-developer-portal",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Why Create an API Developer Portal?",
    "text": "Why Create an API Developer Portal?\nOnce your data is available through a RESTful interface (via PostgREST), the next step is to document it and make it easy for developers to discover and test. This is where Swagger UI comes in.\nWhat is Swagger?\nSwagger UI is an open-source interface that visualizes APIs using the OpenAPI specification. It allows developers to: - üìñ Read documentation for your API endpoints. - üîé Explore input and output parameters. - üß™ Interactively test endpoints with live data. - ü§ù Share API functionality with frontend developers and integrators.\nSwagger connects seamlessly with PostgREST, which automatically serves an OpenAPI schema that Swagger can consume.\nüìö References: - Swagger UI Docs - PostgREST OpenAPI Support"
  },
  {
    "objectID": "pipelines/pipelines.html#step-by-step-deploy-swagger-in-railway",
    "href": "pipelines/pipelines.html#step-by-step-deploy-swagger-in-railway",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "Step-by-Step: Deploy Swagger in Railway",
    "text": "Step-by-Step: Deploy Swagger in Railway\nCreate the Swagger Service\n\nIn Railway, click Create ‚Üí New Service.\nSelect Docker Image.\nType in the image name:\nswaggerapi/swagger-ui\nPress Enter, then click Deploy."
  },
  {
    "objectID": "pipelines/pipelines.html#test-the-swagger-ui",
    "href": "pipelines/pipelines.html#test-the-swagger-ui",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "‚úÖ Test the Swagger UI",
    "text": "‚úÖ Test the Swagger UI\nOpen the public URL you generated. If everything was configured correctly, you should see the Swagger UI load your API documentation automatically. You‚Äôll be able to:\n\nView your general_aviation_weather_view endpoint.\nClick to expand and see parameters, response schema, and sample data.\nRun test requests directly from the browser."
  },
  {
    "objectID": "pipelines/pipelines.html#summary-1",
    "href": "pipelines/pipelines.html#summary-1",
    "title": "üë®üèæ‚ÄçüíªAPI Developer Portal Setup (Swagger)",
    "section": "üéâ Summary",
    "text": "üéâ Summary\n\n‚úÖ Swagger UI provides a developer-friendly interface to your API.\n‚úÖ It connects automatically to PostgREST using the OpenAPI spec.\n‚úÖ Your live API is now self-documenting, testable, and ready to share.\n\nYou now have a complete pipeline: data ingestion ‚Üí transformation ‚Üí structured storage ‚Üí API ‚Üí developer docs."
  },
  {
    "objectID": "pipelines/index.html",
    "href": "pipelines/index.html",
    "title": "Pipelines",
    "section": "",
    "text": "The ability to synthesize new knowledge by integrating disparate data sources is a powerful asset. This guide will walk you through the design and deployment of a data pipeline architecture on Railway.app, a cloud-native development platform that allows you to quickly spin up backend infrastructure with minimal configuration overhead.\nThe goal of this pipeline is to collect, structure, and expose data in a way that supports meaningful analysis and knowledge generation. By combining unstructured and structured data from multiple services into a single relational data model, this system enables real-time insights via a customizable dashboard and a developer-friendly REST API.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/index.html#overview",
    "href": "pipelines/index.html#overview",
    "title": "Pipelines",
    "section": "",
    "text": "The ability to synthesize new knowledge by integrating disparate data sources is a powerful asset. This guide will walk you through the design and deployment of a data pipeline architecture on Railway.app, a cloud-native development platform that allows you to quickly spin up backend infrastructure with minimal configuration overhead.\nThe goal of this pipeline is to collect, structure, and expose data in a way that supports meaningful analysis and knowledge generation. By combining unstructured and structured data from multiple services into a single relational data model, this system enables real-time insights via a customizable dashboard and a developer-friendly REST API.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/index.html#scenario-detecting-no-fly-windows-correlated-with-weather-thresholds",
    "href": "pipelines/index.html#scenario-detecting-no-fly-windows-correlated-with-weather-thresholds",
    "title": "Pipelines",
    "section": "2 Scenario Detecting ‚ÄúNo-Fly Windows‚Äù Correlated with Weather Thresholds",
    "text": "2 Scenario Detecting ‚ÄúNo-Fly Windows‚Äù Correlated with Weather Thresholds\n\n2.1 Research Question\nCan we identify specific weather conditions that correlate with significantly reduced flight activity‚Äîwhat we‚Äôll call ‚Äúno-fly windows‚Äù‚Äîover the Portland metro region? These windows may align with thresholds like heavy precipitation, low visibility, or high wind speeds. To answer this question, we‚Äôll construct a data pipeline that combines live flight and weather data, transforms it into a relational model, and exposes it through dashboards and APIs for analysis and discovery.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/index.html#architecture",
    "href": "pipelines/index.html#architecture",
    "title": "Pipelines",
    "section": "3 Architecture",
    "text": "3 Architecture\nYou will build an architecture composed of six distinct services as illustrated below.\n\n\n\n\n\n   graph TD\n    subgraph Railway.app\n        Postgres[(Postgres Database)]\n        Web2DB1[Web2DB - Scraper/API Service #1 - Flights]\n        Web2DB2[Web2DB - Scraper/API Service #2 - Weather]\n        DataTransform[DataTransform Service]\n        Grafana[Grafana Dashboard]\n        DB2API[PostgREST - DB2API Bridge]\n        Swagger[Swagger UI - API Dev Tool]\n    end\n\n    Web2DB1 --&gt; Postgres\n    Web2DB2 --&gt; Postgres\n    Postgres --&gt; DataTransform\n    DataTransform --&gt; Postgres\n    Postgres --&gt; Grafana\n    Postgres --&gt; DB2API\n    DB2API --&gt; Swagger\n\n\n\n\n\n\n\n3.1 How Each Service Contributes\n\n3.1.1 üíΩ Postgres Database\nThe Postgres database is the central data repository. It stores: - Unstructured data: Raw JSON or payloads directly from the APIs during collection. - Structured data: Normalized tables and time-series records created after transformation. This separation allows for historical data to be archived while maintaining clean, queryable models for analysis.\n\n\n3.1.2 ‚úàÔ∏è Web2DB Flights\nThe Web2DB Flights ingestion service calls the OpenSky Network API every 5 minutes to gather live air traffic data over the Portland metro area. It logs: - Aircraft positions - Altitudes and speeds - Callsigns and ICAO codes - Timestamps and bounding box info This unstructured data is stored directly into Postgres for later processing.\n\n\n3.1.3 üåßÔ∏è Web2DB Weather\nThe Web2DB Weather ingestion service calls the Open-Meteo API every 5 minutes, retrieving weather information for the Portland region. It collects: - Temperature, wind speed and direction - Visibility, cloud cover, and precipitation type - Timestamps and geo-coordinates This raw weather data is logged to the database alongside flight data, aligned by time and location.\n\n\n3.1.4 üîÑ Data Transformation Service (DataTransform)\nOnce per hour, the DataTransform service processes the accumulated unstructured data into a structured relational schema. Key transformations include: - Joining flight and weather records by timestamp - Extracting metrics such as flight count per interval, average altitude, or visibility index - Normalizing date/time formats and location metadata The result is a set of structured tables optimized for query performance and analytical depth.\n\n\n3.1.5 üìä Dashboard Service (Grafana)\nGrafana connects to the structured Postgres schema and generates interactive dashboards to visualize: - Flight activity (e.g., counts, density maps) over time - Weather variable trends - Correlation graphs between weather metrics (e.g., wind speed) and flight drop-offs These dashboards help identify possible ‚Äúno-fly windows‚Äù by aligning dips in flight activity with adverse weather thresholds.\n\n\n3.1.6 üåê DB/API Bridge Service (DB2API)\nUsing PostgREST, this service turns the structured database into a RESTful API. It allows: - Querying historical flight and weather data - Filtering based on date/time, weather conditions, or flight metrics - Serving clean, JSON-based endpoints for developers, analysts, or downstream systems\n\n\n3.1.7 üë®üèæ‚Äçüíª API Developer Interface (Swagger)\nThe Swagger service documents the exposed API endpoints and provides an interactive UI for developers to: - Explore and test API requests in real-time - Understand available data models and query parameters - Integrate this data into custom applications, dashboards, or research tools\n\nTogether, this architecture enables rich, real-time and historical analysis to determine when and why flight activity slows or stops due to environmental conditions. The result is a powerful system for answering not only our current research question, but also a broader range of aviation and climate analytics challenges.\nHead to the next section, where we‚Äôll begin by setting up the Postgres database service and preparing the schema for incoming unstructured data.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html",
    "href": "pipelines/transformation-service.html",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "In this section, you‚Äôll design and deploy a transformation service called DataTransform. This service will extract meaningful fields from your raw JSON flight and weather data and insert them into clean, structured tables in your Postgres database.\nThis transformation process involves: - Understanding the structure of raw JSON. - Designing normalized physical tables for analysis. - Writing SQL to extract values using Postgres JSON functions. - Automating the transformation with a scheduled job in Railway.\n\n\n\n\nBefore transforming the data, you must understand what the raw JSON looks like. You can do this by running:\nSELECT raw_json FROM flight_json_data ORDER BY timestamptz DESC LIMIT 1;\nSELECT raw_json FROM weather_json_data ORDER BY timestamptz DESC LIMIT 1;\nExplore the shape and nesting of each payload. For example: - OpenSky states is an array of arrays. OpenSky JSON Structure - Open-Meteo JSON is a nested object. Open-Meteo Example\nüìö Reference: PostgreSQL JSON Functions and Operators\n\n\n\nBased on the JSON contents, you‚Äôll need normalized tables to store flight and weather data. Here‚Äôs the schema you‚Äôll use:\nCREATE TABLE flight_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE weather_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Structured output tables\nCREATE TABLE flights (\n    id SERIAL PRIMARY KEY,\n    icao24 VARCHAR(10) NOT NULL,\n    callsign VARCHAR(10),\n    country VARCHAR(64),\n    latitude DOUBLE PRECISION,\n    longitude DOUBLE PRECISION,\n    altitude_meters DOUBLE PRECISION,\n    velocity_knots DOUBLE PRECISION,\n    heading_degrees DOUBLE PRECISION,\n    vertical_rate DOUBLE PRECISION,\n    timestamp TIMESTAMPTZ NOT NULL\n);\n\nCREATE TABLE weather_observations (\n    id SERIAL PRIMARY KEY,\n    latitude DOUBLE PRECISION NOT NULL,\n    longitude DOUBLE PRECISION NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL,\n    precipitation_mm DOUBLE PRECISION,\n    weathercode SMALLINT\n);\n\nCREATE TABLE weather_condition (\n    code SMALLINT PRIMARY KEY,\n    description TEXT\n);\nThese tables will be the targets of your transformation logic.\n\n\n\nWhat‚Äôs a Common Table Expression?\nA Common Table Expression (CTE) is a temporary result set defined at the beginning of a SQL statement. It helps modularize complex queries and allows you to: - Break down a multi-step transformation into readable pieces. - Reference intermediate results multiple times.\nüìö Reference: CTEs in PostgreSQL\n\n\n\n\nBEGIN;\n\nWITH raw AS (\n    SELECT id, timestamptz, jsonb_array_elements(raw_json-&gt;'states') AS state\n    FROM flight_json_data\n),\nparsed AS (\n    SELECT\n        id,\n        state-&gt;&gt;0 AS icao24,\n        state-&gt;&gt;1 AS callsign,\n        state-&gt;&gt;2 AS country,\n        (state-&gt;&gt;6)::DOUBLE PRECISION AS longitude,\n        (state-&gt;&gt;5)::DOUBLE PRECISION AS latitude,\n        (state-&gt;&gt;7)::DOUBLE PRECISION AS altitude_meters,\n        (state-&gt;&gt;9)::DOUBLE PRECISION AS velocity_knots,\n        (state-&gt;&gt;10)::DOUBLE PRECISION AS heading_degrees,\n        (state-&gt;&gt;11)::DOUBLE PRECISION AS vertical_rate,\n        timestamptz\n    FROM raw\n)\nINSERT INTO flights (\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamp\n)\nSELECT\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamptz\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM flight_json_data;\n\nCOMMIT;\n\n\n\n\nBEGIN;\n\nWITH raw AS (\n    SELECT id, raw_json, timestamptz\n    FROM weather_json_data\n),\nparsed AS (\n    SELECT\n        (raw_json-&gt;'latitude')::DOUBLE PRECISION AS latitude,\n        (raw_json-&gt;'longitude')::DOUBLE PRECISION AS longitude,\n        (raw_json-&gt;'current'-&gt;'precipitation')::DOUBLE PRECISION AS precipitation_mm,\n        (raw_json-&gt;'current'-&gt;'weathercode')::SMALLINT AS weathercode,\n        timestamptz AS timestamp\n    FROM raw\n)\nINSERT INTO weather_observations (\n    latitude, longitude, precipitation_mm, weathercode, timestamp\n)\nSELECT\n    latitude, longitude, precipitation_mm, weathercode, timestamp\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM weather_json_data;\n\nCOMMIT;\n\n\n\nA TRANSACTION ensures that either all your steps complete successfully, or none of them are applied. This prevents partial writes and data corruption. ‚Ä¢ BEGIN; starts the transaction. ‚Ä¢ COMMIT; applies all changes if no error occurred. ‚Ä¢ If an error occurs, the entire transaction can be rolled back.\n\n\n\n\n1.  [Use the db_transform GitHub template](https://github.com/LucasCordova/db_transform).\n2.  Click the ‚ÄúUse this template‚Äù button.\n3.  Name your repository: `weather_flight_db_transform`\n4.  Choose your GitHub account and click Create Repository.\n\n\n\n1.  Open the new repository on GitHub (recommended) or clone it locally.\n2.  Edit the clean.sql file. If you are editing in the browser, choose the `pencil` icon. Paste your flight and weather transformation SQL inside it.\n3.  Save the file and commit. In the browser editor, click the Commit changes... button, add a message, and click the final Commit changes button.\n\n\n\n1.  Go to your Railway project.\n2.  Click New ‚Üí Deploy from GitHub Repo.\n3.  Click ‚ÄúConfigure GitHub App‚Äù to give Railway access to your GitHub account.\n4.  Choose the repository weather_flight_db_transform.\n5.  Add the following environment variables in the Variables tab. \n\n\n\nKey\nValue\n\n\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\n\n6.  Click Deploy.\n\n\n\n1.  Go to the Settings tab of the DataTransform service.\n2.  Scroll to Triggers ‚Üí New Trigger.\n3.  Choose Cron and enter: `0 * * * *`.\nThis means the service runs once every hour, at the top of the hour.\n\n\n\n‚úÖ Check the Tables in Beekeeper\n\n    After the first cron run, inspect the structured tables:\nSELECT * FROM flights ORDER BY timestamp DESC LIMIT 10;\nSELECT * FROM weather_observations ORDER BY timestamp DESC LIMIT 10;\n‚úÖ View Logs in Railway\n    Use the Logs tab to inspect any errors or debug output.\n\n\n\nNow that your structured data is flowing, you can visualize and analyze it in the next step using Grafana Dashboards. You‚Äôll use SQL queries to explore patterns in flight activity and weather data‚Äîlaying the foundation for answering your core research question.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#overview",
    "href": "pipelines/transformation-service.html#overview",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "In this section, you‚Äôll design and deploy a transformation service called DataTransform. This service will extract meaningful fields from your raw JSON flight and weather data and insert them into clean, structured tables in your Postgres database.\nThis transformation process involves: - Understanding the structure of raw JSON. - Designing normalized physical tables for analysis. - Writing SQL to extract values using Postgres JSON functions. - Automating the transformation with a scheduled job in Railway.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#understand-the-json-structure",
    "href": "pipelines/transformation-service.html#understand-the-json-structure",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "Before transforming the data, you must understand what the raw JSON looks like. You can do this by running:\nSELECT raw_json FROM flight_json_data ORDER BY timestamptz DESC LIMIT 1;\nSELECT raw_json FROM weather_json_data ORDER BY timestamptz DESC LIMIT 1;\nExplore the shape and nesting of each payload. For example: - OpenSky states is an array of arrays. OpenSky JSON Structure - Open-Meteo JSON is a nested object. Open-Meteo Example\nüìö Reference: PostgreSQL JSON Functions and Operators",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#design-your-physical-schema",
    "href": "pipelines/transformation-service.html#design-your-physical-schema",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "Based on the JSON contents, you‚Äôll need normalized tables to store flight and weather data. Here‚Äôs the schema you‚Äôll use:\nCREATE TABLE flight_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE weather_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Structured output tables\nCREATE TABLE flights (\n    id SERIAL PRIMARY KEY,\n    icao24 VARCHAR(10) NOT NULL,\n    callsign VARCHAR(10),\n    country VARCHAR(64),\n    latitude DOUBLE PRECISION,\n    longitude DOUBLE PRECISION,\n    altitude_meters DOUBLE PRECISION,\n    velocity_knots DOUBLE PRECISION,\n    heading_degrees DOUBLE PRECISION,\n    vertical_rate DOUBLE PRECISION,\n    timestamp TIMESTAMPTZ NOT NULL\n);\n\nCREATE TABLE weather_observations (\n    id SERIAL PRIMARY KEY,\n    latitude DOUBLE PRECISION NOT NULL,\n    longitude DOUBLE PRECISION NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL,\n    precipitation_mm DOUBLE PRECISION,\n    weathercode SMALLINT\n);\n\nCREATE TABLE weather_condition (\n    code SMALLINT PRIMARY KEY,\n    description TEXT\n);\nThese tables will be the targets of your transformation logic.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#write-sql-to-transform-the-data",
    "href": "pipelines/transformation-service.html#write-sql-to-transform-the-data",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "What‚Äôs a Common Table Expression?\nA Common Table Expression (CTE) is a temporary result set defined at the beginning of a SQL statement. It helps modularize complex queries and allows you to: - Break down a multi-step transformation into readable pieces. - Reference intermediate results multiple times.\nüìö Reference: CTEs in PostgreSQL",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#example-transformation-for-flights",
    "href": "pipelines/transformation-service.html#example-transformation-for-flights",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "BEGIN;\n\nWITH raw AS (\n    SELECT id, timestamptz, jsonb_array_elements(raw_json-&gt;'states') AS state\n    FROM flight_json_data\n),\nparsed AS (\n    SELECT\n        id,\n        state-&gt;&gt;0 AS icao24,\n        state-&gt;&gt;1 AS callsign,\n        state-&gt;&gt;2 AS country,\n        (state-&gt;&gt;6)::DOUBLE PRECISION AS longitude,\n        (state-&gt;&gt;5)::DOUBLE PRECISION AS latitude,\n        (state-&gt;&gt;7)::DOUBLE PRECISION AS altitude_meters,\n        (state-&gt;&gt;9)::DOUBLE PRECISION AS velocity_knots,\n        (state-&gt;&gt;10)::DOUBLE PRECISION AS heading_degrees,\n        (state-&gt;&gt;11)::DOUBLE PRECISION AS vertical_rate,\n        timestamptz\n    FROM raw\n)\nINSERT INTO flights (\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamp\n)\nSELECT\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamptz\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM flight_json_data;\n\nCOMMIT;",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#example-transformation-for-weather",
    "href": "pipelines/transformation-service.html#example-transformation-for-weather",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "BEGIN;\n\nWITH raw AS (\n    SELECT id, raw_json, timestamptz\n    FROM weather_json_data\n),\nparsed AS (\n    SELECT\n        (raw_json-&gt;'latitude')::DOUBLE PRECISION AS latitude,\n        (raw_json-&gt;'longitude')::DOUBLE PRECISION AS longitude,\n        (raw_json-&gt;'current'-&gt;'precipitation')::DOUBLE PRECISION AS precipitation_mm,\n        (raw_json-&gt;'current'-&gt;'weathercode')::SMALLINT AS weathercode,\n        timestamptz AS timestamp\n    FROM raw\n)\nINSERT INTO weather_observations (\n    latitude, longitude, precipitation_mm, weathercode, timestamp\n)\nSELECT\n    latitude, longitude, precipitation_mm, weathercode, timestamp\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM weather_json_data;\n\nCOMMIT;\n\n\n\nA TRANSACTION ensures that either all your steps complete successfully, or none of them are applied. This prevents partial writes and data corruption. ‚Ä¢ BEGIN; starts the transaction. ‚Ä¢ COMMIT; applies all changes if no error occurred. ‚Ä¢ If an error occurs, the entire transaction can be rolled back.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#use-the-github-template",
    "href": "pipelines/transformation-service.html#use-the-github-template",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "1.  [Use the db_transform GitHub template](https://github.com/LucasCordova/db_transform).\n2.  Click the ‚ÄúUse this template‚Äù button.\n3.  Name your repository: `weather_flight_db_transform`\n4.  Choose your GitHub account and click Create Repository.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#add-your-sql-to-clean.sql",
    "href": "pipelines/transformation-service.html#add-your-sql-to-clean.sql",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "1.  Open the new repository on GitHub (recommended) or clone it locally.\n2.  Edit the clean.sql file. If you are editing in the browser, choose the `pencil` icon. Paste your flight and weather transformation SQL inside it.\n3.  Save the file and commit. In the browser editor, click the Commit changes... button, add a message, and click the final Commit changes button.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#deploy-the-service-in-railway",
    "href": "pipelines/transformation-service.html#deploy-the-service-in-railway",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "1.  Go to your Railway project.\n2.  Click New ‚Üí Deploy from GitHub Repo.\n3.  Click ‚ÄúConfigure GitHub App‚Äù to give Railway access to your GitHub account.\n4.  Choose the repository weather_flight_db_transform.\n5.  Add the following environment variables in the Variables tab. \n\n\n\nKey\nValue\n\n\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\n\n6.  Click Deploy.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#set-cron-schedule",
    "href": "pipelines/transformation-service.html#set-cron-schedule",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "1.  Go to the Settings tab of the DataTransform service.\n2.  Scroll to Triggers ‚Üí New Trigger.\n3.  Choose Cron and enter: `0 * * * *`.\nThis means the service runs once every hour, at the top of the hour.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#testing-and-validation",
    "href": "pipelines/transformation-service.html#testing-and-validation",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "‚úÖ Check the Tables in Beekeeper\n\n    After the first cron run, inspect the structured tables:\nSELECT * FROM flights ORDER BY timestamp DESC LIMIT 10;\nSELECT * FROM weather_observations ORDER BY timestamp DESC LIMIT 10;\n‚úÖ View Logs in Railway\n    Use the Logs tab to inspect any errors or debug output.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#whats-next",
    "href": "pipelines/transformation-service.html#whats-next",
    "title": "üîÑ Data Transformation Service Setup",
    "section": "",
    "text": "Now that your structured data is flowing, you can visualize and analyze it in the next step using Grafana Dashboards. You‚Äôll use SQL queries to explore patterns in flight activity and weather data‚Äîlaying the foundation for answering your core research question.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html",
    "href": "pipelines/dashboard-service.html",
    "title": "Dashboard Service Setup",
    "section": "",
    "text": "A View in Postgres is a virtual table based on a SQL SELECT query. It behaves just like a table for reading, but doesn‚Äôt store any data itself‚Äîit simply runs the query behind it whenever it‚Äôs accessed.\nViews are excellent for dashboards because:\n\n‚úÖ They abstract complex queries behind a simple name.\n‚úÖ They can be reused by multiple clients (like Grafana).\n‚úÖ You can restrict access to views rather than raw tables using Postgres roles.\n‚úÖ They allow you to pre-join and pre-aggregate data in a way that‚Äôs efficient and secure for visualization.",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#but-first-views",
    "href": "pipelines/dashboard-service.html#but-first-views",
    "title": "Dashboard Service Setup",
    "section": "",
    "text": "A View in Postgres is a virtual table based on a SQL SELECT query. It behaves just like a table for reading, but doesn‚Äôt store any data itself‚Äîit simply runs the query behind it whenever it‚Äôs accessed.\nViews are excellent for dashboards because:\n\n‚úÖ They abstract complex queries behind a simple name.\n‚úÖ They can be reused by multiple clients (like Grafana).\n‚úÖ You can restrict access to views rather than raw tables using Postgres roles.\n‚úÖ They allow you to pre-join and pre-aggregate data in a way that‚Äôs efficient and secure for visualization.",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#create-a-view-for-the-dashboard",
    "href": "pipelines/dashboard-service.html#create-a-view-for-the-dashboard",
    "title": "Dashboard Service Setup",
    "section": "2 Create a View for the Dashboard",
    "text": "2 Create a View for the Dashboard\nLet‚Äôs create a view that answers our research question: &gt; Are there ‚Äúno-fly windows‚Äù correlated with weather thresholds?\nThis view joins structured flight and weather data to support correlation queries:\nCREATE OR REPLACE VIEW flight_weather AS\nSELECT\n    f.timestamp,\n    f.callsign,\n    f.altitude_meters,\n    f.velocity_knots,\n    f.latitude AS flight_latitude,\n    f.longitude AS flight_longitude,\n    w.precipitation_mm,\n    w.weathercode,\n    w.latitude AS weather_latitude,\n    w.longitude AS weather_longitude\nFROM\n    flights f\nJOIN\n    weather_observations w\nON\n    date_trunc('minute', f.timestamp) = date_trunc('minute', w.timestamp)\nORDER BY\n    f.timestamp DESC;",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#deploying-grafana-in-railway",
    "href": "pipelines/dashboard-service.html#deploying-grafana-in-railway",
    "title": "Dashboard Service Setup",
    "section": "3 Deploying Grafana in Railway",
    "text": "3 Deploying Grafana in Railway\n\n3.1 Add the Grafana Template\n\nIn your Railway project, click ‚ÄúCreate‚Äù ‚Üí ‚ÄúTemplate‚Äù.\nType Grafana in the search bar.\nSelect the version provided by Andre Lademann‚Äôs Projects.\nAdd the following environment variables:\n\n\n\nKey\nValue\n\n\n\n\nGF_SECURITY_ADMIN_USER\ngrafanareader\n\n\nGF_DEFAULT_INSTANCE_NAME\ngrafanapg\n\n\nGF_SECURITY_ADMIN_PASSWORD\nyour-password\n\n\n\nLeave the 4 pre-configured environment variables as they are.\nOnce you‚Äôve entered the variables, the service will check connectivity and display a message saying ‚ÄúReady to be deployed‚Äù.\nClick [Deploy]\nThis will start the deployment process. It may take a few minutes.\n\n\n\n\n3.2 Enable Serverless Mode on Grafana\nAfter deployment completes:\n\nReturn to the Grafana service in Railway.\nClick on Settings.\nScroll down to ‚ÄúServerless‚Äù.\nToggle Enable Serverless and click Deploy again.\n\n\n‚úÖ Enabling serverless mode helps reduce cost because the service will go to sleep when not in use and wake up on demand. This is ideal for tools like Grafana that are used periodically.",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#add-a-grafana-specific-db-user",
    "href": "pipelines/dashboard-service.html#add-a-grafana-specific-db-user",
    "title": "Dashboard Service Setup",
    "section": "4 Add a Grafana-Specific DB User",
    "text": "4 Add a Grafana-Specific DB User\nTo avoid giving Grafana full access to your database, we‚Äôll create a read-only user that can only access the view.\n\n4.1 Steps:\n\nOpen your Postgres database in Beekeeper Studio.\nRun the following SQL (replace 'password' with one you‚Äôll remember):\n\nCREATE USER grafanareader WITH PASSWORD 'your_password';\nGRANT USAGE ON SCHEMA \"public\" TO grafanareader;\nGRANT SELECT ON \"public\".flight_weather TO grafanareader;",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#connect-grafana-to-postgres",
    "href": "pipelines/dashboard-service.html#connect-grafana-to-postgres",
    "title": "Dashboard Service Setup",
    "section": "5 Connect Grafana to Postgres",
    "text": "5 Connect Grafana to Postgres\n\n5.1 Copy PGHOST from Railway\n\nGo to your Postgres service in Railway.\nUnder Environment Variables, copy the value for PGHOST.\nIt will look like: postgres.railway.internal.\n\n\n\n\n5.2 Open Grafana and Log In\n\nGo to your Grafana URL shown in the Deployments tab.\nExample:\nhttps://grafana-rbi9-production.up.railway.app\nLog in with:\n\nUsername: grafanareader\nPassword: the password you assigned earlier\n\n\n\n\n\n5.3 Add a Data Source\n\nIn Grafana, click Connections on the left panel.\nClick Data Sources.\nSearch for PostgreSQL and select it.\n\n\n5.3.1 Fill in the following:\n\n\n\nField\nValue\n\n\n\n\nHost\npostgres.railway.internal:5432\n\n\nDatabase\nrailway\n\n\nUser\ngrafanareader\n\n\nPassword\nyour_password\n\n\n\nClick Save & Test to confirm connectivity.",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#create-your-first-dashboard-in-grafana",
    "href": "pipelines/dashboard-service.html#create-your-first-dashboard-in-grafana",
    "title": "Dashboard Service Setup",
    "section": "6 Create Your First Dashboard in Grafana",
    "text": "6 Create Your First Dashboard in Grafana\n\nGo back to the Home page in Grafana.\nClick ‚ÄúCreate your first dashboard‚Äù.\nClick ‚ÄúAdd Visualization‚Äù.\n\n\n6.1 Configure the Panel\n\nChoose Data Source: Select the PostgreSQL source you just set up.\nFrom Table: Select the flight_weather view.\nAdd Columns: Choose at least one time-based column (timestamp) and one data column (e.g., altitude_meters or precipitation_mm).\nClick Run Query.\nUse Table Mode or switch to a chart via Suggested Visualizations.\n\nYou now have a live dashboard pulling from your structured data pipeline!",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html#whats-next",
    "href": "pipelines/dashboard-service.html#whats-next",
    "title": "Dashboard Service Setup",
    "section": "7 What‚Äôs Next?",
    "text": "7 What‚Äôs Next?\nFrom here, you can:\n\nAdd filters to segment by weathercode.\nAggregate values like average flight altitude during rain conditions.\nBuild a full visualization of ‚Äúno-fly‚Äù windows over time.\n\nIn the next step we‚Äôll expose your view as a REST API using PostgREST and document it using Swagger.",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html",
    "href": "pipelines/db-api-bridge-service.html",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "Once your structured data is ready for exploration and integration, the next step is to expose it via an API. This allows your data to be accessed securely by dashboards, external applications, or developer tools. Instead of building a custom REST API, we can use PostgREST, which automates this process by generating RESTful endpoints directly from your PostgreSQL schema.\n\n\n\n\nPostgREST is a lightweight, open-source tool that turns your PostgreSQL database into a secure RESTful API. With PostgREST, your views and tables become accessible via HTTP without writing backend code.\n\n\n\nüöÄ Generates REST endpoints for tables, views, and stored procedures.\nüîê Uses PostgreSQL‚Äôs native roles and permissions for access control.\nüìä Perfect for exposing analytic views to tools like Swagger, frontend apps, and more.\n\n\n\n\n\n\n\n\n\nClick Create in your Railway project.\nChoose Docker Image.\nEnter the Docker image name:\npostgrest/postgrest\nClick Deploy.\n\n\n\n\n\nAfter deployment:\n\nClick on the Deployments tab, then select Settings.\nUnder Public Networking, click Generate Domain.\n\nYou can customize the domain to a meaningful name if desired.\nCopy the public URL shown.\n\nScroll to the Serverless section.\nToggle Enable Serverless, then click Deploy again.\n\n\nEnabling Serverless mode helps reduce costs ‚Äî the service sleeps when not in use and wakes up on demand.\n\n\n\n\n\nNavigate to the Variables tab and add the following:\n\n\n\nKey\nValue\n\n\n\n\nPGRST_DB_ANON_ROLE\nweb_anon\n\n\nPGRST_DB_SCHEMA\napi\n\n\nPGRST_DB_URI\n${{DATABASE_URL}}\n\n\nPGRST_OPENAPI_SERVER_PROXY_URI\n&lt;your public hostname&gt;\n\n\n\n\nReplace &lt;your public hostname&gt; with the public domain you generated in the previous step.\n\n\n\n\n\n\nOpen Beekeeper Studio and execute the following SQL:\n-- Create a non-login role to define anonymous API access\nCREATE ROLE web_anon NOLOGIN;\n\n-- Allow that role to access the \"api\" schema\nGRANT USAGE ON SCHEMA api TO web_anon;\n\n-- Limit access only to this view\nGRANT SELECT ON api.general_aviation_weather_view TO web_anon;\nThis ensures PostgREST can only read from a specific schema (api) and specific view (general_aviation_weather_view). It does not grant access to other tables or schemas.\n\n\n\n\nRather than using the powerful postgres role to connect from PostgREST, we create a separate authenticator user that can assume the web_anon role. This is more secure and scalable.\nCREATE ROLE authenticator NOINHERIT LOGIN PASSWORD '&lt;your password&gt;';\nGRANT web_anon TO authenticator;\n\n\n\nNOINHERIT: the authenticator role cannot directly access anything.\nLOGIN: it can be used to log in from PostgREST.\nGRANT web_anon: allows this user to act as web_anon.\n\nüìö See PostgREST Tutorial, Step 3\n\n\n\n\n\nTo test your PostgREST API:\n\nOpen your browser or use a tool like curl.\nVisit your public API URL, appending the view name:\n\nhttps://&lt;your-domain&gt;.up.railway.app/general_aviation_weather_view\nIf everything is configured correctly, you should see a JSON response:\n[\n  {\n    \"timestamp\": \"2025-04-08T16:00:00Z\",\n    \"callsign\": \"N12345\",\n    \"altitude_meters\": 1524,\n    \"precipitation_mm\": 0.8,\n    \"weathercode\": 61\n  },\n  ...\n]\n‚úÖ If you see data, your DB API bridge is fully operational!\n\n\n\n\n\n‚úÖ PostgREST converts your database views into REST endpoints.\n‚úÖ You isolated access using a dedicated schema (api) and view.\n‚úÖ You secured the system using PostgreSQL roles (web_anon and authenticator).\n‚úÖ You can now integrate this endpoint with Swagger, frontend tools, or other systems.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#why-set-up-a-db-api-bridge",
    "href": "pipelines/db-api-bridge-service.html#why-set-up-a-db-api-bridge",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "Once your structured data is ready for exploration and integration, the next step is to expose it via an API. This allows your data to be accessed securely by dashboards, external applications, or developer tools. Instead of building a custom REST API, we can use PostgREST, which automates this process by generating RESTful endpoints directly from your PostgreSQL schema.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#what-is-postgrest",
    "href": "pipelines/db-api-bridge-service.html#what-is-postgrest",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "PostgREST is a lightweight, open-source tool that turns your PostgreSQL database into a secure RESTful API. With PostgREST, your views and tables become accessible via HTTP without writing backend code.\n\n\n\nüöÄ Generates REST endpoints for tables, views, and stored procedures.\nüîê Uses PostgreSQL‚Äôs native roles and permissions for access control.\nüìä Perfect for exposing analytic views to tools like Swagger, frontend apps, and more.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#step-by-step-deploy-postgrest-in-railway",
    "href": "pipelines/db-api-bridge-service.html#step-by-step-deploy-postgrest-in-railway",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "Click Create in your Railway project.\nChoose Docker Image.\nEnter the Docker image name:\npostgrest/postgrest\nClick Deploy.\n\n\n\n\n\nAfter deployment:\n\nClick on the Deployments tab, then select Settings.\nUnder Public Networking, click Generate Domain.\n\nYou can customize the domain to a meaningful name if desired.\nCopy the public URL shown.\n\nScroll to the Serverless section.\nToggle Enable Serverless, then click Deploy again.\n\n\nEnabling Serverless mode helps reduce costs ‚Äî the service sleeps when not in use and wakes up on demand.\n\n\n\n\n\nNavigate to the Variables tab and add the following:\n\n\n\nKey\nValue\n\n\n\n\nPGRST_DB_ANON_ROLE\nweb_anon\n\n\nPGRST_DB_SCHEMA\napi\n\n\nPGRST_DB_URI\n${{DATABASE_URL}}\n\n\nPGRST_OPENAPI_SERVER_PROXY_URI\n&lt;your public hostname&gt;\n\n\n\n\nReplace &lt;your public hostname&gt; with the public domain you generated in the previous step.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#set-up-database-permissions",
    "href": "pipelines/db-api-bridge-service.html#set-up-database-permissions",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "Open Beekeeper Studio and execute the following SQL:\n-- Create a non-login role to define anonymous API access\nCREATE ROLE web_anon NOLOGIN;\n\n-- Allow that role to access the \"api\" schema\nGRANT USAGE ON SCHEMA api TO web_anon;\n\n-- Limit access only to this view\nGRANT SELECT ON api.general_aviation_weather_view TO web_anon;\nThis ensures PostgREST can only read from a specific schema (api) and specific view (general_aviation_weather_view). It does not grant access to other tables or schemas.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#create-a-dedicated-authenticator-role",
    "href": "pipelines/db-api-bridge-service.html#create-a-dedicated-authenticator-role",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "Rather than using the powerful postgres role to connect from PostgREST, we create a separate authenticator user that can assume the web_anon role. This is more secure and scalable.\nCREATE ROLE authenticator NOINHERIT LOGIN PASSWORD '&lt;your password&gt;';\nGRANT web_anon TO authenticator;\n\n\n\nNOINHERIT: the authenticator role cannot directly access anything.\nLOGIN: it can be used to log in from PostgREST.\nGRANT web_anon: allows this user to act as web_anon.\n\nüìö See PostgREST Tutorial, Step 3",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#test-the-api-endpoint",
    "href": "pipelines/db-api-bridge-service.html#test-the-api-endpoint",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "To test your PostgREST API:\n\nOpen your browser or use a tool like curl.\nVisit your public API URL, appending the view name:\n\nhttps://&lt;your-domain&gt;.up.railway.app/general_aviation_weather_view\nIf everything is configured correctly, you should see a JSON response:\n[\n  {\n    \"timestamp\": \"2025-04-08T16:00:00Z\",\n    \"callsign\": \"N12345\",\n    \"altitude_meters\": 1524,\n    \"precipitation_mm\": 0.8,\n    \"weathercode\": 61\n  },\n  ...\n]\n‚úÖ If you see data, your DB API bridge is fully operational!",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html#summary",
    "href": "pipelines/db-api-bridge-service.html#summary",
    "title": "üåâDB API Bridge Service Setup",
    "section": "",
    "text": "‚úÖ PostgREST converts your database views into REST endpoints.\n‚úÖ You isolated access using a dedicated schema (api) and view.\n‚úÖ You secured the system using PostgreSQL roles (web_anon and authenticator).\n‚úÖ You can now integrate this endpoint with Swagger, frontend tools, or other systems.",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html",
    "href": "pipelines/weather-ingestion-service.html",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Weather ingestion service, which fetches live weather data for the Portland area from the Open-Meteo API and stores it in your Postgres database every five minutes.\nThis service collects unstructured weather readings‚Äîlike temperature and wind speed‚Äîand writes them to the weather_json_data table as raw JSON for later transformation and analysis. The data will later be joined with flight data to investigate correlations such as weather-driven no-fly windows.\n\n\n\n\nFor this demo, we‚Äôre using the following Open-Meteo endpoint:\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\n\nlatitude=45.52 and longitude=-122.68 pinpoint downtown Portland.\ncurrent=... specifies the current weather variables to return:\n\ntemperature_2m: air temperature 2 meters above the ground.\nwind_speed_10m: wind speed 10 meters above the ground.\nweathercode: numeric weather condition code (e.g., fog, rain, snow).\n\n\n\n\n\n### üìö Learn more at:\n\n\nüîó Open-Meteo API Docs\n\n\n\n\n\n\n\n\n\n\nIn your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce the image is deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nweather_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\n‚úÖ Tip: Use Railway‚Äôs variable picker to link to your Postgres service when setting DATABASE_URL.\n\n\n\n\n\n\n\n\n\nGo to the Settings tab of the Web2DB Weather service.\nScroll to Triggers and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the trigger type.\nUse this schedule: */5 * * * *\n\nThis means the service will run every 5 minutes.\nüìö Reference: Crontab Guru ‚Äî an easy tool to preview cron expressions.\n\n\n\n\n\nOnce your cron trigger and environment variables are saved, the service will begin pulling weather data.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if you haven‚Äôt already).\nRun:\n\nSELECT * FROM weather_json_data ORDER BY timestamptz DESC LIMIT 10;\nYou should see recent entries showing full JSON responses from the Open-Meteo API.\n\n\n\n\n- Check the Deployments tab in Railway to confirm successful runs.\n- Use the Logs tab to inspect real-time debug output.\n- Ensure the SITE_URL is formatted correctly (with no line breaks or spaces).\n- If the table is still empty after 5‚Äì10 minutes, double-check the DATABASE_URL, TABLE_NAME, and cron settings.\n\n\n\nWith both Web2DB Flights and Web2DB Weather streaming live data into your Postgres instance, you‚Äôre ready to build the DataTransform service that will process this raw data into structured, relational models‚Äîmaking it easier to perform analytics and build dashboards.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#overview",
    "href": "pipelines/weather-ingestion-service.html#overview",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Weather ingestion service, which fetches live weather data for the Portland area from the Open-Meteo API and stores it in your Postgres database every five minutes.\nThis service collects unstructured weather readings‚Äîlike temperature and wind speed‚Äîand writes them to the weather_json_data table as raw JSON for later transformation and analysis. The data will later be joined with flight data to investigate correlations such as weather-driven no-fly windows.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#about-the-open-meteo-api-url",
    "href": "pipelines/weather-ingestion-service.html#about-the-open-meteo-api-url",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "For this demo, we‚Äôre using the following Open-Meteo endpoint:\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\n\nlatitude=45.52 and longitude=-122.68 pinpoint downtown Portland.\ncurrent=... specifies the current weather variables to return:\n\ntemperature_2m: air temperature 2 meters above the ground.\nwind_speed_10m: wind speed 10 meters above the ground.\nweathercode: numeric weather condition code (e.g., fog, rain, snow).\n\n\n\n\n\n### üìö Learn more at:\n\n\nüîó Open-Meteo API Docs",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#deploying-the-web2db-weather-service",
    "href": "pipelines/weather-ingestion-service.html#deploying-the-web2db-weather-service",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "In your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce the image is deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nweather_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\n‚úÖ Tip: Use Railway‚Äôs variable picker to link to your Postgres service when setting DATABASE_URL.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#scheduling-the-ingestion",
    "href": "pipelines/weather-ingestion-service.html#scheduling-the-ingestion",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "Go to the Settings tab of the Web2DB Weather service.\nScroll to Triggers and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the trigger type.\nUse this schedule: */5 * * * *\n\nThis means the service will run every 5 minutes.\nüìö Reference: Crontab Guru ‚Äî an easy tool to preview cron expressions.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#testing-the-ingestion",
    "href": "pipelines/weather-ingestion-service.html#testing-the-ingestion",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "Once your cron trigger and environment variables are saved, the service will begin pulling weather data.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if you haven‚Äôt already).\nRun:\n\nSELECT * FROM weather_json_data ORDER BY timestamptz DESC LIMIT 10;\nYou should see recent entries showing full JSON responses from the Open-Meteo API.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#troubleshooting-tips",
    "href": "pipelines/weather-ingestion-service.html#troubleshooting-tips",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "- Check the Deployments tab in Railway to confirm successful runs.\n- Use the Logs tab to inspect real-time debug output.\n- Ensure the SITE_URL is formatted correctly (with no line breaks or spaces).\n- If the table is still empty after 5‚Äì10 minutes, double-check the DATABASE_URL, TABLE_NAME, and cron settings.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#whats-next",
    "href": "pipelines/weather-ingestion-service.html#whats-next",
    "title": "üå¶Ô∏è Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "With both Web2DB Flights and Web2DB Weather streaming live data into your Postgres instance, you‚Äôre ready to build the DataTransform service that will process this raw data into structured, relational models‚Äîmaking it easier to perform analytics and build dashboards.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html",
    "href": "pipelines/flight-ingestion-service.html",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Flights ingestion service, which fetches live flight data from the OpenSky Network API and stores it in your Postgres database every five minutes.\nThis service will collect unstructured flight telemetry data and append it to the flight_json_data table in raw JSON format. You‚Äôll deploy it as a container on Railway using the lucascordova/web2db image and configure it to run on a schedule.\n\n\n\n\nIf you‚Äôre building your own ingestion service using a different API, here‚Äôs the general process:\n\nCheck if the API requires authentication (e.g., an API key or access token).\nRead the API documentation to understand query parameters, rate limits, and response structure.\nBuild your request URL including any necessary geographic filters, authentication tokens, or query strings.\nTest the request manually (e.g., in your browser or Postman) before deploying it inside a service like Railway.\n\n\n\n\n\nFor APIs that require keys or tokens, you should always inject these as environment variables, never hardcoded in your code or Docker image.\n\n\n\n\nFor this demo, we‚Äôre using the following public OpenSky endpoint: `https://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00‚Äô\n\n\n\nlamin and lamax: latitude min/max (45.08 to 45.88)\nlomin and lomax: longitude min/max (‚àí123.50 to ‚àí122.00)\n\nThis bounding box covers the Portland metropolitan area and surrounding airspace.\nüìö Learn more at:\nüîó OpenSky Network REST API Documentation\n\n\n\n\n\n\n\n\nIn your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nflight_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\nüí° Use the variable picker in Railway to reference your Postgres service directly for DATABASE_URL.\n\n\n\n\n\n\n\n\n\nGo to the Settings tab of the Web2DB Flights service.\nScroll to the Triggers section and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the type.\nEnter the schedule: */5 * * * *\n\nThis means: ‚ÄúRun every 5 minutes.‚Äù\nüß† Cron Format Reference: - */5 = every 5 minutes\n- Full format is minute hour day month weekday\nüìö Crontab Guru is a great tool to preview and test your expressions.\n\n\n\n\n\nAfter saving the environment variables and trigger, your ingestion service should begin collecting data every 5 minutes.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if not already connected).\nRun the following SQL query:\n\nSELECT * FROM flight_json_data ORDER BY timestamptz DESC LIMIT 10;\nIf data is being ingested correctly, you should see rows with timestamps and raw JSON.\nüõ†Ô∏è Troubleshooting Tips ‚Ä¢ Go to the Deployments tab to check if the latest deployment Completed successfully. ‚Ä¢ Check the Logs tab to view debug output (especially helpful if DEBUG=TRUE is set). ‚Ä¢ Review the Cron Triggers to make sure they‚Äôre running on schedule. ‚Ä¢ Double-check the SITE_URL and DATABASE_URL for typos or incorrect formatting.\n\n\n\n\n\nNow that your Web2DB Flights service is live and filling your database with raw flight data, we‚Äôll set up the companion Web2DB Weather service to do the same with meteorological data. Together, these two data streams will power our future correlation and transformation services.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#overview",
    "href": "pipelines/flight-ingestion-service.html#overview",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Flights ingestion service, which fetches live flight data from the OpenSky Network API and stores it in your Postgres database every five minutes.\nThis service will collect unstructured flight telemetry data and append it to the flight_json_data table in raw JSON format. You‚Äôll deploy it as a container on Railway using the lucascordova/web2db image and configure it to run on a schedule.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#using-public-apis-in-your-own-projects",
    "href": "pipelines/flight-ingestion-service.html#using-public-apis-in-your-own-projects",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "If you‚Äôre building your own ingestion service using a different API, here‚Äôs the general process:\n\nCheck if the API requires authentication (e.g., an API key or access token).\nRead the API documentation to understand query parameters, rate limits, and response structure.\nBuild your request URL including any necessary geographic filters, authentication tokens, or query strings.\nTest the request manually (e.g., in your browser or Postman) before deploying it inside a service like Railway.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#using-public-apis-continued",
    "href": "pipelines/flight-ingestion-service.html#using-public-apis-continued",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "For APIs that require keys or tokens, you should always inject these as environment variables, never hardcoded in your code or Docker image.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#about-the-opensky-api-url",
    "href": "pipelines/flight-ingestion-service.html#about-the-opensky-api-url",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "For this demo, we‚Äôre using the following public OpenSky endpoint: `https://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00‚Äô\n\n\n\nlamin and lamax: latitude min/max (45.08 to 45.88)\nlomin and lomax: longitude min/max (‚àí123.50 to ‚àí122.00)\n\nThis bounding box covers the Portland metropolitan area and surrounding airspace.\nüìö Learn more at:\nüîó OpenSky Network REST API Documentation",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#deploying-the-web2db-flights-service",
    "href": "pipelines/flight-ingestion-service.html#deploying-the-web2db-flights-service",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "In your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nflight_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\nüí° Use the variable picker in Railway to reference your Postgres service directly for DATABASE_URL.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#scheduling-the-ingestion",
    "href": "pipelines/flight-ingestion-service.html#scheduling-the-ingestion",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "Go to the Settings tab of the Web2DB Flights service.\nScroll to the Triggers section and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the type.\nEnter the schedule: */5 * * * *\n\nThis means: ‚ÄúRun every 5 minutes.‚Äù\nüß† Cron Format Reference: - */5 = every 5 minutes\n- Full format is minute hour day month weekday\nüìö Crontab Guru is a great tool to preview and test your expressions.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#testing-the-ingestion",
    "href": "pipelines/flight-ingestion-service.html#testing-the-ingestion",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "After saving the environment variables and trigger, your ingestion service should begin collecting data every 5 minutes.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if not already connected).\nRun the following SQL query:\n\nSELECT * FROM flight_json_data ORDER BY timestamptz DESC LIMIT 10;\nIf data is being ingested correctly, you should see rows with timestamps and raw JSON.\nüõ†Ô∏è Troubleshooting Tips ‚Ä¢ Go to the Deployments tab to check if the latest deployment Completed successfully. ‚Ä¢ Check the Logs tab to view debug output (especially helpful if DEBUG=TRUE is set). ‚Ä¢ Review the Cron Triggers to make sure they‚Äôre running on schedule. ‚Ä¢ Double-check the SITE_URL and DATABASE_URL for typos or incorrect formatting.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#whats-next",
    "href": "pipelines/flight-ingestion-service.html#whats-next",
    "title": "‚úàÔ∏è Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "Now that your Web2DB Flights service is live and filling your database with raw flight data, we‚Äôll set up the companion Web2DB Weather service to do the same with meteorological data. Together, these two data streams will power our future correlation and transformation services.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html",
    "href": "pipelines/db-service.html",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "In this pipeline, the Postgres service functions as the central knowledge repository for all collected and transformed data. It plays a dual role:\n\nRaw Data Storage ‚Äì Capturing unstructured API responses from external ingestion services (e.g., OpenSky Network for flights, Open-Meteo for weather).\nStructured Relational Modeling ‚Äì Housing clean, normalized data models post-transformation‚Äîready for querying, visualization, and API exposure.\n\n\n\n\n\nAs we build a system to detect ‚Äúno-fly windows‚Äù based on weather thresholds over Portland, Postgres serves as the long-term memory of the pipeline. In this step, we‚Äôll deploy it using Railway and connect via Beekeeper Studio to validate connectivity and inspect future data.\n\n\n\n\nBefore proceeding, make sure you have:\n\nA GitHub account.\nA Railway account with at least Hobby tier access.\nThis requires a $5 credit deposit using a credit card.\nBeekeeper Studio installed on your computer.\nBeekeeper is an open-source desktop SQL client for inspecting and querying PostgreSQL databases.\n\n\n\n\n\n\n\n\nLog in at railway.app.\nClick ‚ÄúNew Project‚Äù.\nSelect ‚ÄúBlank Project‚Äù.\nName your project (e.g., no-fly-pipeline) and click ‚ÄúCreate Project‚Äù.\n\n\n\n\n\n\nIn your project dashboard, click ‚ÄúNew‚Äù to add a service.\nChoose ‚ÄúDatabase‚Äù, then click ‚ÄúPostgreSQL‚Äù.\nRailway will now deploy your Postgres service.\n\nOnce provisioned, this database becomes the shared backend for all services in your pipeline.\n\n\n\n\n\nClick into the Postgres service from the project dashboard.\nOpen the Variables tab.\nCopy the DATABASE_PUBLIC_URL ‚Äî it will look something like: postgresql://postgres:&lt;some-passsword&gt;@&lt;some-server&gt;.proxy.rlwy.net:20848/railway.\n\nThis URL is your access point for Beekeeper and other services that will connect to the database.\n\n\n\n\nYou‚Äôll use Beekeeper Studio to explore and verify your connection to the database.\n\n\nOpen the app and click ‚ÄúNew Connection‚Äù.\n\n\n\n\nChoose ‚ÄúPostgreSQL‚Äù from the connection type options.\nClick ‚ÄúImport from URL‚Äù (top-right or near the bottom of the connection screen).\nPaste the DATABASE_PUBLIC_URL from Railway.\nClick ‚ÄúConnect‚Äù.\n\n\n\n\nOnce connected, you‚Äôll see an empty database. That‚Äôs expected‚Äîyour ingestion and transformation services will populate it in later steps. For now, you can:\n\nTest SQL queries.\nMonitor schema evolution.\nVerify your database is reachable from your local environment.\n\n\n\n\n\n\nBefore data ingestion can begin, you need to create two tables in Postgres to hold the raw, unstructured API responses from the flight and weather ingestion services. These tables will serve as append-only logs that store the full JSON payloads alongside timestamps for later transformation.\nWe‚Äôll create:\n\nflight_json_data: stores data retrieved from the OpenSky Network.\nweather_json_data: stores data retrieved from the Open-Meteo API.\n\n\n\n\n\n\nEach table includes:\n\nid: an auto-incrementing primary key.\nraw_json: the unmodified API response stored as a jsonb object.\ntimestamptz: a timestamp indicating when the data was ingested.\n\n\n\nPaste the following SQL into Beekeeper Studio and execute it to create both tables:\nCREATE TABLE flight_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\nCREATE TABLE weather_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\n\n\n\n\nNow that your Postgres service is up and running, it‚Äôs ready to accept incoming unstructured data from your ingestion services: - ‚úàÔ∏è Web2DB Flights: gathers flight telemetry from the OpenSky Network. - üåßÔ∏è Web2DB Weather: gathers Portland weather conditions from Open-Meteo.\nThese services will write raw JSON payloads to the database every 5 minutes. Later, the DataTransform service will structure that data hourly into relational tables optimized for analysis.\nIn the next step, we‚Äôll configure the Web2DB Flights service to begin live ingestion.\nüìö References - https://docs.railway.app/guides/postgresql - https://www.beekeeperstudio.io/ - https://docs.railway.app/databases/external-access",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#overview",
    "href": "pipelines/db-service.html#overview",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "In this pipeline, the Postgres service functions as the central knowledge repository for all collected and transformed data. It plays a dual role:\n\nRaw Data Storage ‚Äì Capturing unstructured API responses from external ingestion services (e.g., OpenSky Network for flights, Open-Meteo for weather).\nStructured Relational Modeling ‚Äì Housing clean, normalized data models post-transformation‚Äîready for querying, visualization, and API exposure.",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#overview-continued",
    "href": "pipelines/db-service.html#overview-continued",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "As we build a system to detect ‚Äúno-fly windows‚Äù based on weather thresholds over Portland, Postgres serves as the long-term memory of the pipeline. In this step, we‚Äôll deploy it using Railway and connect via Beekeeper Studio to validate connectivity and inspect future data.",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#prerequisites",
    "href": "pipelines/db-service.html#prerequisites",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "Before proceeding, make sure you have:\n\nA GitHub account.\nA Railway account with at least Hobby tier access.\nThis requires a $5 credit deposit using a credit card.\nBeekeeper Studio installed on your computer.\nBeekeeper is an open-source desktop SQL client for inspecting and querying PostgreSQL databases.",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#steps-to-set-up-the-postgres-service",
    "href": "pipelines/db-service.html#steps-to-set-up-the-postgres-service",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "Log in at railway.app.\nClick ‚ÄúNew Project‚Äù.\nSelect ‚ÄúBlank Project‚Äù.\nName your project (e.g., no-fly-pipeline) and click ‚ÄúCreate Project‚Äù.\n\n\n\n\n\n\nIn your project dashboard, click ‚ÄúNew‚Äù to add a service.\nChoose ‚ÄúDatabase‚Äù, then click ‚ÄúPostgreSQL‚Äù.\nRailway will now deploy your Postgres service.\n\nOnce provisioned, this database becomes the shared backend for all services in your pipeline.\n\n\n\n\n\nClick into the Postgres service from the project dashboard.\nOpen the Variables tab.\nCopy the DATABASE_PUBLIC_URL ‚Äî it will look something like: postgresql://postgres:&lt;some-passsword&gt;@&lt;some-server&gt;.proxy.rlwy.net:20848/railway.\n\nThis URL is your access point for Beekeeper and other services that will connect to the database.\n\n\n\n\nYou‚Äôll use Beekeeper Studio to explore and verify your connection to the database.\n\n\nOpen the app and click ‚ÄúNew Connection‚Äù.\n\n\n\n\nChoose ‚ÄúPostgreSQL‚Äù from the connection type options.\nClick ‚ÄúImport from URL‚Äù (top-right or near the bottom of the connection screen).\nPaste the DATABASE_PUBLIC_URL from Railway.\nClick ‚ÄúConnect‚Äù.\n\n\n\n\nOnce connected, you‚Äôll see an empty database. That‚Äôs expected‚Äîyour ingestion and transformation services will populate it in later steps. For now, you can:\n\nTest SQL queries.\nMonitor schema evolution.\nVerify your database is reachable from your local environment.\n\n\n\n\n\n\nBefore data ingestion can begin, you need to create two tables in Postgres to hold the raw, unstructured API responses from the flight and weather ingestion services. These tables will serve as append-only logs that store the full JSON payloads alongside timestamps for later transformation.\nWe‚Äôll create:\n\nflight_json_data: stores data retrieved from the OpenSky Network.\nweather_json_data: stores data retrieved from the Open-Meteo API.",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#raw-json-storage-continued",
    "href": "pipelines/db-service.html#raw-json-storage-continued",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "Each table includes:\n\nid: an auto-incrementing primary key.\nraw_json: the unmodified API response stored as a jsonb object.\ntimestamptz: a timestamp indicating when the data was ingested.\n\n\n\nPaste the following SQL into Beekeeper Studio and execute it to create both tables:\nCREATE TABLE flight_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\nCREATE TABLE weather_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#whats-next",
    "href": "pipelines/db-service.html#whats-next",
    "title": "üíΩ Database Service Setup",
    "section": "",
    "text": "Now that your Postgres service is up and running, it‚Äôs ready to accept incoming unstructured data from your ingestion services: - ‚úàÔ∏è Web2DB Flights: gathers flight telemetry from the OpenSky Network. - üåßÔ∏è Web2DB Weather: gathers Portland weather conditions from Open-Meteo.\nThese services will write raw JSON payloads to the database every 5 minutes. Later, the DataTransform service will structure that data hourly into relational tables optimized for analysis.\nIn the next step, we‚Äôll configure the Web2DB Flights service to begin live ingestion.\nüìö References - https://docs.railway.app/guides/postgresql - https://www.beekeeperstudio.io/ - https://docs.railway.app/databases/external-access",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "psql/index.html",
    "href": "psql/index.html",
    "title": "PSQL",
    "section": "",
    "text": "Advanced PSQL Features\nCopying CSV from Local to Remote\nPerformance Tuning\nData Mining and Regex"
  },
  {
    "objectID": "psql/mining.html#what-is-data-mining",
    "href": "psql/mining.html#what-is-data-mining",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "What is Data Mining?",
    "text": "What is Data Mining?\n\nThe process of discovering patterns in large datasets\nInvolves techniques from:\n\nStatistics\nMachine Learning\nPattern Recognition\n\nPostgreSQL provides extensible SQL querying capabilities perfect for early-stage data mining",
    "crumbs": [
      "Data Mining and REGEX"
    ]
  },
  {
    "objectID": "psql/mining.html#the-pagila-database",
    "href": "psql/mining.html#the-pagila-database",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "The Pagila Database",
    "text": "The Pagila Database\n\nInspired by the Sakila database from MySQL\nContains:\n\nFilms, actors, and categories\nRentals and payments\nCustomers and their demographics",
    "crumbs": [
      "Data Mining and REGEX"
    ]
  },
  {
    "objectID": "psql/mining.html#regular-expressions-in-postgresql",
    "href": "psql/mining.html#regular-expressions-in-postgresql",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Regular Expressions in PostgreSQL",
    "text": "Regular Expressions in PostgreSQL\n\n~ : match regex\n\n~* : match case-insensitive\n\n!~ : does not match regex\n\n!~* : does not match case-insensitive",
    "crumbs": [
      "Data Mining and REGEX"
    ]
  },
  {
    "objectID": "psql/mining.html#examples",
    "href": "psql/mining.html#examples",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Examples",
    "text": "Examples",
    "crumbs": [
      "Data Mining and REGEX"
    ]
  },
  {
    "objectID": "psql/mining.html#case-insensitive-matching",
    "href": "psql/mining.html#case-insensitive-matching",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Case-Insensitive Matching",
    "text": "Case-Insensitive Matching",
    "crumbs": [
      "Data Mining and REGEX"
    ]
  },
  {
    "objectID": "psql/mining.html#group-films-by-those-containing-love-vs-not",
    "href": "psql/mining.html#group-films-by-those-containing-love-vs-not",
    "title": "‚õèÔ∏è Data Mining with PostgreSQL",
    "section": "Group films by those containing ‚Äòlove‚Äô vs not",
    "text": "Group films by those containing ‚Äòlove‚Äô vs not\nSELECT\n  CASE WHEN title ~* 'love' THEN 'Contains Love'\n       ELSE 'Does Not Contain Love' END AS love_category,\n  COUNT(*) AS film_count\nFROM film\nGROUP BY 1;",
    "crumbs": [
      "Data Mining and REGEX"
    ]
  },
  {
    "objectID": "psql/adv-psql.html",
    "href": "psql/adv-psql.html",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "",
    "text": "For this lecture, we use the Pagila database ‚Äî a PostgreSQL-compatible version of the classic Sakila DVD rental database.\nDownload the SQL file from Canvas to follow along.\nTo load it into Postgres: 1. Create a new database called pagila_dvd. 2. Execute the contents of the SQL file you downloaded."
  },
  {
    "objectID": "psql/adv-psql.html#dataset-used",
    "href": "psql/adv-psql.html#dataset-used",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "",
    "text": "For this lecture, we use the Pagila database ‚Äî a PostgreSQL-compatible version of the classic Sakila DVD rental database.\nDownload the SQL file from Canvas to follow along.\nTo load it into Postgres: 1. Create a new database called pagila_dvd. 2. Execute the contents of the SQL file you downloaded."
  },
  {
    "objectID": "psql/adv-psql.html#learning-objectives",
    "href": "psql/adv-psql.html#learning-objectives",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "2 Learning Objectives",
    "text": "2 Learning Objectives\n\nUnderstand and create Views in PostgreSQL\nUnderstand and create User-Defined Functions\nUnderstand and create Stored Procedures\nUnderstand and create Triggers\nSee practical examples using the Pagila dataset"
  },
  {
    "objectID": "psql/adv-psql.html#views-in-postgresql",
    "href": "psql/adv-psql.html#views-in-postgresql",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "3 Views in PostgreSQL",
    "text": "3 Views in PostgreSQL\nA View is a stored query that you can treat like a table.\n\n3.1 Why Use Views?\n\nSimplify complex queries\nReuse SQL logic\nAdd a security layer\nEncapsulate business logic"
  },
  {
    "objectID": "psql/adv-psql.html#creating-a-simple-view",
    "href": "psql/adv-psql.html#creating-a-simple-view",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "4 Creating a Simple View",
    "text": "4 Creating a Simple View\n\n4.1 Example: Top 5 Most Rented Films\nCREATE VIEW top_5_rented_films AS\nSELECT f.title, COUNT(r.rental_id) AS rental_count\nFROM film f\nJOIN inventory i ON f.film_id = i.film_id\nJOIN rental r ON i.inventory_id = r.inventory_id\nGROUP BY f.title\nORDER BY rental_count DESC\nLIMIT 5;\nYou can now query it like a table:\nSELECT * FROM top_5_rented_films;"
  },
  {
    "objectID": "psql/adv-psql.html#updatable-views",
    "href": "psql/adv-psql.html#updatable-views",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "5 Updatable Views",
    "text": "5 Updatable Views\nViews are read-only unless:\n\nBased on a single table\nNo aggregates or GROUP BY\nNo DISTINCT, LIMIT, OFFSET, JOIN\n\nYou can make them updatable with INSTEAD OF triggers."
  },
  {
    "objectID": "psql/adv-psql.html#functions-in-postgresql",
    "href": "psql/adv-psql.html#functions-in-postgresql",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "6 Functions in PostgreSQL",
    "text": "6 Functions in PostgreSQL\nUser-defined functions (UDFs) allow you to write reusable logic. They can return:\n\nScalar values\nTable results (set-returning functions)\n\nWritten in SQL or PL/pgSQL (PostgreSQL‚Äôs procedural language).\n\n6.1 Example: Scalar Function\nReturn number of films an actor has appeared in:\nCREATE FUNCTION film_count(actor_id INT) RETURNS INT AS $$\n  SELECT COUNT(*)\n  FROM film_actor\n  WHERE film_actor.actor_id = $1;\n$$ LANGUAGE SQL;\nUsage:\nSELECT film_count(1);\n\n\n6.2 Example: Table Function\nList all films rented by a customer:\nCREATE OR REPLACE FUNCTION customer_rentals(cust_id INT)\nRETURNS TABLE(title TEXT, rental_date TIMESTAMP) AS $$\nBEGIN\n  RETURN QUERY\n  SELECT f.title, r.rental_date::TIMESTAMP\n  FROM rental r\n  JOIN inventory i ON r.inventory_id = i.inventory_id\n  JOIN film f ON i.film_id = f.film_id\n  WHERE r.customer_id = cust_id;\nEND;\n$$ LANGUAGE plpgsql\nUsage:\nSELECT * FROM customer_rentals(1);"
  },
  {
    "objectID": "psql/adv-psql.html#stored-procedures",
    "href": "psql/adv-psql.html#stored-procedures",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "7 Stored Procedures",
    "text": "7 Stored Procedures\nProcedures are like functions but:\n\nDo not return values\nSupport transactions (e.g., COMMIT/ROLLBACK)\n\nUseful for batch operations, data migrations, etc.\n\n7.1 Example: Stored Procedure\nLet‚Äôs use a Stored Procedure to store deleted customers.\nFirst, let‚Äôs create a customer_deletion_log table.\nCREATE TABLE customer_deletion_log (\n    customer_id SERIAL PRIMARY KEY,\n    deleted_at TIMESTAMP default NOW()\n);\n\n\n7.2 Example: Stored Procedure\nNow, let‚Äôs create a Stored Procedure to delete a customer and log the deletion:\nCREATE PROCEDURE delete_customer(cust_id INT)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n  DELETE FROM payment WHERE customer_id = cust_id;\n  DELETE FROM rental WHERE customer_id = cust_id;\n  DELETE FROM customer WHERE customer_id = cust_id;\n  INSERT INTO customer_deletion_log(customer_id, deleted_at)\n  VALUES (cust_id, NOW());\nEND;\n$$;\nCall it with:\nCALL delete_customer(1);"
  },
  {
    "objectID": "psql/adv-psql.html#functions-vs-stored-procedures",
    "href": "psql/adv-psql.html#functions-vs-stored-procedures",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "8 Functions vs Stored Procedures",
    "text": "8 Functions vs Stored Procedures\nFeature | Function | Stored Procedure |\n||-|‚Äì| | Returns | Scalar or table data | No return (output via OUT params) | | Used in SQL | Yes (can be called in queries) | No (must use CALL) | | Transaction Control | No | Yes (can use COMMIT/ROLLBACK) | | Ideal Use Case | Computation, data transformation | Batch jobs, transactional workflows |\n\n8.1 When to Use Which?\n\nUse functions when you need return values or reusable logic inside queries.\nUse procedures for complex tasks that modify many rows, interact with multiple tables, or require transaction control."
  },
  {
    "objectID": "psql/adv-psql.html#triggers-in-postgresql",
    "href": "psql/adv-psql.html#triggers-in-postgresql",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "9 Triggers in PostgreSQL",
    "text": "9 Triggers in PostgreSQL\nA trigger runs a function automatically when a specified event occurs on a table.\n\n9.1 Events:\n\nBEFORE or AFTER\nINSERT, UPDATE, DELETE, or TRUNCATE"
  },
  {
    "objectID": "psql/adv-psql.html#trigger-example-log-customer-deletion",
    "href": "psql/adv-psql.html#trigger-example-log-customer-deletion",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "10 Trigger Example: Log Customer Deletion",
    "text": "10 Trigger Example: Log Customer Deletion\n\n10.1 Step 1: Create Log Table (if you haven‚Äôt already)\nCREATE TABLE customer_deletion_log (\n  customer_id INT,\n  deleted_at TIMESTAMP DEFAULT NOW()\n);"
  },
  {
    "objectID": "psql/adv-psql.html#create-trigger",
    "href": "psql/adv-psql.html#create-trigger",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "11 Create Trigger",
    "text": "11 Create Trigger\n\n11.1 Step 2: Create Trigger Function\nCREATE OR REPLACE FUNCTION log_customer_deletion()\nRETURNS TRIGGER AS $$\nBEGIN\n  INSERT INTO customer_deletion_log(customer_id, deleted_at)\n  VALUES (OLD.customer_id, NOW());\n  RETURN OLD;\nEND;\n$$ LANGUAGE plpgsql;"
  },
  {
    "objectID": "psql/adv-psql.html#create-trigger-1",
    "href": "psql/adv-psql.html#create-trigger-1",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "12 Create Trigger",
    "text": "12 Create Trigger\n\n12.1 Step 3: Attach Trigger to customer Table\nCREATE TRIGGER trg_log_customer_delete\nAFTER DELETE ON customer\nFOR EACH ROW\nEXECUTE FUNCTION log_customer_deletion();\n\n\n12.2 Test It!\nDELETE FROM customer WHERE customer_id = 1;\nSELECT * FROM customer_deletion_log;"
  },
  {
    "objectID": "psql/adv-psql.html#summary",
    "href": "psql/adv-psql.html#summary",
    "title": "üîç Postgres Advanced Features: Views, Functions, and Stored Procedures",
    "section": "13 Summary",
    "text": "13 Summary\n\nViews: Encapsulate reusable queries\nFunctions: Reusable logic that can return scalar/table data\nProcedures: Logic with transaction control, no return value\nTriggers: Automatically invoke logic on data events (e.g., inserts/deletes)"
  }
]