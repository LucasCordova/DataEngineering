[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Overview of the step"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Home",
    "section": "",
    "text": "Overview of the step"
  },
  {
    "objectID": "index.html#steps",
    "href": "index.html#steps",
    "title": "Home",
    "section": "2 Steps",
    "text": "2 Steps\nHere are the steps."
  },
  {
    "objectID": "pipelines/api-developer-portal-service.html",
    "href": "pipelines/api-developer-portal-service.html",
    "title": "API Developer Portal Service Setup",
    "section": "",
    "text": "Content goes here‚Ä¶",
    "crumbs": [
      "Home",
      "7. API Developer Portal Service Setup"
    ]
  },
  {
    "objectID": "pipelines/api-developer-portal-service.html#overview",
    "href": "pipelines/api-developer-portal-service.html#overview",
    "title": "API Developer Portal Service Setup",
    "section": "",
    "text": "Content goes here‚Ä¶",
    "crumbs": [
      "Home",
      "7. API Developer Portal Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html",
    "href": "pipelines/weather-ingestion-service.html",
    "title": "Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Weather ingestion service, which fetches live weather data for the Portland area from the Open-Meteo API and stores it in your Postgres database every five minutes.\nThis service collects unstructured weather readings‚Äîlike temperature and wind speed‚Äîand writes them to the weather_json_data table as raw JSON for later transformation and analysis. The data will later be joined with flight data to investigate correlations such as weather-driven no-fly windows.\n\n\n\n\nFor this demo, we‚Äôre using the following Open-Meteo endpoint:\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\n\nlatitude=45.52 and longitude=-122.68 pinpoint downtown Portland.\ncurrent=... specifies the current weather variables to return:\n\ntemperature_2m: air temperature 2 meters above the ground.\nwind_speed_10m: wind speed 10 meters above the ground.\nweathercode: numeric weather condition code (e.g., fog, rain, snow).\n\n\nüìö Learn more at:\nüîó Open-Meteo API Docs\n\n\n\n\n\n\n\n\nIn your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce the image is deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nweather_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\n‚úÖ Tip: Use Railway‚Äôs variable picker to link to your Postgres service when setting DATABASE_URL.\n\n\n\n\n\n\n\n\n\nGo to the Settings tab of the Web2DB Weather service.\nScroll to Triggers and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the trigger type.\nUse this schedule: */5 * * * *\n\nThis means the service will run every 5 minutes.\nüìö Reference: Crontab Guru ‚Äî an easy tool to preview cron expressions.\n\n\n\n\n\nOnce your cron trigger and environment variables are saved, the service will begin pulling weather data.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if you haven‚Äôt already).\nRun:\n\nSELECT * FROM weather_json_data ORDER BY timestamptz DESC LIMIT 10;\nYou should see recent entries showing full JSON responses from the Open-Meteo API.\n\n\n\n\n‚Ä¢   Check the Deployments tab in Railway to confirm successful runs.\n‚Ä¢   Use the Logs tab to inspect real-time debug output.\n‚Ä¢   Ensure the SITE_URL is formatted correctly (with no line breaks or spaces).\n‚Ä¢   If the table is still empty after 5‚Äì10 minutes, double-check the DATABASE_URL, TABLE_NAME, and cron settings.\n\n\n\nWith both Web2DB Flights and Web2DB Weather streaming live data into your Postgres instance, you‚Äôre ready to build the DataTransform service that will process this raw data into structured, relational models‚Äîmaking it easier to perform analytics and build dashboards.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#overview",
    "href": "pipelines/weather-ingestion-service.html#overview",
    "title": "Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Weather ingestion service, which fetches live weather data for the Portland area from the Open-Meteo API and stores it in your Postgres database every five minutes.\nThis service collects unstructured weather readings‚Äîlike temperature and wind speed‚Äîand writes them to the weather_json_data table as raw JSON for later transformation and analysis. The data will later be joined with flight data to investigate correlations such as weather-driven no-fly windows.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#about-the-open-meteo-api-url",
    "href": "pipelines/weather-ingestion-service.html#about-the-open-meteo-api-url",
    "title": "Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "For this demo, we‚Äôre using the following Open-Meteo endpoint:\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\n\nlatitude=45.52 and longitude=-122.68 pinpoint downtown Portland.\ncurrent=... specifies the current weather variables to return:\n\ntemperature_2m: air temperature 2 meters above the ground.\nwind_speed_10m: wind speed 10 meters above the ground.\nweathercode: numeric weather condition code (e.g., fog, rain, snow).\n\n\nüìö Learn more at:\nüîó Open-Meteo API Docs",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#deploying-the-web2db-weather-service",
    "href": "pipelines/weather-ingestion-service.html#deploying-the-web2db-weather-service",
    "title": "Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "In your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce the image is deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://api.open-meteo.com/v1/forecast?latitude=45.52&longitude=-122.68&current=temperature_2m,wind_speed_10m,weathercode\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nweather_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\n‚úÖ Tip: Use Railway‚Äôs variable picker to link to your Postgres service when setting DATABASE_URL.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#scheduling-the-ingestion",
    "href": "pipelines/weather-ingestion-service.html#scheduling-the-ingestion",
    "title": "Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "Go to the Settings tab of the Web2DB Weather service.\nScroll to Triggers and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the trigger type.\nUse this schedule: */5 * * * *\n\nThis means the service will run every 5 minutes.\nüìö Reference: Crontab Guru ‚Äî an easy tool to preview cron expressions.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#testing-the-ingestion",
    "href": "pipelines/weather-ingestion-service.html#testing-the-ingestion",
    "title": "Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "Once your cron trigger and environment variables are saved, the service will begin pulling weather data.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if you haven‚Äôt already).\nRun:\n\nSELECT * FROM weather_json_data ORDER BY timestamptz DESC LIMIT 10;\nYou should see recent entries showing full JSON responses from the Open-Meteo API.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#troubleshooting-tips",
    "href": "pipelines/weather-ingestion-service.html#troubleshooting-tips",
    "title": "Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "‚Ä¢   Check the Deployments tab in Railway to confirm successful runs.\n‚Ä¢   Use the Logs tab to inspect real-time debug output.\n‚Ä¢   Ensure the SITE_URL is formatted correctly (with no line breaks or spaces).\n‚Ä¢   If the table is still empty after 5‚Äì10 minutes, double-check the DATABASE_URL, TABLE_NAME, and cron settings.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/weather-ingestion-service.html#whats-next",
    "href": "pipelines/weather-ingestion-service.html#whats-next",
    "title": "Web2DB Weather Ingestion Setup",
    "section": "",
    "text": "With both Web2DB Flights and Web2DB Weather streaming live data into your Postgres instance, you‚Äôre ready to build the DataTransform service that will process this raw data into structured, relational models‚Äîmaking it easier to perform analytics and build dashboards.",
    "crumbs": [
      "Home",
      "3. Weather Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-api-bridge-service.html",
    "href": "pipelines/db-api-bridge-service.html",
    "title": "DB/API Bridge Service",
    "section": "",
    "text": "Content goes here‚Ä¶",
    "crumbs": [
      "Home",
      "6. DB/API Bridge Service Setup"
    ]
  },
  {
    "objectID": "pipelines/dashboard-service.html",
    "href": "pipelines/dashboard-service.html",
    "title": "Dashboard Service Setup",
    "section": "",
    "text": "Content goes here‚Ä¶",
    "crumbs": [
      "Home",
      "5. Dashboard Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html",
    "href": "pipelines/transformation-service.html",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "In this section, you‚Äôll design and deploy a transformation service called DataTransform. This service will extract meaningful fields from your raw JSON flight and weather data and insert them into clean, structured tables in your Postgres database.\nThis transformation process involves: - Understanding the structure of raw JSON. - Designing normalized physical tables for analysis. - Writing SQL to extract values using Postgres JSON functions. - Automating the transformation with a scheduled job in Railway.\n\n\n\n\nBefore transforming the data, you must understand what the raw JSON looks like. You can do this by running:\nSELECT raw_json FROM flight_json_data ORDER BY timestamptz DESC LIMIT 1;\nSELECT raw_json FROM weather_json_data ORDER BY timestamptz DESC LIMIT 1;\nExplore the shape and nesting of each payload. For example: - OpenSky states is an array of arrays. OpenSky JSON Structure - Open-Meteo JSON is a nested object. Open-Meteo Example\nüìö Reference: PostgreSQL JSON Functions and Operators\n\n\n\nBased on the JSON contents, you‚Äôll need normalized tables to store flight and weather data. Here‚Äôs the schema you‚Äôll use:\nCREATE TABLE flight_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE weather_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Structured output tables\nCREATE TABLE flights (\n    id SERIAL PRIMARY KEY,\n    icao24 VARCHAR(10) NOT NULL,\n    callsign VARCHAR(10),\n    country VARCHAR(64),\n    latitude DOUBLE PRECISION,\n    longitude DOUBLE PRECISION,\n    altitude_meters DOUBLE PRECISION,\n    velocity_knots DOUBLE PRECISION,\n    heading_degrees DOUBLE PRECISION,\n    vertical_rate DOUBLE PRECISION,\n    timestamp TIMESTAMPTZ NOT NULL\n);\n\nCREATE TABLE weather_observations (\n    id SERIAL PRIMARY KEY,\n    latitude DOUBLE PRECISION NOT NULL,\n    longitude DOUBLE PRECISION NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL,\n    precipitation_mm DOUBLE PRECISION,\n    weathercode SMALLINT\n);\n\nCREATE TABLE weather_condition (\n    code SMALLINT PRIMARY KEY,\n    description TEXT\n);\nThese tables will be the targets of your transformation logic.\n\n\n\nWhat‚Äôs a Common Table Expression?\nA Common Table Expression (CTE) is a temporary result set defined at the beginning of a SQL statement. It helps modularize complex queries and allows you to: - Break down a multi-step transformation into readable pieces. - Reference intermediate results multiple times.\nüìö Reference: CTEs in PostgreSQL\nExample Transformation for Flights\nBEGIN;\n\nWITH raw AS (\n    SELECT id, timestamptz, jsonb_array_elements(raw_json-&gt;'states') AS state\n    FROM flight_json_data\n),\nparsed AS (\n    SELECT\n        id,\n        state-&gt;&gt;0 AS icao24,\n        state-&gt;&gt;1 AS callsign,\n        state-&gt;&gt;2 AS country,\n        (state-&gt;&gt;6)::DOUBLE PRECISION AS longitude,\n        (state-&gt;&gt;5)::DOUBLE PRECISION AS latitude,\n        (state-&gt;&gt;7)::DOUBLE PRECISION AS altitude_meters,\n        (state-&gt;&gt;9)::DOUBLE PRECISION AS velocity_knots,\n        (state-&gt;&gt;10)::DOUBLE PRECISION AS heading_degrees,\n        (state-&gt;&gt;11)::DOUBLE PRECISION AS vertical_rate,\n        timestamptz\n    FROM raw\n)\nINSERT INTO flights (\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamp\n)\nSELECT\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamptz\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM flight_json_data;\n\nCOMMIT;\nExample Transformation for Weather\nBEGIN;\n\nWITH raw AS (\n    SELECT id, raw_json, timestamptz\n    FROM weather_json_data\n),\nparsed AS (\n    SELECT\n        (raw_json-&gt;'latitude')::DOUBLE PRECISION AS latitude,\n        (raw_json-&gt;'longitude')::DOUBLE PRECISION AS longitude,\n        (raw_json-&gt;'current'-&gt;'precipitation')::DOUBLE PRECISION AS precipitation_mm,\n        (raw_json-&gt;'current'-&gt;'weathercode')::SMALLINT AS weathercode,\n        timestamptz AS timestamp\n    FROM raw\n)\nINSERT INTO weather_observations (\n    latitude, longitude, precipitation_mm, weathercode, timestamp\n)\nSELECT\n    latitude, longitude, precipitation_mm, weathercode, timestamp\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM weather_json_data;\n\nCOMMIT;\n\n\nA TRANSACTION ensures that either all your steps complete successfully, or none of them are applied. This prevents partial writes and data corruption. ‚Ä¢ BEGIN; starts the transaction. ‚Ä¢ COMMIT; applies all changes if no error occurred. ‚Ä¢ If an error occurs, the entire transaction can be rolled back.\n\n\n\n\n1.  Visit: [https://github.com/LucasCordova/db_transform](https://github.com/LucasCordova/db_transform).\n2.  Click the ‚ÄúUse this template‚Äù button.\n3.  Name your repository: `weather_flight_db_transform`\n4.  Choose your GitHub account and click Create Repository.\n\n\n\n1.  Open the new repository on GitHub (recommended) or clone it locally.\n2.  Edit the clean.sql file. If you are editing in the browser, choose the `pencil` icon. Paste your flight and weather transformation SQL inside it.\n3.  Save the file and commit. In the browser editor, click the Commit changes... button, add a message, and click the final Commit changes button.\n\n\n\n1.  Go to your Railway project.\n2.  Click New ‚Üí Deploy from GitHub Repo.\n3.  Click ‚ÄúConfigure GitHub App‚Äù to give Railway access to your GitHub account.\n4.  Choose the repository weather_flight_db_transform.\n5.  Click Deploy.\n\n*Add Environment Variable*\n    Key: DATABASE_URL\n    Value: ${{Postgres.DATABASE_PUBLIC_URL}}\n\n\n\n1.  Go to the Settings tab of the DataTransform service.\n2.  Scroll to Triggers ‚Üí New Trigger.\n3.  Choose Cron and enter: `0 * * * *`.\nThis means the service runs once every hour, at the top of the hour.\n\n\n\n‚úÖ Check the Tables in Beekeeper\n\n    After the first cron run, inspect the structured tables:\n\n    ```sql\n    SELECT * FROM flights ORDER BY timestamp DESC LIMIT 10;\n    SELECT * FROM weather_observations ORDER BY timestamp DESC LIMIT 10;\n    ```\n\n‚úÖ View Logs in Railway\n    Use the Logs tab to inspect any errors or debug output.\n\n\n\nNow that your structured data is flowing, you can visualize and analyze it in the next step using Grafana Dashboards. You‚Äôll use SQL queries to explore patterns in flight activity and weather data‚Äîlaying the foundation for answering your core research question.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#overview",
    "href": "pipelines/transformation-service.html#overview",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "In this section, you‚Äôll design and deploy a transformation service called DataTransform. This service will extract meaningful fields from your raw JSON flight and weather data and insert them into clean, structured tables in your Postgres database.\nThis transformation process involves: - Understanding the structure of raw JSON. - Designing normalized physical tables for analysis. - Writing SQL to extract values using Postgres JSON functions. - Automating the transformation with a scheduled job in Railway.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#understand-the-json-structure",
    "href": "pipelines/transformation-service.html#understand-the-json-structure",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "Before transforming the data, you must understand what the raw JSON looks like. You can do this by running:\nSELECT raw_json FROM flight_json_data ORDER BY timestamptz DESC LIMIT 1;\nSELECT raw_json FROM weather_json_data ORDER BY timestamptz DESC LIMIT 1;\nExplore the shape and nesting of each payload. For example: - OpenSky states is an array of arrays. OpenSky JSON Structure - Open-Meteo JSON is a nested object. Open-Meteo Example\nüìö Reference: PostgreSQL JSON Functions and Operators",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#design-your-physical-schema",
    "href": "pipelines/transformation-service.html#design-your-physical-schema",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "Based on the JSON contents, you‚Äôll need normalized tables to store flight and weather data. Here‚Äôs the schema you‚Äôll use:\nCREATE TABLE flight_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE weather_json_data (\n    id SERIAL PRIMARY KEY,\n    raw_json JSONB NOT NULL,\n    timestamptz TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Structured output tables\nCREATE TABLE flights (\n    id SERIAL PRIMARY KEY,\n    icao24 VARCHAR(10) NOT NULL,\n    callsign VARCHAR(10),\n    country VARCHAR(64),\n    latitude DOUBLE PRECISION,\n    longitude DOUBLE PRECISION,\n    altitude_meters DOUBLE PRECISION,\n    velocity_knots DOUBLE PRECISION,\n    heading_degrees DOUBLE PRECISION,\n    vertical_rate DOUBLE PRECISION,\n    timestamp TIMESTAMPTZ NOT NULL\n);\n\nCREATE TABLE weather_observations (\n    id SERIAL PRIMARY KEY,\n    latitude DOUBLE PRECISION NOT NULL,\n    longitude DOUBLE PRECISION NOT NULL,\n    timestamp TIMESTAMPTZ NOT NULL,\n    precipitation_mm DOUBLE PRECISION,\n    weathercode SMALLINT\n);\n\nCREATE TABLE weather_condition (\n    code SMALLINT PRIMARY KEY,\n    description TEXT\n);\nThese tables will be the targets of your transformation logic.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#write-sql-to-transform-the-data",
    "href": "pipelines/transformation-service.html#write-sql-to-transform-the-data",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "What‚Äôs a Common Table Expression?\nA Common Table Expression (CTE) is a temporary result set defined at the beginning of a SQL statement. It helps modularize complex queries and allows you to: - Break down a multi-step transformation into readable pieces. - Reference intermediate results multiple times.\nüìö Reference: CTEs in PostgreSQL\nExample Transformation for Flights\nBEGIN;\n\nWITH raw AS (\n    SELECT id, timestamptz, jsonb_array_elements(raw_json-&gt;'states') AS state\n    FROM flight_json_data\n),\nparsed AS (\n    SELECT\n        id,\n        state-&gt;&gt;0 AS icao24,\n        state-&gt;&gt;1 AS callsign,\n        state-&gt;&gt;2 AS country,\n        (state-&gt;&gt;6)::DOUBLE PRECISION AS longitude,\n        (state-&gt;&gt;5)::DOUBLE PRECISION AS latitude,\n        (state-&gt;&gt;7)::DOUBLE PRECISION AS altitude_meters,\n        (state-&gt;&gt;9)::DOUBLE PRECISION AS velocity_knots,\n        (state-&gt;&gt;10)::DOUBLE PRECISION AS heading_degrees,\n        (state-&gt;&gt;11)::DOUBLE PRECISION AS vertical_rate,\n        timestamptz\n    FROM raw\n)\nINSERT INTO flights (\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamp\n)\nSELECT\n    icao24, callsign, country, latitude, longitude,\n    altitude_meters, velocity_knots, heading_degrees,\n    vertical_rate, timestamptz\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM flight_json_data;\n\nCOMMIT;\nExample Transformation for Weather\nBEGIN;\n\nWITH raw AS (\n    SELECT id, raw_json, timestamptz\n    FROM weather_json_data\n),\nparsed AS (\n    SELECT\n        (raw_json-&gt;'latitude')::DOUBLE PRECISION AS latitude,\n        (raw_json-&gt;'longitude')::DOUBLE PRECISION AS longitude,\n        (raw_json-&gt;'current'-&gt;'precipitation')::DOUBLE PRECISION AS precipitation_mm,\n        (raw_json-&gt;'current'-&gt;'weathercode')::SMALLINT AS weathercode,\n        timestamptz AS timestamp\n    FROM raw\n)\nINSERT INTO weather_observations (\n    latitude, longitude, precipitation_mm, weathercode, timestamp\n)\nSELECT\n    latitude, longitude, precipitation_mm, weathercode, timestamp\nFROM parsed;\n\n-- Delete processed rows\nDELETE FROM weather_json_data;\n\nCOMMIT;\n\n\nA TRANSACTION ensures that either all your steps complete successfully, or none of them are applied. This prevents partial writes and data corruption. ‚Ä¢ BEGIN; starts the transaction. ‚Ä¢ COMMIT; applies all changes if no error occurred. ‚Ä¢ If an error occurs, the entire transaction can be rolled back.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#use-the-github-template",
    "href": "pipelines/transformation-service.html#use-the-github-template",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "1.  Visit: [https://github.com/LucasCordova/db_transform](https://github.com/LucasCordova/db_transform).\n2.  Click the ‚ÄúUse this template‚Äù button.\n3.  Name your repository: `weather_flight_db_transform`\n4.  Choose your GitHub account and click Create Repository.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#add-your-sql-to-clean.sql",
    "href": "pipelines/transformation-service.html#add-your-sql-to-clean.sql",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "1.  Open the new repository on GitHub (recommended) or clone it locally.\n2.  Edit the clean.sql file. If you are editing in the browser, choose the `pencil` icon. Paste your flight and weather transformation SQL inside it.\n3.  Save the file and commit. In the browser editor, click the Commit changes... button, add a message, and click the final Commit changes button.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#deploy-the-service-in-railway",
    "href": "pipelines/transformation-service.html#deploy-the-service-in-railway",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "1.  Go to your Railway project.\n2.  Click New ‚Üí Deploy from GitHub Repo.\n3.  Click ‚ÄúConfigure GitHub App‚Äù to give Railway access to your GitHub account.\n4.  Choose the repository weather_flight_db_transform.\n5.  Click Deploy.\n\n*Add Environment Variable*\n    Key: DATABASE_URL\n    Value: ${{Postgres.DATABASE_PUBLIC_URL}}",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#set-cron-schedule",
    "href": "pipelines/transformation-service.html#set-cron-schedule",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "1.  Go to the Settings tab of the DataTransform service.\n2.  Scroll to Triggers ‚Üí New Trigger.\n3.  Choose Cron and enter: `0 * * * *`.\nThis means the service runs once every hour, at the top of the hour.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#testing-and-validation",
    "href": "pipelines/transformation-service.html#testing-and-validation",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "‚úÖ Check the Tables in Beekeeper\n\n    After the first cron run, inspect the structured tables:\n\n    ```sql\n    SELECT * FROM flights ORDER BY timestamp DESC LIMIT 10;\n    SELECT * FROM weather_observations ORDER BY timestamp DESC LIMIT 10;\n    ```\n\n‚úÖ View Logs in Railway\n    Use the Logs tab to inspect any errors or debug output.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/transformation-service.html#whats-next",
    "href": "pipelines/transformation-service.html#whats-next",
    "title": "Data Transformation Service Setup",
    "section": "",
    "text": "Now that your structured data is flowing, you can visualize and analyze it in the next step using Grafana Dashboards. You‚Äôll use SQL queries to explore patterns in flight activity and weather data‚Äîlaying the foundation for answering your core research question.",
    "crumbs": [
      "Home",
      "4. Data Transformation Service Setup"
    ]
  },
  {
    "objectID": "pipelines/index.html",
    "href": "pipelines/index.html",
    "title": "Pipelines",
    "section": "",
    "text": "The ability to synthesize new knowledge by integrating disparate data sources is a powerful asset. This guide will walk you through the design and deployment of a data pipeline architecture on Railway.app, a cloud-native development platform that allows you to quickly spin up backend infrastructure with minimal configuration overhead.\nThe goal of this pipeline is to collect, structure, and expose data in a way that supports meaningful analysis and knowledge generation. By combining unstructured and structured data from multiple services into a single relational data model, this system enables real-time insights via a customizable dashboard and a developer-friendly REST API.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/index.html#overview",
    "href": "pipelines/index.html#overview",
    "title": "Pipelines",
    "section": "",
    "text": "The ability to synthesize new knowledge by integrating disparate data sources is a powerful asset. This guide will walk you through the design and deployment of a data pipeline architecture on Railway.app, a cloud-native development platform that allows you to quickly spin up backend infrastructure with minimal configuration overhead.\nThe goal of this pipeline is to collect, structure, and expose data in a way that supports meaningful analysis and knowledge generation. By combining unstructured and structured data from multiple services into a single relational data model, this system enables real-time insights via a customizable dashboard and a developer-friendly REST API.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/index.html#scenario-detecting-no-fly-windows-correlated-with-weather-thresholds",
    "href": "pipelines/index.html#scenario-detecting-no-fly-windows-correlated-with-weather-thresholds",
    "title": "Pipelines",
    "section": "2 Scenario Detecting ‚ÄúNo-Fly Windows‚Äù Correlated with Weather Thresholds",
    "text": "2 Scenario Detecting ‚ÄúNo-Fly Windows‚Äù Correlated with Weather Thresholds\n\n2.1 Research Question\nCan we identify specific weather conditions that correlate with significantly reduced flight activity‚Äîwhat we‚Äôll call ‚Äúno-fly windows‚Äù‚Äîover the Portland metro region? These windows may align with thresholds like heavy precipitation, low visibility, or high wind speeds. To answer this question, we‚Äôll construct a data pipeline that combines live flight and weather data, transforms it into a relational model, and exposes it through dashboards and APIs for analysis and discovery.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/index.html#architecture",
    "href": "pipelines/index.html#architecture",
    "title": "Pipelines",
    "section": "3 Architecture",
    "text": "3 Architecture\nYou will build an architecture composed of six distinct services as illustrated below.\n\n\n\n\n\n   graph TD\n    subgraph Railway.app\n        Postgres[(Postgres Database)]\n        Web2DB1[Web2DB - Scraper/API Service #1 - Flights]\n        Web2DB2[Web2DB - Scraper/API Service #2 - Weather]\n        DataTransform[DataTransform Service]\n        Grafana[Grafana Dashboard]\n        DB2API[PostgREST - DB2API Bridge]\n        Swagger[Swagger UI - API Dev Tool]\n    end\n\n    Web2DB1 --&gt; Postgres\n    Web2DB2 --&gt; Postgres\n    Postgres --&gt; DataTransform\n    DataTransform --&gt; Postgres\n    Postgres --&gt; Grafana\n    Postgres --&gt; DB2API\n    DB2API --&gt; Swagger\n\n\n\n\n\n\n\n3.1 How Each Service Contributes\n\n3.1.1 üõ¢Ô∏è Postgres Database\nThe Postgres database is the central data repository. It stores: - Unstructured data: Raw JSON or payloads directly from the APIs during collection. - Structured data: Normalized tables and time-series records created after transformation. This separation allows for historical data to be archived while maintaining clean, queryable models for analysis.\n\n\n3.1.2 ‚úàÔ∏è Web2DB Flights\nThe Web2DB Flights ingestion service calls the OpenSky Network API every 5 minutes to gather live air traffic data over the Portland metro area. It logs: - Aircraft positions - Altitudes and speeds - Callsigns and ICAO codes - Timestamps and bounding box info This unstructured data is stored directly into Postgres for later processing.\n\n\n3.1.3 üåßÔ∏è Web2DB Weather\nThe Web2DB Weather ingestion service calls the Open-Meteo API every 5 minutes, retrieving weather information for the Portland region. It collects: - Temperature, wind speed and direction - Visibility, cloud cover, and precipitation type - Timestamps and geo-coordinates This raw weather data is logged to the database alongside flight data, aligned by time and location.\n\n\n3.1.4 üîÑ Data Transformation Service (DataTransform)\nOnce per hour, the DataTransform service processes the accumulated unstructured data into a structured relational schema. Key transformations include: - Joining flight and weather records by timestamp - Extracting metrics such as flight count per interval, average altitude, or visibility index - Normalizing date/time formats and location metadata The result is a set of structured tables optimized for query performance and analytical depth.\n\n\n3.1.5 üìä Dashboard Service (Grafana)\nGrafana connects to the structured Postgres schema and generates interactive dashboards to visualize: - Flight activity (e.g., counts, density maps) over time - Weather variable trends - Correlation graphs between weather metrics (e.g., wind speed) and flight drop-offs These dashboards help identify possible ‚Äúno-fly windows‚Äù by aligning dips in flight activity with adverse weather thresholds.\n\n\n3.1.6 üåê DB/API Bridge Service (DB2API)\nUsing PostgREST, this service turns the structured database into a RESTful API. It allows: - Querying historical flight and weather data - Filtering based on date/time, weather conditions, or flight metrics - Serving clean, JSON-based endpoints for developers, analysts, or downstream systems\n\n\n3.1.7 üß™ API Developer Interface (Swagger)\nThe Swagger service documents the exposed API endpoints and provides an interactive UI for developers to: - Explore and test API requests in real-time - Understand available data models and query parameters - Integrate this data into custom applications, dashboards, or research tools\n\nTogether, this architecture enables rich, real-time and historical analysis to determine when and why flight activity slows or stops due to environmental conditions. The result is a powerful system for answering not only our current research question, but also a broader range of aviation and climate analytics challenges.\nHead to the next section, where we‚Äôll begin by setting up the Postgres database service and preparing the schema for incoming unstructured data.",
    "crumbs": [
      "Home",
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html",
    "href": "pipelines/flight-ingestion-service.html",
    "title": "Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Flights ingestion service, which fetches live flight data from the OpenSky Network API and stores it in your Postgres database every five minutes.\nThis service will collect unstructured flight telemetry data and append it to the flight_json_data table in raw JSON format. You‚Äôll deploy it as a container on Railway using the lucascordova/web2db image and configure it to run on a schedule.\n\n\n\n\nIf you‚Äôre building your own ingestion service using a different API, here‚Äôs the general process:\n\nCheck if the API requires authentication (e.g., an API key or access token).\nRead the API documentation to understand query parameters, rate limits, and response structure.\nBuild your request URL including any necessary geographic filters, authentication tokens, or query strings.\nTest the request manually (e.g., in your browser or Postman) before deploying it inside a service like Railway.\n\nFor APIs that require keys or tokens, you should always inject these as environment variables, never hardcoded in your code or Docker image.\n\n\n\n\nFor this demo, we‚Äôre using the following public OpenSky endpoint: `https://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00‚Äô\n\n\n\nlamin and lamax: latitude min/max (45.08 to 45.88)\nlomin and lomax: longitude min/max (‚àí123.50 to ‚àí122.00)\n\nThis bounding box covers the Portland metropolitan area and surrounding airspace.\nüìö Learn more at:\nüîó OpenSky Network REST API Documentation\n\n\n\n\n\n\n\n\nIn your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nflight_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\nüí° Use the variable picker in Railway to reference your Postgres service directly for DATABASE_URL.\n\n\n\n\n\n\n\n\n\nGo to the Settings tab of the Web2DB Flights service.\nScroll to the Triggers section and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the type.\nEnter the schedule: */5 * * * *\n\nThis means: ‚ÄúRun every 5 minutes.‚Äù\nüß† Cron Format Reference: - */5 = every 5 minutes\n- Full format is minute hour day month weekday\nüìö Crontab Guru is a great tool to preview and test your expressions.\n\n\n\n\n\nAfter saving the environment variables and trigger, your ingestion service should begin collecting data every 5 minutes.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if not already connected).\nRun the following SQL query:\n\nSELECT * FROM flight_json_data ORDER BY timestamptz DESC LIMIT 10;\nIf data is being ingested correctly, you should see rows with timestamps and raw JSON.\nüõ†Ô∏è Troubleshooting Tips ‚Ä¢ Go to the Deployments tab to check if the latest deployment Completed successfully. ‚Ä¢ Check the Logs tab to view debug output (especially helpful if DEBUG=TRUE is set). ‚Ä¢ Review the Cron Triggers to make sure they‚Äôre running on schedule. ‚Ä¢ Double-check the SITE_URL and DATABASE_URL for typos or incorrect formatting.\nWhat‚Äôs Next?\nNow that your Web2DB Flights service is live and filling your database with raw flight data, we‚Äôll set up the companion Web2DB Weather service to do the same with meteorological data. Together, these two data streams will power our future correlation and transformation services.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#overview",
    "href": "pipelines/flight-ingestion-service.html#overview",
    "title": "Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "In this step, you‚Äôll set up the Web2DB Flights ingestion service, which fetches live flight data from the OpenSky Network API and stores it in your Postgres database every five minutes.\nThis service will collect unstructured flight telemetry data and append it to the flight_json_data table in raw JSON format. You‚Äôll deploy it as a container on Railway using the lucascordova/web2db image and configure it to run on a schedule.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#using-public-apis-in-your-own-projects",
    "href": "pipelines/flight-ingestion-service.html#using-public-apis-in-your-own-projects",
    "title": "Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "If you‚Äôre building your own ingestion service using a different API, here‚Äôs the general process:\n\nCheck if the API requires authentication (e.g., an API key or access token).\nRead the API documentation to understand query parameters, rate limits, and response structure.\nBuild your request URL including any necessary geographic filters, authentication tokens, or query strings.\nTest the request manually (e.g., in your browser or Postman) before deploying it inside a service like Railway.\n\nFor APIs that require keys or tokens, you should always inject these as environment variables, never hardcoded in your code or Docker image.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#about-the-opensky-api-url",
    "href": "pipelines/flight-ingestion-service.html#about-the-opensky-api-url",
    "title": "Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "For this demo, we‚Äôre using the following public OpenSky endpoint: `https://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00‚Äô\n\n\n\nlamin and lamax: latitude min/max (45.08 to 45.88)\nlomin and lomax: longitude min/max (‚àí123.50 to ‚àí122.00)\n\nThis bounding box covers the Portland metropolitan area and surrounding airspace.\nüìö Learn more at:\nüîó OpenSky Network REST API Documentation",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#deploying-the-web2db-flights-service",
    "href": "pipelines/flight-ingestion-service.html#deploying-the-web2db-flights-service",
    "title": "Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "In your Railway project, click ‚ÄúNew‚Äù ‚Üí ‚ÄúDeploy from Docker Image‚Äù.\nEnter the image name: lucascordova/web2db\nClick Deploy.\n\n\n\n\n\nOnce deployed, go to the Variables tab and add the following:\n\n\n\n\n\n\n\nKey\nValue\n\n\n\n\nSITE_URL\nhttps://opensky-network.org/api/states/all?lamin=45.08&lomin=-123.50&lamax=45.88&lomax=-122.00\n\n\nDATABASE_URL\n${{Postgres.DATABASE_PUBLIC_URL}}\n\n\nTABLE_NAME\nflight_json_data\n\n\nDEBUG\nTRUE\n\n\n\n\nüí° Use the variable picker in Railway to reference your Postgres service directly for DATABASE_URL.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#scheduling-the-ingestion",
    "href": "pipelines/flight-ingestion-service.html#scheduling-the-ingestion",
    "title": "Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "Go to the Settings tab of the Web2DB Flights service.\nScroll to the Triggers section and click ‚ÄúNew Trigger‚Äù.\nChoose ‚ÄúCron‚Äù as the type.\nEnter the schedule: */5 * * * *\n\nThis means: ‚ÄúRun every 5 minutes.‚Äù\nüß† Cron Format Reference: - */5 = every 5 minutes\n- Full format is minute hour day month weekday\nüìö Crontab Guru is a great tool to preview and test your expressions.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/flight-ingestion-service.html#testing-the-ingestion",
    "href": "pipelines/flight-ingestion-service.html#testing-the-ingestion",
    "title": "Web2DB Flights Ingestion Setup",
    "section": "",
    "text": "After saving the environment variables and trigger, your ingestion service should begin collecting data every 5 minutes.\n\n\n\nOpen Beekeeper Studio.\nConnect to your Postgres database (if not already connected).\nRun the following SQL query:\n\nSELECT * FROM flight_json_data ORDER BY timestamptz DESC LIMIT 10;\nIf data is being ingested correctly, you should see rows with timestamps and raw JSON.\nüõ†Ô∏è Troubleshooting Tips ‚Ä¢ Go to the Deployments tab to check if the latest deployment Completed successfully. ‚Ä¢ Check the Logs tab to view debug output (especially helpful if DEBUG=TRUE is set). ‚Ä¢ Review the Cron Triggers to make sure they‚Äôre running on schedule. ‚Ä¢ Double-check the SITE_URL and DATABASE_URL for typos or incorrect formatting.\nWhat‚Äôs Next?\nNow that your Web2DB Flights service is live and filling your database with raw flight data, we‚Äôll set up the companion Web2DB Weather service to do the same with meteorological data. Together, these two data streams will power our future correlation and transformation services.",
    "crumbs": [
      "Home",
      "2. Flight Data Ingestion Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html",
    "href": "pipelines/db-service.html",
    "title": "Database Service Setup",
    "section": "",
    "text": "In this pipeline, the Postgres service functions as the central knowledge repository for all collected and transformed data. It plays a dual role:\n\nRaw Data Storage ‚Äì Capturing unstructured API responses from external ingestion services (e.g., OpenSky Network for flights, Open-Meteo for weather).\nStructured Relational Modeling ‚Äì Housing clean, normalized data models post-transformation‚Äîready for querying, visualization, and API exposure.\n\nAs we build a system to detect ‚Äúno-fly windows‚Äù based on weather thresholds over Portland, Postgres serves as the long-term memory of the pipeline. In this step, we‚Äôll deploy it using Railway and connect via Beekeeper Studio to validate connectivity and inspect future data.\n\n\n\n\nBefore proceeding, make sure you have:\n\nA GitHub account.\nA Railway account with at least Hobby tier access.\n&gt; This requires a $5 credit deposit using a credit card.\nBeekeeper Studio installed on your computer.\n&gt; Beekeeper is an open-source desktop SQL client for inspecting and querying PostgreSQL databases.\n\n\n\n\n\n\n\n\nLog in at railway.app.\nClick ‚ÄúNew Project‚Äù.\nSelect ‚ÄúBlank Project‚Äù.\nName your project (e.g., no-fly-pipeline) and click ‚ÄúCreate Project‚Äù.\n\n\n\n\n\n\nIn your project dashboard, click ‚ÄúNew‚Äù to add a service.\nChoose ‚ÄúDatabase‚Äù, then click ‚ÄúPostgreSQL‚Äù.\nRailway will now deploy your Postgres service.\n\n\n\nAdding PostgreSQL Service\n\n\n\nOnce provisioned, this database becomes the shared backend for all services in your pipeline.\n\n\n\n\n\nClick into the Postgres service from the project dashboard.\nOpen the Variables tab.\nCopy the DATABASE_PUBLIC_URL ‚Äî it will look something like: postgresql://postgres:&lt;some-passsword&gt;@&lt;some-server&gt;.proxy.rlwy.net:20848/railway. \n\nThis URL is your access point for Beekeeper and other services that will connect to the database.\n\n\n\n\nYou‚Äôll use Beekeeper Studio to explore and verify your connection to the database.\n\n\nOpen the app and click ‚ÄúNew Connection‚Äù.\n\n\n\n\nChoose ‚ÄúPostgreSQL‚Äù from the connection type options.\nClick ‚ÄúImport from URL‚Äù (top-right or near the bottom of the connection screen).\nPaste the DATABASE_PUBLIC_URL from Railway.\nClick ‚ÄúConnect‚Äù.\n\n\n\n\nBeekeeper Import URL Example\n\n\n\n\n\nOnce connected, you‚Äôll see an empty database. That‚Äôs expected‚Äîyour ingestion and transformation services will populate it in later steps. For now, you can:\n\nTest SQL queries.\nMonitor schema evolution.\nVerify your database is reachable from your local environment.\n\n\n\n\n\n\nBefore data ingestion can begin, you need to create two tables in Postgres to hold the raw, unstructured API responses from the flight and weather ingestion services. These tables will serve as append-only logs that store the full JSON payloads alongside timestamps for later transformation.\nWe‚Äôll create:\n\nflight_json_data: stores data retrieved from the OpenSky Network.\nweather_json_data: stores data retrieved from the Open-Meteo API.\n\nEach table includes:\n\nid: an auto-incrementing primary key.\nraw_json: the unmodified API response stored as a jsonb object.\ntimestamptz: a timestamp indicating when the data was ingested.\n\n\n\nPaste the following SQL into Beekeeper Studio and execute it to create both tables:\nCREATE TABLE flight_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\nCREATE TABLE weather_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\n\n\n\n\n\nNow that your Postgres service is up and running, it‚Äôs ready to accept incoming unstructured data from your ingestion services: - Web2DB Flights: gathers flight telemetry from the OpenSky Network. - Web2DB Weather: gathers Portland weather conditions from Open-Meteo.\nThese services will write raw JSON payloads to the database every 5 minutes. Later, the DataTransform service will structure that data hourly into relational tables optimized for analysis.\nIn the next step, we‚Äôll configure the Web2DB Flights service to begin live ingestion.\nüìö References - https://docs.railway.app/guides/postgresql - https://www.beekeeperstudio.io/ - https://docs.railway.app/databases/external-access",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#overview",
    "href": "pipelines/db-service.html#overview",
    "title": "Database Service Setup",
    "section": "",
    "text": "In this pipeline, the Postgres service functions as the central knowledge repository for all collected and transformed data. It plays a dual role:\n\nRaw Data Storage ‚Äì Capturing unstructured API responses from external ingestion services (e.g., OpenSky Network for flights, Open-Meteo for weather).\nStructured Relational Modeling ‚Äì Housing clean, normalized data models post-transformation‚Äîready for querying, visualization, and API exposure.\n\nAs we build a system to detect ‚Äúno-fly windows‚Äù based on weather thresholds over Portland, Postgres serves as the long-term memory of the pipeline. In this step, we‚Äôll deploy it using Railway and connect via Beekeeper Studio to validate connectivity and inspect future data.",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#prerequisites",
    "href": "pipelines/db-service.html#prerequisites",
    "title": "Database Service Setup",
    "section": "",
    "text": "Before proceeding, make sure you have:\n\nA GitHub account.\nA Railway account with at least Hobby tier access.\n&gt; This requires a $5 credit deposit using a credit card.\nBeekeeper Studio installed on your computer.\n&gt; Beekeeper is an open-source desktop SQL client for inspecting and querying PostgreSQL databases.",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#steps-to-set-up-the-postgres-service",
    "href": "pipelines/db-service.html#steps-to-set-up-the-postgres-service",
    "title": "Database Service Setup",
    "section": "",
    "text": "Log in at railway.app.\nClick ‚ÄúNew Project‚Äù.\nSelect ‚ÄúBlank Project‚Äù.\nName your project (e.g., no-fly-pipeline) and click ‚ÄúCreate Project‚Äù.\n\n\n\n\n\n\nIn your project dashboard, click ‚ÄúNew‚Äù to add a service.\nChoose ‚ÄúDatabase‚Äù, then click ‚ÄúPostgreSQL‚Äù.\nRailway will now deploy your Postgres service.\n\n\n\nAdding PostgreSQL Service\n\n\n\nOnce provisioned, this database becomes the shared backend for all services in your pipeline.\n\n\n\n\n\nClick into the Postgres service from the project dashboard.\nOpen the Variables tab.\nCopy the DATABASE_PUBLIC_URL ‚Äî it will look something like: postgresql://postgres:&lt;some-passsword&gt;@&lt;some-server&gt;.proxy.rlwy.net:20848/railway. \n\nThis URL is your access point for Beekeeper and other services that will connect to the database.\n\n\n\n\nYou‚Äôll use Beekeeper Studio to explore and verify your connection to the database.\n\n\nOpen the app and click ‚ÄúNew Connection‚Äù.\n\n\n\n\nChoose ‚ÄúPostgreSQL‚Äù from the connection type options.\nClick ‚ÄúImport from URL‚Äù (top-right or near the bottom of the connection screen).\nPaste the DATABASE_PUBLIC_URL from Railway.\nClick ‚ÄúConnect‚Äù.\n\n\n\n\nBeekeeper Import URL Example\n\n\n\n\n\nOnce connected, you‚Äôll see an empty database. That‚Äôs expected‚Äîyour ingestion and transformation services will populate it in later steps. For now, you can:\n\nTest SQL queries.\nMonitor schema evolution.\nVerify your database is reachable from your local environment.\n\n\n\n\n\n\nBefore data ingestion can begin, you need to create two tables in Postgres to hold the raw, unstructured API responses from the flight and weather ingestion services. These tables will serve as append-only logs that store the full JSON payloads alongside timestamps for later transformation.\nWe‚Äôll create:\n\nflight_json_data: stores data retrieved from the OpenSky Network.\nweather_json_data: stores data retrieved from the Open-Meteo API.\n\nEach table includes:\n\nid: an auto-incrementing primary key.\nraw_json: the unmodified API response stored as a jsonb object.\ntimestamptz: a timestamp indicating when the data was ingested.\n\n\n\nPaste the following SQL into Beekeeper Studio and execute it to create both tables:\nCREATE TABLE flight_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\n\nCREATE TABLE weather_json_data (\n id SERIAL PRIMARY KEY,\n raw_json JSONB NOT NULL,\n timestamptz TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  },
  {
    "objectID": "pipelines/db-service.html#whats-next",
    "href": "pipelines/db-service.html#whats-next",
    "title": "Database Service Setup",
    "section": "",
    "text": "Now that your Postgres service is up and running, it‚Äôs ready to accept incoming unstructured data from your ingestion services: - Web2DB Flights: gathers flight telemetry from the OpenSky Network. - Web2DB Weather: gathers Portland weather conditions from Open-Meteo.\nThese services will write raw JSON payloads to the database every 5 minutes. Later, the DataTransform service will structure that data hourly into relational tables optimized for analysis.\nIn the next step, we‚Äôll configure the Web2DB Flights service to begin live ingestion.\nüìö References - https://docs.railway.app/guides/postgresql - https://www.beekeeperstudio.io/ - https://docs.railway.app/databases/external-access",
    "crumbs": [
      "Home",
      "1. Database Service Setup"
    ]
  }
]